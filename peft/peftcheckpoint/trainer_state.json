{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.6182380216383307,
  "eval_steps": 500,
  "global_step": 4000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0,
      "grad_norm": 0.1827082633972168,
      "learning_rate": 0.0029953632148377127,
      "loss": 2.2288,
      "step": 10
    },
    {
      "epoch": 0.0,
      "grad_norm": 0.23032601177692413,
      "learning_rate": 0.002990726429675425,
      "loss": 2.1161,
      "step": 20
    },
    {
      "epoch": 0.0,
      "grad_norm": 0.14165084064006805,
      "learning_rate": 0.0029860896445131377,
      "loss": 2.0501,
      "step": 30
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.28399693965911865,
      "learning_rate": 0.0029814528593508503,
      "loss": 1.9873,
      "step": 40
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.19603374600410461,
      "learning_rate": 0.002976816074188563,
      "loss": 1.943,
      "step": 50
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.198317289352417,
      "learning_rate": 0.0029721792890262753,
      "loss": 1.9324,
      "step": 60
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.09821882843971252,
      "learning_rate": 0.0029675425038639875,
      "loss": 1.8758,
      "step": 70
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.10801924020051956,
      "learning_rate": 0.0029629057187017,
      "loss": 1.8712,
      "step": 80
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.17553089559078217,
      "learning_rate": 0.0029582689335394124,
      "loss": 1.9194,
      "step": 90
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.2702557444572449,
      "learning_rate": 0.002953632148377125,
      "loss": 1.9741,
      "step": 100
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.24067147076129913,
      "learning_rate": 0.002948995363214838,
      "loss": 1.8591,
      "step": 110
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.11206557601690292,
      "learning_rate": 0.00294435857805255,
      "loss": 1.8834,
      "step": 120
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.12493634968996048,
      "learning_rate": 0.0029397217928902627,
      "loss": 1.8768,
      "step": 130
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.10515356063842773,
      "learning_rate": 0.0029350850077279754,
      "loss": 2.0406,
      "step": 140
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.16905808448791504,
      "learning_rate": 0.002930448222565688,
      "loss": 1.91,
      "step": 150
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.2035696804523468,
      "learning_rate": 0.0029258114374034003,
      "loss": 1.8144,
      "step": 160
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.14898920059204102,
      "learning_rate": 0.002921174652241113,
      "loss": 1.8375,
      "step": 170
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.13567404448986053,
      "learning_rate": 0.0029165378670788257,
      "loss": 1.8096,
      "step": 180
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.21737957000732422,
      "learning_rate": 0.002911901081916538,
      "loss": 1.8465,
      "step": 190
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.1331099271774292,
      "learning_rate": 0.0029072642967542506,
      "loss": 1.8469,
      "step": 200
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.1985447108745575,
      "learning_rate": 0.002902627511591963,
      "loss": 1.7648,
      "step": 210
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.1333112269639969,
      "learning_rate": 0.0028979907264296756,
      "loss": 1.9536,
      "step": 220
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.23018954694271088,
      "learning_rate": 0.002893353941267388,
      "loss": 1.782,
      "step": 230
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.11467501521110535,
      "learning_rate": 0.0028887171561051005,
      "loss": 1.8898,
      "step": 240
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.12480627745389938,
      "learning_rate": 0.002884080370942813,
      "loss": 1.6925,
      "step": 250
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.14103640615940094,
      "learning_rate": 0.0028794435857805254,
      "loss": 1.8464,
      "step": 260
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.20648719370365143,
      "learning_rate": 0.002874806800618238,
      "loss": 1.7871,
      "step": 270
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.348296195268631,
      "learning_rate": 0.0028701700154559508,
      "loss": 1.844,
      "step": 280
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.15350747108459473,
      "learning_rate": 0.002865533230293663,
      "loss": 1.7349,
      "step": 290
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.20535728335380554,
      "learning_rate": 0.0028608964451313757,
      "loss": 1.8821,
      "step": 300
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.13721361756324768,
      "learning_rate": 0.0028562596599690884,
      "loss": 1.8333,
      "step": 310
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.1207142323255539,
      "learning_rate": 0.0028516228748068006,
      "loss": 1.8902,
      "step": 320
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.23003454506397247,
      "learning_rate": 0.0028469860896445133,
      "loss": 1.5936,
      "step": 330
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.11439401656389236,
      "learning_rate": 0.0028423493044822255,
      "loss": 1.7225,
      "step": 340
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.1668432354927063,
      "learning_rate": 0.0028377125193199382,
      "loss": 1.6643,
      "step": 350
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.17358307540416718,
      "learning_rate": 0.0028330757341576505,
      "loss": 1.688,
      "step": 360
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.13454343378543854,
      "learning_rate": 0.002828438948995363,
      "loss": 1.8566,
      "step": 370
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.14647991955280304,
      "learning_rate": 0.002823802163833076,
      "loss": 1.8753,
      "step": 380
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.16252708435058594,
      "learning_rate": 0.002819165378670788,
      "loss": 1.7266,
      "step": 390
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.3870472311973572,
      "learning_rate": 0.0028145285935085008,
      "loss": 1.6336,
      "step": 400
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.14557825028896332,
      "learning_rate": 0.0028098918083462134,
      "loss": 1.7409,
      "step": 410
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.23146063089370728,
      "learning_rate": 0.002805255023183926,
      "loss": 1.7544,
      "step": 420
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.3954998254776001,
      "learning_rate": 0.0028006182380216384,
      "loss": 1.7413,
      "step": 430
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.1626642644405365,
      "learning_rate": 0.002795981452859351,
      "loss": 1.723,
      "step": 440
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.13682767748832703,
      "learning_rate": 0.0027913446676970637,
      "loss": 1.7119,
      "step": 450
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.1152455285191536,
      "learning_rate": 0.002786707882534776,
      "loss": 1.7852,
      "step": 460
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.41169434785842896,
      "learning_rate": 0.0027820710973724882,
      "loss": 1.7918,
      "step": 470
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.18594443798065186,
      "learning_rate": 0.002777434312210201,
      "loss": 1.825,
      "step": 480
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.19897373020648956,
      "learning_rate": 0.0027727975270479136,
      "loss": 1.7555,
      "step": 490
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.22565840184688568,
      "learning_rate": 0.002768160741885626,
      "loss": 1.7735,
      "step": 500
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.1967439353466034,
      "learning_rate": 0.0027635239567233385,
      "loss": 1.6503,
      "step": 510
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.31237974762916565,
      "learning_rate": 0.002758887171561051,
      "loss": 1.7318,
      "step": 520
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.16539694368839264,
      "learning_rate": 0.0027542503863987634,
      "loss": 1.7325,
      "step": 530
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.11439265310764313,
      "learning_rate": 0.002749613601236476,
      "loss": 1.8018,
      "step": 540
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.18843910098075867,
      "learning_rate": 0.002744976816074189,
      "loss": 1.5509,
      "step": 550
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.5252532958984375,
      "learning_rate": 0.002740340030911901,
      "loss": 1.6861,
      "step": 560
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.18954700231552124,
      "learning_rate": 0.0027357032457496137,
      "loss": 1.8563,
      "step": 570
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.1684211641550064,
      "learning_rate": 0.0027310664605873264,
      "loss": 1.7111,
      "step": 580
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.1351114958524704,
      "learning_rate": 0.0027264296754250386,
      "loss": 1.7266,
      "step": 590
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.0985289216041565,
      "learning_rate": 0.0027217928902627513,
      "loss": 1.9093,
      "step": 600
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.08333272486925125,
      "learning_rate": 0.0027171561051004636,
      "loss": 1.754,
      "step": 610
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.10226127505302429,
      "learning_rate": 0.0027125193199381763,
      "loss": 1.7928,
      "step": 620
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.232292041182518,
      "learning_rate": 0.0027078825347758885,
      "loss": 1.7688,
      "step": 630
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.1194806694984436,
      "learning_rate": 0.002703245749613601,
      "loss": 1.7174,
      "step": 640
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.2187120020389557,
      "learning_rate": 0.002698608964451314,
      "loss": 1.8835,
      "step": 650
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.18830382823944092,
      "learning_rate": 0.0026939721792890265,
      "loss": 1.6423,
      "step": 660
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.12723277509212494,
      "learning_rate": 0.002689335394126739,
      "loss": 1.8117,
      "step": 670
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.17211537063121796,
      "learning_rate": 0.0026846986089644515,
      "loss": 1.8308,
      "step": 680
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.1861012727022171,
      "learning_rate": 0.002680061823802164,
      "loss": 1.7713,
      "step": 690
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.07755443453788757,
      "learning_rate": 0.0026754250386398764,
      "loss": 1.6775,
      "step": 700
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.1552964746952057,
      "learning_rate": 0.002670788253477589,
      "loss": 1.7612,
      "step": 710
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.130941241979599,
      "learning_rate": 0.0026661514683153013,
      "loss": 1.7652,
      "step": 720
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.08457673341035843,
      "learning_rate": 0.002661514683153014,
      "loss": 1.8359,
      "step": 730
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.18476267158985138,
      "learning_rate": 0.0026568778979907262,
      "loss": 1.6688,
      "step": 740
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.10509080439805984,
      "learning_rate": 0.002652241112828439,
      "loss": 1.8132,
      "step": 750
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.22183158993721008,
      "learning_rate": 0.0026476043276661516,
      "loss": 1.6709,
      "step": 760
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.14859932661056519,
      "learning_rate": 0.002642967542503864,
      "loss": 1.8868,
      "step": 770
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.17919179797172546,
      "learning_rate": 0.0026383307573415765,
      "loss": 1.7072,
      "step": 780
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.12590596079826355,
      "learning_rate": 0.002633693972179289,
      "loss": 1.7132,
      "step": 790
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.2507385313510895,
      "learning_rate": 0.0026290571870170015,
      "loss": 1.6715,
      "step": 800
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.0936330258846283,
      "learning_rate": 0.002624420401854714,
      "loss": 1.6944,
      "step": 810
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.20546410977840424,
      "learning_rate": 0.002619783616692427,
      "loss": 1.7866,
      "step": 820
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.14859479665756226,
      "learning_rate": 0.002615146831530139,
      "loss": 1.6591,
      "step": 830
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.35589534044265747,
      "learning_rate": 0.0026105100463678517,
      "loss": 1.7712,
      "step": 840
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.19403856992721558,
      "learning_rate": 0.0026058732612055644,
      "loss": 1.7658,
      "step": 850
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.11910645663738251,
      "learning_rate": 0.0026012364760432767,
      "loss": 1.8205,
      "step": 860
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.17468897998332977,
      "learning_rate": 0.002596599690880989,
      "loss": 1.7356,
      "step": 870
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.1176188737154007,
      "learning_rate": 0.0025919629057187016,
      "loss": 1.8507,
      "step": 880
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.13336320221424103,
      "learning_rate": 0.0025873261205564143,
      "loss": 1.6312,
      "step": 890
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.2077338844537735,
      "learning_rate": 0.0025826893353941265,
      "loss": 1.6948,
      "step": 900
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.11226984858512878,
      "learning_rate": 0.002578052550231839,
      "loss": 1.7893,
      "step": 910
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.1751793920993805,
      "learning_rate": 0.002573415765069552,
      "loss": 1.6732,
      "step": 920
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.12598447501659393,
      "learning_rate": 0.0025687789799072646,
      "loss": 1.7778,
      "step": 930
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.12773269414901733,
      "learning_rate": 0.002564142194744977,
      "loss": 1.6584,
      "step": 940
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.1459798365831375,
      "learning_rate": 0.0025595054095826895,
      "loss": 1.8363,
      "step": 950
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.1271747648715973,
      "learning_rate": 0.002554868624420402,
      "loss": 1.6452,
      "step": 960
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.16306008398532867,
      "learning_rate": 0.0025502318392581144,
      "loss": 1.8258,
      "step": 970
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.10080061852931976,
      "learning_rate": 0.002545595054095827,
      "loss": 1.7082,
      "step": 980
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.1070970669388771,
      "learning_rate": 0.0025409582689335393,
      "loss": 1.805,
      "step": 990
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.19502224028110504,
      "learning_rate": 0.002536321483771252,
      "loss": 1.8091,
      "step": 1000
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.140761598944664,
      "learning_rate": 0.0025316846986089643,
      "loss": 1.6784,
      "step": 1010
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.10268115252256393,
      "learning_rate": 0.002527047913446677,
      "loss": 1.6491,
      "step": 1020
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.12345387786626816,
      "learning_rate": 0.0025224111282843896,
      "loss": 1.7059,
      "step": 1030
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.12473756819963455,
      "learning_rate": 0.002517774343122102,
      "loss": 1.6746,
      "step": 1040
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.36649835109710693,
      "learning_rate": 0.0025131375579598146,
      "loss": 1.9143,
      "step": 1050
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.15427754819393158,
      "learning_rate": 0.0025085007727975272,
      "loss": 1.7144,
      "step": 1060
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.10020237416028976,
      "learning_rate": 0.0025038639876352395,
      "loss": 1.7192,
      "step": 1070
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.15810315310955048,
      "learning_rate": 0.002499227202472952,
      "loss": 1.6444,
      "step": 1080
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.09492458403110504,
      "learning_rate": 0.002494590417310665,
      "loss": 1.6856,
      "step": 1090
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.3003155291080475,
      "learning_rate": 0.0024899536321483775,
      "loss": 1.7669,
      "step": 1100
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.22699998319149017,
      "learning_rate": 0.0024853168469860898,
      "loss": 1.6561,
      "step": 1110
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.16275714337825775,
      "learning_rate": 0.0024806800618238025,
      "loss": 1.8321,
      "step": 1120
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.20657950639724731,
      "learning_rate": 0.0024760432766615147,
      "loss": 1.7082,
      "step": 1130
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.20252864062786102,
      "learning_rate": 0.002471406491499227,
      "loss": 1.7417,
      "step": 1140
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.11131657660007477,
      "learning_rate": 0.0024667697063369396,
      "loss": 1.734,
      "step": 1150
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.1202133521437645,
      "learning_rate": 0.0024621329211746523,
      "loss": 1.7321,
      "step": 1160
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.22389808297157288,
      "learning_rate": 0.0024574961360123646,
      "loss": 1.8092,
      "step": 1170
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.23470179736614227,
      "learning_rate": 0.0024528593508500772,
      "loss": 1.8997,
      "step": 1180
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.2632077634334564,
      "learning_rate": 0.00244822256568779,
      "loss": 1.6347,
      "step": 1190
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.19663220643997192,
      "learning_rate": 0.0024435857805255026,
      "loss": 1.7699,
      "step": 1200
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.15787309408187866,
      "learning_rate": 0.002438948995363215,
      "loss": 1.717,
      "step": 1210
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.11016197502613068,
      "learning_rate": 0.0024343122102009275,
      "loss": 1.7549,
      "step": 1220
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.245070680975914,
      "learning_rate": 0.00242967542503864,
      "loss": 1.8703,
      "step": 1230
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.13275159895420074,
      "learning_rate": 0.0024250386398763524,
      "loss": 1.8605,
      "step": 1240
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.08320485800504684,
      "learning_rate": 0.002420401854714065,
      "loss": 1.7193,
      "step": 1250
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.19001497328281403,
      "learning_rate": 0.0024157650695517774,
      "loss": 1.7937,
      "step": 1260
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.14725327491760254,
      "learning_rate": 0.00241112828438949,
      "loss": 1.8194,
      "step": 1270
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.2631571292877197,
      "learning_rate": 0.0024064914992272023,
      "loss": 1.6823,
      "step": 1280
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.17520977556705475,
      "learning_rate": 0.002401854714064915,
      "loss": 1.7485,
      "step": 1290
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.19749030470848083,
      "learning_rate": 0.0023972179289026277,
      "loss": 1.7191,
      "step": 1300
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.18763208389282227,
      "learning_rate": 0.00239258114374034,
      "loss": 1.6305,
      "step": 1310
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.14163276553153992,
      "learning_rate": 0.0023879443585780526,
      "loss": 1.5397,
      "step": 1320
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.16986003518104553,
      "learning_rate": 0.0023833075734157653,
      "loss": 1.7411,
      "step": 1330
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.22112171351909637,
      "learning_rate": 0.0023786707882534775,
      "loss": 1.6895,
      "step": 1340
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.16604304313659668,
      "learning_rate": 0.00237403400309119,
      "loss": 1.7756,
      "step": 1350
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.12197692692279816,
      "learning_rate": 0.002369397217928903,
      "loss": 1.7658,
      "step": 1360
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.29329565167427063,
      "learning_rate": 0.0023647604327666156,
      "loss": 1.7224,
      "step": 1370
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.09922406822443008,
      "learning_rate": 0.002360123647604328,
      "loss": 1.6676,
      "step": 1380
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.1884511411190033,
      "learning_rate": 0.00235548686244204,
      "loss": 1.6433,
      "step": 1390
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.16838522255420685,
      "learning_rate": 0.0023508500772797527,
      "loss": 1.524,
      "step": 1400
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.2446645051240921,
      "learning_rate": 0.002346213292117465,
      "loss": 1.6595,
      "step": 1410
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.11684145033359528,
      "learning_rate": 0.0023415765069551777,
      "loss": 1.7128,
      "step": 1420
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.10325075685977936,
      "learning_rate": 0.0023369397217928903,
      "loss": 1.7382,
      "step": 1430
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.12558874487876892,
      "learning_rate": 0.0023323029366306026,
      "loss": 1.9348,
      "step": 1440
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.09923427551984787,
      "learning_rate": 0.0023276661514683153,
      "loss": 1.687,
      "step": 1450
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.16055211424827576,
      "learning_rate": 0.002323029366306028,
      "loss": 1.6034,
      "step": 1460
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.3224713206291199,
      "learning_rate": 0.0023183925811437406,
      "loss": 1.6759,
      "step": 1470
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.14576983451843262,
      "learning_rate": 0.002313755795981453,
      "loss": 1.6937,
      "step": 1480
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.2243543565273285,
      "learning_rate": 0.0023091190108191655,
      "loss": 1.8664,
      "step": 1490
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.12311491370201111,
      "learning_rate": 0.0023044822256568782,
      "loss": 1.7547,
      "step": 1500
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.16244420409202576,
      "learning_rate": 0.0022998454404945905,
      "loss": 1.7655,
      "step": 1510
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.13510189950466156,
      "learning_rate": 0.002295208655332303,
      "loss": 1.721,
      "step": 1520
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.15124954283237457,
      "learning_rate": 0.0022905718701700154,
      "loss": 1.6677,
      "step": 1530
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.1869400590658188,
      "learning_rate": 0.002285935085007728,
      "loss": 1.7003,
      "step": 1540
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.19741351902484894,
      "learning_rate": 0.0022812982998454403,
      "loss": 1.8195,
      "step": 1550
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.28537997603416443,
      "learning_rate": 0.002276661514683153,
      "loss": 1.7213,
      "step": 1560
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.25589632987976074,
      "learning_rate": 0.0022720247295208657,
      "loss": 1.7694,
      "step": 1570
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.16470500826835632,
      "learning_rate": 0.002267387944358578,
      "loss": 1.7914,
      "step": 1580
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.1098145842552185,
      "learning_rate": 0.0022627511591962906,
      "loss": 1.7388,
      "step": 1590
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.14924319088459015,
      "learning_rate": 0.0022581143740340033,
      "loss": 1.824,
      "step": 1600
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.25499215722084045,
      "learning_rate": 0.0022534775888717155,
      "loss": 1.7554,
      "step": 1610
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.2307574301958084,
      "learning_rate": 0.0022488408037094282,
      "loss": 1.7918,
      "step": 1620
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.12928009033203125,
      "learning_rate": 0.002244204018547141,
      "loss": 1.6445,
      "step": 1630
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.15857090055942535,
      "learning_rate": 0.002239567233384853,
      "loss": 1.7137,
      "step": 1640
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.11790432035923004,
      "learning_rate": 0.002234930448222566,
      "loss": 1.7087,
      "step": 1650
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.11809727549552917,
      "learning_rate": 0.002230293663060278,
      "loss": 1.6809,
      "step": 1660
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.09928213059902191,
      "learning_rate": 0.0022256568778979907,
      "loss": 1.7486,
      "step": 1670
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.16879507899284363,
      "learning_rate": 0.002221020092735703,
      "loss": 1.7839,
      "step": 1680
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.13080526888370514,
      "learning_rate": 0.0022163833075734157,
      "loss": 1.6307,
      "step": 1690
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.13965141773223877,
      "learning_rate": 0.0022117465224111284,
      "loss": 1.7008,
      "step": 1700
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.48635852336883545,
      "learning_rate": 0.002207109737248841,
      "loss": 1.6745,
      "step": 1710
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.1375236064195633,
      "learning_rate": 0.0022024729520865533,
      "loss": 1.6742,
      "step": 1720
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.33685413002967834,
      "learning_rate": 0.002197836166924266,
      "loss": 1.7526,
      "step": 1730
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.1271842122077942,
      "learning_rate": 0.0021931993817619786,
      "loss": 1.6496,
      "step": 1740
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.1981387436389923,
      "learning_rate": 0.002188562596599691,
      "loss": 1.7102,
      "step": 1750
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.1496933251619339,
      "learning_rate": 0.0021839258114374036,
      "loss": 1.7609,
      "step": 1760
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.1630469709634781,
      "learning_rate": 0.0021792890262751162,
      "loss": 1.8183,
      "step": 1770
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.3143593966960907,
      "learning_rate": 0.0021746522411128285,
      "loss": 1.63,
      "step": 1780
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.27614569664001465,
      "learning_rate": 0.0021700154559505407,
      "loss": 1.6452,
      "step": 1790
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.1382720172405243,
      "learning_rate": 0.0021653786707882534,
      "loss": 1.7261,
      "step": 1800
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.2337302714586258,
      "learning_rate": 0.002160741885625966,
      "loss": 1.631,
      "step": 1810
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.14480537176132202,
      "learning_rate": 0.0021561051004636783,
      "loss": 1.6035,
      "step": 1820
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.1648329496383667,
      "learning_rate": 0.002151468315301391,
      "loss": 1.6244,
      "step": 1830
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.29077452421188354,
      "learning_rate": 0.0021468315301391037,
      "loss": 1.6637,
      "step": 1840
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.21980565786361694,
      "learning_rate": 0.002142194744976816,
      "loss": 1.6918,
      "step": 1850
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.20992252230644226,
      "learning_rate": 0.0021375579598145286,
      "loss": 1.7205,
      "step": 1860
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.5175544023513794,
      "learning_rate": 0.0021329211746522413,
      "loss": 1.7892,
      "step": 1870
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.19172035157680511,
      "learning_rate": 0.002128284389489954,
      "loss": 1.6728,
      "step": 1880
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.2942498028278351,
      "learning_rate": 0.0021236476043276662,
      "loss": 1.6295,
      "step": 1890
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.13884660601615906,
      "learning_rate": 0.002119010819165379,
      "loss": 1.7009,
      "step": 1900
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.10348007827997208,
      "learning_rate": 0.002114374034003091,
      "loss": 1.6599,
      "step": 1910
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.12712162733078003,
      "learning_rate": 0.002109737248840804,
      "loss": 1.7589,
      "step": 1920
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.13403546810150146,
      "learning_rate": 0.002105100463678516,
      "loss": 1.6781,
      "step": 1930
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.2654331624507904,
      "learning_rate": 0.0021004636785162288,
      "loss": 1.6123,
      "step": 1940
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.19582630693912506,
      "learning_rate": 0.002095826893353941,
      "loss": 1.7115,
      "step": 1950
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.15252280235290527,
      "learning_rate": 0.0020911901081916537,
      "loss": 1.8058,
      "step": 1960
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.24223707616329193,
      "learning_rate": 0.0020865533230293664,
      "loss": 1.7091,
      "step": 1970
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.1410362869501114,
      "learning_rate": 0.002081916537867079,
      "loss": 1.7475,
      "step": 1980
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.20187382400035858,
      "learning_rate": 0.0020772797527047913,
      "loss": 1.8032,
      "step": 1990
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.20793560147285461,
      "learning_rate": 0.002072642967542504,
      "loss": 1.6255,
      "step": 2000
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.10806542634963989,
      "learning_rate": 0.0020680061823802167,
      "loss": 1.7691,
      "step": 2010
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.10541032254695892,
      "learning_rate": 0.002063369397217929,
      "loss": 1.7494,
      "step": 2020
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.11625830084085464,
      "learning_rate": 0.0020587326120556416,
      "loss": 1.6694,
      "step": 2030
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.131997212767601,
      "learning_rate": 0.002054095826893354,
      "loss": 1.7219,
      "step": 2040
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.2547522783279419,
      "learning_rate": 0.0020494590417310665,
      "loss": 1.5305,
      "step": 2050
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.08398568630218506,
      "learning_rate": 0.0020448222565687788,
      "loss": 1.7101,
      "step": 2060
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.12880687415599823,
      "learning_rate": 0.0020401854714064914,
      "loss": 1.6751,
      "step": 2070
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.3656477630138397,
      "learning_rate": 0.002035548686244204,
      "loss": 1.7495,
      "step": 2080
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.2082054167985916,
      "learning_rate": 0.0020309119010819164,
      "loss": 1.5999,
      "step": 2090
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.19568811357021332,
      "learning_rate": 0.002026275115919629,
      "loss": 1.7253,
      "step": 2100
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.12764646112918854,
      "learning_rate": 0.0020216383307573417,
      "loss": 1.607,
      "step": 2110
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.1584482491016388,
      "learning_rate": 0.002017001545595054,
      "loss": 1.7595,
      "step": 2120
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.14955741167068481,
      "learning_rate": 0.0020123647604327667,
      "loss": 1.8304,
      "step": 2130
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.297229528427124,
      "learning_rate": 0.0020077279752704793,
      "loss": 1.6287,
      "step": 2140
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.13537627458572388,
      "learning_rate": 0.002003091190108192,
      "loss": 1.8272,
      "step": 2150
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.11444664746522903,
      "learning_rate": 0.0019984544049459043,
      "loss": 1.7029,
      "step": 2160
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.12045438587665558,
      "learning_rate": 0.001993817619783617,
      "loss": 1.705,
      "step": 2170
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.1579943150281906,
      "learning_rate": 0.001989180834621329,
      "loss": 1.7755,
      "step": 2180
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.2206084132194519,
      "learning_rate": 0.0019845440494590414,
      "loss": 1.6156,
      "step": 2190
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.12855063378810883,
      "learning_rate": 0.001979907264296754,
      "loss": 1.6938,
      "step": 2200
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.16510874032974243,
      "learning_rate": 0.001975270479134467,
      "loss": 1.742,
      "step": 2210
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.15399888157844543,
      "learning_rate": 0.001970633693972179,
      "loss": 1.7083,
      "step": 2220
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.15010342001914978,
      "learning_rate": 0.0019659969088098917,
      "loss": 1.6375,
      "step": 2230
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.14939798414707184,
      "learning_rate": 0.0019613601236476044,
      "loss": 1.788,
      "step": 2240
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.12175147980451584,
      "learning_rate": 0.001956723338485317,
      "loss": 1.7809,
      "step": 2250
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.15091456472873688,
      "learning_rate": 0.0019520865533230293,
      "loss": 1.8055,
      "step": 2260
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.13534900546073914,
      "learning_rate": 0.001947449768160742,
      "loss": 1.6097,
      "step": 2270
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.251056045293808,
      "learning_rate": 0.0019428129829984545,
      "loss": 1.5928,
      "step": 2280
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.18535083532333374,
      "learning_rate": 0.0019381761978361667,
      "loss": 1.683,
      "step": 2290
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.20574110746383667,
      "learning_rate": 0.0019335394126738794,
      "loss": 1.762,
      "step": 2300
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.2169119119644165,
      "learning_rate": 0.001928902627511592,
      "loss": 1.774,
      "step": 2310
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.17223253846168518,
      "learning_rate": 0.0019242658423493048,
      "loss": 1.698,
      "step": 2320
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.1038820669054985,
      "learning_rate": 0.001919629057187017,
      "loss": 1.8049,
      "step": 2330
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.14219626784324646,
      "learning_rate": 0.0019149922720247297,
      "loss": 1.6924,
      "step": 2340
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.16828007996082306,
      "learning_rate": 0.0019103554868624422,
      "loss": 1.7803,
      "step": 2350
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.09904154390096664,
      "learning_rate": 0.0019057187017001544,
      "loss": 1.6742,
      "step": 2360
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.17213870584964752,
      "learning_rate": 0.001901081916537867,
      "loss": 1.793,
      "step": 2370
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.1400328278541565,
      "learning_rate": 0.0018964451313755798,
      "loss": 1.8741,
      "step": 2380
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.17747355997562408,
      "learning_rate": 0.001891808346213292,
      "loss": 1.7746,
      "step": 2390
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.18107542395591736,
      "learning_rate": 0.0018871715610510047,
      "loss": 1.5517,
      "step": 2400
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.16909918189048767,
      "learning_rate": 0.0018825347758887172,
      "loss": 1.8033,
      "step": 2410
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.11645974218845367,
      "learning_rate": 0.0018778979907264298,
      "loss": 1.6611,
      "step": 2420
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.4463360011577606,
      "learning_rate": 0.001873261205564142,
      "loss": 1.674,
      "step": 2430
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.18836022913455963,
      "learning_rate": 0.0018686244204018548,
      "loss": 1.5991,
      "step": 2440
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.13442809879779816,
      "learning_rate": 0.0018639876352395674,
      "loss": 1.5154,
      "step": 2450
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.21840807795524597,
      "learning_rate": 0.0018593508500772797,
      "loss": 1.8921,
      "step": 2460
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.18305033445358276,
      "learning_rate": 0.0018547140649149924,
      "loss": 1.7625,
      "step": 2470
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.1848963350057602,
      "learning_rate": 0.0018500772797527048,
      "loss": 1.7263,
      "step": 2480
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.15554767847061157,
      "learning_rate": 0.0018454404945904173,
      "loss": 1.6714,
      "step": 2490
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.16335681080818176,
      "learning_rate": 0.0018408037094281298,
      "loss": 1.7903,
      "step": 2500
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.1910611391067505,
      "learning_rate": 0.0018361669242658424,
      "loss": 1.7684,
      "step": 2510
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.168549582362175,
      "learning_rate": 0.0018315301391035551,
      "loss": 1.6428,
      "step": 2520
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.11424627900123596,
      "learning_rate": 0.0018268933539412674,
      "loss": 1.7259,
      "step": 2530
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.12308303266763687,
      "learning_rate": 0.00182225656877898,
      "loss": 1.6784,
      "step": 2540
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.1884012073278427,
      "learning_rate": 0.0018176197836166925,
      "loss": 1.9505,
      "step": 2550
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.2810092270374298,
      "learning_rate": 0.0018129829984544048,
      "loss": 1.5872,
      "step": 2560
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.15934568643569946,
      "learning_rate": 0.0018083462132921174,
      "loss": 1.6763,
      "step": 2570
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.11727486550807953,
      "learning_rate": 0.0018037094281298301,
      "loss": 1.664,
      "step": 2580
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.15338416397571564,
      "learning_rate": 0.0017990726429675428,
      "loss": 1.6314,
      "step": 2590
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.15296140313148499,
      "learning_rate": 0.001794435857805255,
      "loss": 1.7765,
      "step": 2600
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.1451486200094223,
      "learning_rate": 0.0017897990726429675,
      "loss": 1.7881,
      "step": 2610
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.18561553955078125,
      "learning_rate": 0.0017851622874806802,
      "loss": 1.6062,
      "step": 2620
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.13537338376045227,
      "learning_rate": 0.0017805255023183924,
      "loss": 1.7478,
      "step": 2630
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.40316644310951233,
      "learning_rate": 0.001775888717156105,
      "loss": 1.8685,
      "step": 2640
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.1437413990497589,
      "learning_rate": 0.0017712519319938178,
      "loss": 1.7321,
      "step": 2650
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.1250849962234497,
      "learning_rate": 0.00176661514683153,
      "loss": 1.6774,
      "step": 2660
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.20880644023418427,
      "learning_rate": 0.0017619783616692427,
      "loss": 1.6948,
      "step": 2670
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.19430464506149292,
      "learning_rate": 0.0017573415765069552,
      "loss": 1.7889,
      "step": 2680
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.3664723336696625,
      "learning_rate": 0.0017527047913446679,
      "loss": 1.6384,
      "step": 2690
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.11949285864830017,
      "learning_rate": 0.00174806800618238,
      "loss": 1.6507,
      "step": 2700
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.16299499571323395,
      "learning_rate": 0.0017434312210200928,
      "loss": 1.7853,
      "step": 2710
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.1152646392583847,
      "learning_rate": 0.0017387944358578055,
      "loss": 1.6699,
      "step": 2720
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.1095452681183815,
      "learning_rate": 0.0017341576506955177,
      "loss": 1.7972,
      "step": 2730
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.16251017153263092,
      "learning_rate": 0.0017295208655332304,
      "loss": 1.6519,
      "step": 2740
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.1378898173570633,
      "learning_rate": 0.0017248840803709429,
      "loss": 1.7503,
      "step": 2750
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.14797252416610718,
      "learning_rate": 0.0017202472952086555,
      "loss": 1.6263,
      "step": 2760
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.1608828753232956,
      "learning_rate": 0.0017156105100463678,
      "loss": 1.6391,
      "step": 2770
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.15264110267162323,
      "learning_rate": 0.0017109737248840805,
      "loss": 1.6073,
      "step": 2780
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.20866571366786957,
      "learning_rate": 0.0017063369397217931,
      "loss": 1.6805,
      "step": 2790
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.2192082405090332,
      "learning_rate": 0.0017017001545595054,
      "loss": 1.7335,
      "step": 2800
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.14488959312438965,
      "learning_rate": 0.0016970633693972178,
      "loss": 1.6688,
      "step": 2810
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.1169748306274414,
      "learning_rate": 0.0016924265842349305,
      "loss": 1.6511,
      "step": 2820
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.16338668763637543,
      "learning_rate": 0.0016877897990726428,
      "loss": 1.786,
      "step": 2830
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.12312091886997223,
      "learning_rate": 0.0016831530139103555,
      "loss": 1.5793,
      "step": 2840
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.16681043803691864,
      "learning_rate": 0.0016785162287480681,
      "loss": 1.7122,
      "step": 2850
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.15842704474925995,
      "learning_rate": 0.0016738794435857806,
      "loss": 1.6663,
      "step": 2860
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.18157267570495605,
      "learning_rate": 0.001669242658423493,
      "loss": 1.6066,
      "step": 2870
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.1961253583431244,
      "learning_rate": 0.0016646058732612055,
      "loss": 1.6484,
      "step": 2880
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.15606236457824707,
      "learning_rate": 0.0016599690880989182,
      "loss": 1.7908,
      "step": 2890
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.24851712584495544,
      "learning_rate": 0.0016553323029366305,
      "loss": 1.6894,
      "step": 2900
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.10659607499837875,
      "learning_rate": 0.0016506955177743431,
      "loss": 1.7025,
      "step": 2910
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.1168166920542717,
      "learning_rate": 0.0016460587326120558,
      "loss": 1.7255,
      "step": 2920
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.18983158469200134,
      "learning_rate": 0.0016414219474497683,
      "loss": 1.8558,
      "step": 2930
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.1607852280139923,
      "learning_rate": 0.0016367851622874807,
      "loss": 1.6832,
      "step": 2940
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.08296454697847366,
      "learning_rate": 0.0016321483771251932,
      "loss": 1.6948,
      "step": 2950
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.11231057345867157,
      "learning_rate": 0.0016275115919629059,
      "loss": 1.7451,
      "step": 2960
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.1698756366968155,
      "learning_rate": 0.0016228748068006181,
      "loss": 1.649,
      "step": 2970
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.13773088157176971,
      "learning_rate": 0.0016182380216383308,
      "loss": 1.6312,
      "step": 2980
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.17153136432170868,
      "learning_rate": 0.0016136012364760435,
      "loss": 1.6738,
      "step": 2990
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.12339365482330322,
      "learning_rate": 0.0016089644513137557,
      "loss": 1.6367,
      "step": 3000
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.22498966753482819,
      "learning_rate": 0.0016043276661514682,
      "loss": 1.7248,
      "step": 3010
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.18235382437705994,
      "learning_rate": 0.0015996908809891809,
      "loss": 1.6315,
      "step": 3020
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.17755459249019623,
      "learning_rate": 0.0015950540958268936,
      "loss": 1.6308,
      "step": 3030
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.20938988029956818,
      "learning_rate": 0.0015904173106646058,
      "loss": 1.6838,
      "step": 3040
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.1463167816400528,
      "learning_rate": 0.0015857805255023185,
      "loss": 1.6397,
      "step": 3050
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.10662084817886353,
      "learning_rate": 0.001581143740340031,
      "loss": 1.7815,
      "step": 3060
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.2926318943500519,
      "learning_rate": 0.0015765069551777434,
      "loss": 1.6047,
      "step": 3070
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.21641074120998383,
      "learning_rate": 0.0015718701700154559,
      "loss": 1.5915,
      "step": 3080
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.25241756439208984,
      "learning_rate": 0.0015672333848531686,
      "loss": 1.7213,
      "step": 3090
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.12029769271612167,
      "learning_rate": 0.0015625965996908808,
      "loss": 1.65,
      "step": 3100
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.13180123269557953,
      "learning_rate": 0.0015579598145285935,
      "loss": 1.8242,
      "step": 3110
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.3692359924316406,
      "learning_rate": 0.0015533230293663062,
      "loss": 1.6648,
      "step": 3120
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.1755092889070511,
      "learning_rate": 0.0015486862442040186,
      "loss": 1.6274,
      "step": 3130
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.08909422904253006,
      "learning_rate": 0.001544049459041731,
      "loss": 1.6748,
      "step": 3140
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.18202930688858032,
      "learning_rate": 0.0015394126738794436,
      "loss": 1.6238,
      "step": 3150
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.09978414326906204,
      "learning_rate": 0.0015347758887171562,
      "loss": 1.6539,
      "step": 3160
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.13223421573638916,
      "learning_rate": 0.0015301391035548685,
      "loss": 1.6052,
      "step": 3170
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.3036644458770752,
      "learning_rate": 0.0015255023183925812,
      "loss": 1.4938,
      "step": 3180
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.20075005292892456,
      "learning_rate": 0.0015208655332302938,
      "loss": 1.7042,
      "step": 3190
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.11068259924650192,
      "learning_rate": 0.0015162287480680063,
      "loss": 1.602,
      "step": 3200
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.19340457022190094,
      "learning_rate": 0.0015115919629057185,
      "loss": 1.5642,
      "step": 3210
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.14573809504508972,
      "learning_rate": 0.0015069551777434312,
      "loss": 1.7093,
      "step": 3220
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.19775035977363586,
      "learning_rate": 0.001502318392581144,
      "loss": 1.6505,
      "step": 3230
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.18800754845142365,
      "learning_rate": 0.0014976816074188564,
      "loss": 1.6048,
      "step": 3240
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.10178998857736588,
      "learning_rate": 0.0014930448222565688,
      "loss": 1.7885,
      "step": 3250
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.14941897988319397,
      "learning_rate": 0.0014884080370942815,
      "loss": 1.6494,
      "step": 3260
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.09137509763240814,
      "learning_rate": 0.0014837712519319938,
      "loss": 1.742,
      "step": 3270
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.1316138505935669,
      "learning_rate": 0.0014791344667697062,
      "loss": 1.7683,
      "step": 3280
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.1523759663105011,
      "learning_rate": 0.001474497681607419,
      "loss": 1.6642,
      "step": 3290
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.1196465790271759,
      "learning_rate": 0.0014698608964451314,
      "loss": 1.821,
      "step": 3300
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.13922381401062012,
      "learning_rate": 0.001465224111282844,
      "loss": 1.6381,
      "step": 3310
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.10841252654790878,
      "learning_rate": 0.0014605873261205565,
      "loss": 1.6809,
      "step": 3320
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.2210296392440796,
      "learning_rate": 0.001455950540958269,
      "loss": 1.7513,
      "step": 3330
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.3985048234462738,
      "learning_rate": 0.0014513137557959814,
      "loss": 1.6355,
      "step": 3340
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.14732371270656586,
      "learning_rate": 0.001446676970633694,
      "loss": 1.6354,
      "step": 3350
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.09750425815582275,
      "learning_rate": 0.0014420401854714066,
      "loss": 1.6332,
      "step": 3360
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.16384267807006836,
      "learning_rate": 0.001437403400309119,
      "loss": 1.6107,
      "step": 3370
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.07978972792625427,
      "learning_rate": 0.0014327666151468315,
      "loss": 1.5751,
      "step": 3380
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.1541309356689453,
      "learning_rate": 0.0014281298299845442,
      "loss": 1.5325,
      "step": 3390
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.18317310512065887,
      "learning_rate": 0.0014234930448222567,
      "loss": 1.6474,
      "step": 3400
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.29826486110687256,
      "learning_rate": 0.0014188562596599691,
      "loss": 1.7678,
      "step": 3410
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.10386297106742859,
      "learning_rate": 0.0014142194744976816,
      "loss": 1.737,
      "step": 3420
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.17813052237033844,
      "learning_rate": 0.001409582689335394,
      "loss": 1.6661,
      "step": 3430
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.1535247266292572,
      "learning_rate": 0.0014049459041731067,
      "loss": 1.6439,
      "step": 3440
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.13134750723838806,
      "learning_rate": 0.0014003091190108192,
      "loss": 1.7553,
      "step": 3450
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.3734779953956604,
      "learning_rate": 0.0013956723338485319,
      "loss": 1.851,
      "step": 3460
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.12776793539524078,
      "learning_rate": 0.0013910355486862441,
      "loss": 1.7385,
      "step": 3470
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.13726787269115448,
      "learning_rate": 0.0013863987635239568,
      "loss": 1.7596,
      "step": 3480
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.15949833393096924,
      "learning_rate": 0.0013817619783616693,
      "loss": 1.7634,
      "step": 3490
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.10843951255083084,
      "learning_rate": 0.0013771251931993817,
      "loss": 1.6418,
      "step": 3500
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.13413739204406738,
      "learning_rate": 0.0013724884080370944,
      "loss": 1.6616,
      "step": 3510
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.11571930348873138,
      "learning_rate": 0.0013678516228748069,
      "loss": 1.6432,
      "step": 3520
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.09742426872253418,
      "learning_rate": 0.0013632148377125193,
      "loss": 1.6387,
      "step": 3530
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.38088417053222656,
      "learning_rate": 0.0013585780525502318,
      "loss": 1.6733,
      "step": 3540
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.08803217858076096,
      "learning_rate": 0.0013539412673879443,
      "loss": 1.5685,
      "step": 3550
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.1416994333267212,
      "learning_rate": 0.001349304482225657,
      "loss": 1.6603,
      "step": 3560
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.1375555843114853,
      "learning_rate": 0.0013446676970633694,
      "loss": 1.7835,
      "step": 3570
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.15183058381080627,
      "learning_rate": 0.001340030911901082,
      "loss": 1.7492,
      "step": 3580
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.19175614416599274,
      "learning_rate": 0.0013353941267387945,
      "loss": 1.7516,
      "step": 3590
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.09791148453950882,
      "learning_rate": 0.001330757341576507,
      "loss": 1.7291,
      "step": 3600
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.14721937477588654,
      "learning_rate": 0.0013261205564142195,
      "loss": 1.5916,
      "step": 3610
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.1513708084821701,
      "learning_rate": 0.001321483771251932,
      "loss": 1.6781,
      "step": 3620
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.16240723431110382,
      "learning_rate": 0.0013168469860896446,
      "loss": 1.8981,
      "step": 3630
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.09192956238985062,
      "learning_rate": 0.001312210200927357,
      "loss": 1.6541,
      "step": 3640
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.16317497193813324,
      "learning_rate": 0.0013075734157650695,
      "loss": 1.6021,
      "step": 3650
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.11589314788579941,
      "learning_rate": 0.0013029366306027822,
      "loss": 1.7843,
      "step": 3660
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.30598026514053345,
      "learning_rate": 0.0012982998454404945,
      "loss": 1.6727,
      "step": 3670
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.16684643924236298,
      "learning_rate": 0.0012936630602782071,
      "loss": 1.6887,
      "step": 3680
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.1676190346479416,
      "learning_rate": 0.0012890262751159196,
      "loss": 1.6431,
      "step": 3690
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.3145937919616699,
      "learning_rate": 0.0012843894899536323,
      "loss": 1.7087,
      "step": 3700
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.12223563343286514,
      "learning_rate": 0.0012797527047913447,
      "loss": 1.6727,
      "step": 3710
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.28433871269226074,
      "learning_rate": 0.0012751159196290572,
      "loss": 1.7053,
      "step": 3720
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.1400517076253891,
      "learning_rate": 0.0012704791344667697,
      "loss": 1.6353,
      "step": 3730
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.09192794561386108,
      "learning_rate": 0.0012658423493044821,
      "loss": 1.8053,
      "step": 3740
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.17726996541023254,
      "learning_rate": 0.0012612055641421948,
      "loss": 1.7043,
      "step": 3750
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.20208251476287842,
      "learning_rate": 0.0012565687789799073,
      "loss": 1.6121,
      "step": 3760
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.139785498380661,
      "learning_rate": 0.0012519319938176197,
      "loss": 1.6056,
      "step": 3770
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.20093344151973724,
      "learning_rate": 0.0012472952086553324,
      "loss": 1.6122,
      "step": 3780
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.22318120300769806,
      "learning_rate": 0.0012426584234930449,
      "loss": 1.639,
      "step": 3790
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.1496504247188568,
      "learning_rate": 0.0012380216383307573,
      "loss": 1.713,
      "step": 3800
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.16829971969127655,
      "learning_rate": 0.0012333848531684698,
      "loss": 1.6397,
      "step": 3810
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.20381760597229004,
      "learning_rate": 0.0012287480680061823,
      "loss": 1.6971,
      "step": 3820
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.4666798412799835,
      "learning_rate": 0.001224111282843895,
      "loss": 1.5942,
      "step": 3830
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.13633331656455994,
      "learning_rate": 0.0012194744976816074,
      "loss": 1.7435,
      "step": 3840
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.17887549102306366,
      "learning_rate": 0.00121483771251932,
      "loss": 1.7565,
      "step": 3850
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.15981636941432953,
      "learning_rate": 0.0012102009273570326,
      "loss": 1.6551,
      "step": 3860
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.22342528402805328,
      "learning_rate": 0.001205564142194745,
      "loss": 1.6669,
      "step": 3870
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.12596014142036438,
      "learning_rate": 0.0012009273570324575,
      "loss": 1.6927,
      "step": 3880
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.14123976230621338,
      "learning_rate": 0.00119629057187017,
      "loss": 1.735,
      "step": 3890
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.2683235704898834,
      "learning_rate": 0.0011916537867078826,
      "loss": 1.7502,
      "step": 3900
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.15745310485363007,
      "learning_rate": 0.001187017001545595,
      "loss": 1.6368,
      "step": 3910
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.12250125408172607,
      "learning_rate": 0.0011823802163833078,
      "loss": 1.6802,
      "step": 3920
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.10672224313020706,
      "learning_rate": 0.00117774343122102,
      "loss": 1.687,
      "step": 3930
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.2701374292373657,
      "learning_rate": 0.0011731066460587325,
      "loss": 1.5986,
      "step": 3940
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.2525327205657959,
      "learning_rate": 0.0011684698608964452,
      "loss": 1.5699,
      "step": 3950
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.21782484650611877,
      "learning_rate": 0.0011638330757341576,
      "loss": 1.5995,
      "step": 3960
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.15092657506465912,
      "learning_rate": 0.0011591962905718703,
      "loss": 1.7158,
      "step": 3970
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.10328491777181625,
      "learning_rate": 0.0011545595054095828,
      "loss": 1.752,
      "step": 3980
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.38835665583610535,
      "learning_rate": 0.0011499227202472952,
      "loss": 1.6549,
      "step": 3990
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.10276567190885544,
      "learning_rate": 0.0011452859350850077,
      "loss": 1.739,
      "step": 4000
    }
  ],
  "logging_steps": 10,
  "max_steps": 6470,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "total_flos": 1.8492756719026176e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
