{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9319938176197837,
  "eval_steps": 500,
  "global_step": 10000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0,
      "grad_norm": 0.32876673340797424,
      "learning_rate": 0.0029971020092735704,
      "loss": 2.3268,
      "step": 10
    },
    {
      "epoch": 0.0,
      "grad_norm": 0.2286442518234253,
      "learning_rate": 0.0029942040185471407,
      "loss": 2.0205,
      "step": 20
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.09092947095632553,
      "learning_rate": 0.002991306027820711,
      "loss": 1.841,
      "step": 30
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.1250830590724945,
      "learning_rate": 0.0029884080370942813,
      "loss": 2.0267,
      "step": 40
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.1740046590566635,
      "learning_rate": 0.0029855100463678516,
      "loss": 1.8933,
      "step": 50
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.1769358515739441,
      "learning_rate": 0.002982612055641422,
      "loss": 1.8578,
      "step": 60
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.1530272662639618,
      "learning_rate": 0.0029797140649149923,
      "loss": 1.9322,
      "step": 70
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.165969580411911,
      "learning_rate": 0.002976816074188563,
      "loss": 1.7619,
      "step": 80
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.14187468588352203,
      "learning_rate": 0.002973918083462133,
      "loss": 1.8866,
      "step": 90
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.14714093506336212,
      "learning_rate": 0.0029710200927357032,
      "loss": 1.7219,
      "step": 100
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.1383618712425232,
      "learning_rate": 0.0029681221020092735,
      "loss": 1.8621,
      "step": 110
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.12116587907075882,
      "learning_rate": 0.002965224111282844,
      "loss": 1.8653,
      "step": 120
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.1433546096086502,
      "learning_rate": 0.002962326120556414,
      "loss": 1.8987,
      "step": 130
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.15441852807998657,
      "learning_rate": 0.0029594281298299845,
      "loss": 1.8305,
      "step": 140
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.11361770331859589,
      "learning_rate": 0.0029565301391035552,
      "loss": 1.9387,
      "step": 150
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.15343838930130005,
      "learning_rate": 0.002953632148377125,
      "loss": 1.8352,
      "step": 160
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.16597488522529602,
      "learning_rate": 0.0029507341576506954,
      "loss": 1.7707,
      "step": 170
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.12915091216564178,
      "learning_rate": 0.0029478361669242658,
      "loss": 1.7521,
      "step": 180
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.18847627937793732,
      "learning_rate": 0.002944938176197836,
      "loss": 1.6677,
      "step": 190
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.22499041259288788,
      "learning_rate": 0.002942040185471407,
      "loss": 1.8109,
      "step": 200
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.26966413855552673,
      "learning_rate": 0.0029391421947449767,
      "loss": 1.8591,
      "step": 210
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.15142571926116943,
      "learning_rate": 0.0029362442040185475,
      "loss": 1.7198,
      "step": 220
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.11896826326847076,
      "learning_rate": 0.0029333462132921173,
      "loss": 1.8311,
      "step": 230
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.223640576004982,
      "learning_rate": 0.002930448222565688,
      "loss": 1.8057,
      "step": 240
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.10589566826820374,
      "learning_rate": 0.002927550231839258,
      "loss": 1.8216,
      "step": 250
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.18025274574756622,
      "learning_rate": 0.0029246522411128283,
      "loss": 1.7867,
      "step": 260
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.20588666200637817,
      "learning_rate": 0.002921754250386399,
      "loss": 1.7492,
      "step": 270
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.15948982536792755,
      "learning_rate": 0.002918856259659969,
      "loss": 1.7698,
      "step": 280
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.13207271695137024,
      "learning_rate": 0.0029159582689335397,
      "loss": 1.8455,
      "step": 290
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.10977022349834442,
      "learning_rate": 0.0029130602782071096,
      "loss": 1.7967,
      "step": 300
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.17917238175868988,
      "learning_rate": 0.0029101622874806803,
      "loss": 1.8587,
      "step": 310
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.12500137090682983,
      "learning_rate": 0.0029072642967542506,
      "loss": 1.8253,
      "step": 320
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.296869158744812,
      "learning_rate": 0.002904366306027821,
      "loss": 1.8319,
      "step": 330
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.07028663158416748,
      "learning_rate": 0.0029014683153013913,
      "loss": 1.7794,
      "step": 340
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.1608789563179016,
      "learning_rate": 0.002898570324574961,
      "loss": 1.6986,
      "step": 350
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.15470251441001892,
      "learning_rate": 0.002895672333848532,
      "loss": 1.7498,
      "step": 360
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.08976272493600845,
      "learning_rate": 0.0028927743431221018,
      "loss": 1.7153,
      "step": 370
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.2598921060562134,
      "learning_rate": 0.0028898763523956725,
      "loss": 1.9827,
      "step": 380
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.18759171664714813,
      "learning_rate": 0.002886978361669243,
      "loss": 1.9005,
      "step": 390
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.4854089319705963,
      "learning_rate": 0.002884080370942813,
      "loss": 1.6831,
      "step": 400
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.1391519159078598,
      "learning_rate": 0.0028811823802163835,
      "loss": 1.8031,
      "step": 410
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.14901070296764374,
      "learning_rate": 0.002878284389489954,
      "loss": 1.72,
      "step": 420
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.1547454446554184,
      "learning_rate": 0.002875386398763524,
      "loss": 1.877,
      "step": 430
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.09401172399520874,
      "learning_rate": 0.0028724884080370944,
      "loss": 1.8013,
      "step": 440
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.22383448481559753,
      "learning_rate": 0.0028695904173106647,
      "loss": 1.7812,
      "step": 450
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.10985484719276428,
      "learning_rate": 0.002866692426584235,
      "loss": 1.5912,
      "step": 460
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.3403097689151764,
      "learning_rate": 0.0028637944358578054,
      "loss": 1.758,
      "step": 470
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.14941389858722687,
      "learning_rate": 0.0028608964451313757,
      "loss": 1.4998,
      "step": 480
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.15618279576301575,
      "learning_rate": 0.002857998454404946,
      "loss": 1.778,
      "step": 490
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.2845280170440674,
      "learning_rate": 0.0028551004636785163,
      "loss": 1.7512,
      "step": 500
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.1559150069952011,
      "learning_rate": 0.0028522024729520866,
      "loss": 1.7991,
      "step": 510
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.11655628681182861,
      "learning_rate": 0.002849304482225657,
      "loss": 1.6579,
      "step": 520
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.16595688462257385,
      "learning_rate": 0.0028464064914992273,
      "loss": 1.7152,
      "step": 530
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.32605838775634766,
      "learning_rate": 0.0028435085007727976,
      "loss": 1.8246,
      "step": 540
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.2293117493391037,
      "learning_rate": 0.002840610510046368,
      "loss": 1.7334,
      "step": 550
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.15891322493553162,
      "learning_rate": 0.0028377125193199382,
      "loss": 1.6848,
      "step": 560
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.12766288220882416,
      "learning_rate": 0.0028348145285935085,
      "loss": 1.7006,
      "step": 570
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.18853576481342316,
      "learning_rate": 0.002831916537867079,
      "loss": 1.6296,
      "step": 580
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.15328966081142426,
      "learning_rate": 0.002829018547140649,
      "loss": 1.7351,
      "step": 590
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.15471230447292328,
      "learning_rate": 0.0028261205564142195,
      "loss": 1.6823,
      "step": 600
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.22200188040733337,
      "learning_rate": 0.00282322256568779,
      "loss": 1.6464,
      "step": 610
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.19772979617118835,
      "learning_rate": 0.00282032457496136,
      "loss": 1.7786,
      "step": 620
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.13190728425979614,
      "learning_rate": 0.0028174265842349304,
      "loss": 1.7295,
      "step": 630
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.12490758299827576,
      "learning_rate": 0.0028145285935085008,
      "loss": 1.7283,
      "step": 640
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.21287988126277924,
      "learning_rate": 0.002811630602782071,
      "loss": 1.8118,
      "step": 650
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.32878461480140686,
      "learning_rate": 0.0028087326120556414,
      "loss": 1.7296,
      "step": 660
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.20657584071159363,
      "learning_rate": 0.002805834621329212,
      "loss": 1.9279,
      "step": 670
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.12901665270328522,
      "learning_rate": 0.002802936630602782,
      "loss": 1.8184,
      "step": 680
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.16413386166095734,
      "learning_rate": 0.0028000386398763523,
      "loss": 1.7078,
      "step": 690
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.18352127075195312,
      "learning_rate": 0.0027971406491499227,
      "loss": 1.6685,
      "step": 700
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.2627735435962677,
      "learning_rate": 0.002794242658423493,
      "loss": 1.7345,
      "step": 710
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.34345999360084534,
      "learning_rate": 0.0027913446676970637,
      "loss": 1.7302,
      "step": 720
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.12599140405654907,
      "learning_rate": 0.0027884466769706336,
      "loss": 1.6206,
      "step": 730
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.12032537907361984,
      "learning_rate": 0.0027855486862442044,
      "loss": 1.7747,
      "step": 740
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.22600652277469635,
      "learning_rate": 0.0027826506955177742,
      "loss": 1.7285,
      "step": 750
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.22041799128055573,
      "learning_rate": 0.0027797527047913446,
      "loss": 1.6057,
      "step": 760
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.21425749361515045,
      "learning_rate": 0.002776854714064915,
      "loss": 1.8302,
      "step": 770
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.17348316311836243,
      "learning_rate": 0.002773956723338485,
      "loss": 1.7021,
      "step": 780
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.20556782186031342,
      "learning_rate": 0.002771058732612056,
      "loss": 1.7465,
      "step": 790
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.3553626239299774,
      "learning_rate": 0.002768160741885626,
      "loss": 1.9215,
      "step": 800
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.21586941182613373,
      "learning_rate": 0.0027652627511591966,
      "loss": 1.7447,
      "step": 810
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.43559911847114563,
      "learning_rate": 0.0027623647604327665,
      "loss": 1.8559,
      "step": 820
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.30800166726112366,
      "learning_rate": 0.002759466769706337,
      "loss": 1.6256,
      "step": 830
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.09463637322187424,
      "learning_rate": 0.0027565687789799075,
      "loss": 1.6143,
      "step": 840
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.24242250621318817,
      "learning_rate": 0.0027536707882534774,
      "loss": 1.7262,
      "step": 850
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.16631512343883514,
      "learning_rate": 0.002750772797527048,
      "loss": 1.7605,
      "step": 860
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.19168002903461456,
      "learning_rate": 0.002747874806800618,
      "loss": 1.7372,
      "step": 870
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.14930127561092377,
      "learning_rate": 0.002744976816074189,
      "loss": 1.6746,
      "step": 880
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.21093223989009857,
      "learning_rate": 0.0027420788253477587,
      "loss": 1.7618,
      "step": 890
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.1436319500207901,
      "learning_rate": 0.0027391808346213294,
      "loss": 1.6761,
      "step": 900
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.2920990288257599,
      "learning_rate": 0.0027362828438948997,
      "loss": 1.9423,
      "step": 910
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.20001305639743805,
      "learning_rate": 0.00273338485316847,
      "loss": 1.6424,
      "step": 920
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.15564413368701935,
      "learning_rate": 0.0027304868624420404,
      "loss": 1.6898,
      "step": 930
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.09404236078262329,
      "learning_rate": 0.0027275888717156103,
      "loss": 1.7888,
      "step": 940
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.21865282952785492,
      "learning_rate": 0.002724690880989181,
      "loss": 1.7228,
      "step": 950
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.2009102702140808,
      "learning_rate": 0.0027217928902627513,
      "loss": 1.8549,
      "step": 960
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.16208134591579437,
      "learning_rate": 0.0027188948995363216,
      "loss": 1.6556,
      "step": 970
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.4588562548160553,
      "learning_rate": 0.002715996908809892,
      "loss": 1.6426,
      "step": 980
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.3243612051010132,
      "learning_rate": 0.0027130989180834623,
      "loss": 1.7154,
      "step": 990
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.2315530925989151,
      "learning_rate": 0.0027102009273570326,
      "loss": 1.7007,
      "step": 1000
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.3378889858722687,
      "learning_rate": 0.0027073029366306025,
      "loss": 1.8045,
      "step": 1010
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.1644841581583023,
      "learning_rate": 0.0027044049459041732,
      "loss": 1.6927,
      "step": 1020
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.2497849464416504,
      "learning_rate": 0.0027015069551777435,
      "loss": 1.6338,
      "step": 1030
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.2259538173675537,
      "learning_rate": 0.002698608964451314,
      "loss": 1.6981,
      "step": 1040
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.39462849497795105,
      "learning_rate": 0.002695710973724884,
      "loss": 1.6527,
      "step": 1050
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.15004685521125793,
      "learning_rate": 0.0026928129829984545,
      "loss": 1.823,
      "step": 1060
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.1598026007413864,
      "learning_rate": 0.002689914992272025,
      "loss": 1.8756,
      "step": 1070
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.11631391942501068,
      "learning_rate": 0.002687017001545595,
      "loss": 1.6602,
      "step": 1080
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.2864421308040619,
      "learning_rate": 0.0026841190108191654,
      "loss": 1.6851,
      "step": 1090
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.19436302781105042,
      "learning_rate": 0.0026812210200927358,
      "loss": 1.7159,
      "step": 1100
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.20727196335792542,
      "learning_rate": 0.002678323029366306,
      "loss": 1.8046,
      "step": 1110
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.2828081548213959,
      "learning_rate": 0.0026754250386398764,
      "loss": 1.6734,
      "step": 1120
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.2284664660692215,
      "learning_rate": 0.0026725270479134467,
      "loss": 1.6368,
      "step": 1130
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.1495916247367859,
      "learning_rate": 0.002669629057187017,
      "loss": 1.7869,
      "step": 1140
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.2123148888349533,
      "learning_rate": 0.0026667310664605873,
      "loss": 1.902,
      "step": 1150
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.23058082163333893,
      "learning_rate": 0.0026638330757341577,
      "loss": 1.7238,
      "step": 1160
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.2754021883010864,
      "learning_rate": 0.002660935085007728,
      "loss": 1.7628,
      "step": 1170
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.35184407234191895,
      "learning_rate": 0.0026580370942812983,
      "loss": 1.8382,
      "step": 1180
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.21586139500141144,
      "learning_rate": 0.0026551391035548686,
      "loss": 1.724,
      "step": 1190
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.11233588308095932,
      "learning_rate": 0.002652241112828439,
      "loss": 1.7182,
      "step": 1200
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.15161029994487762,
      "learning_rate": 0.0026493431221020092,
      "loss": 1.6853,
      "step": 1210
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.22021768987178802,
      "learning_rate": 0.0026464451313755796,
      "loss": 1.6892,
      "step": 1220
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.18965581059455872,
      "learning_rate": 0.00264354714064915,
      "loss": 1.6945,
      "step": 1230
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.15019825100898743,
      "learning_rate": 0.0026406491499227206,
      "loss": 1.5706,
      "step": 1240
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.16640621423721313,
      "learning_rate": 0.0026377511591962905,
      "loss": 1.6717,
      "step": 1250
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.18681903183460236,
      "learning_rate": 0.0026348531684698613,
      "loss": 1.7188,
      "step": 1260
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.20463323593139648,
      "learning_rate": 0.002631955177743431,
      "loss": 1.7869,
      "step": 1270
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.31012868881225586,
      "learning_rate": 0.0026290571870170015,
      "loss": 1.7061,
      "step": 1280
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.22095869481563568,
      "learning_rate": 0.0026261591962905718,
      "loss": 1.7392,
      "step": 1290
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.36887940764427185,
      "learning_rate": 0.002623261205564142,
      "loss": 1.7761,
      "step": 1300
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.1136542409658432,
      "learning_rate": 0.002620363214837713,
      "loss": 1.6562,
      "step": 1310
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.26891157031059265,
      "learning_rate": 0.0026174652241112827,
      "loss": 1.6688,
      "step": 1320
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.11437585949897766,
      "learning_rate": 0.0026145672333848535,
      "loss": 1.7742,
      "step": 1330
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.15444378554821014,
      "learning_rate": 0.0026116692426584234,
      "loss": 1.7665,
      "step": 1340
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.2305990606546402,
      "learning_rate": 0.0026087712519319937,
      "loss": 1.67,
      "step": 1350
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.15504075586795807,
      "learning_rate": 0.0026058732612055644,
      "loss": 1.6624,
      "step": 1360
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.16037745773792267,
      "learning_rate": 0.0026029752704791343,
      "loss": 1.579,
      "step": 1370
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.22213460505008698,
      "learning_rate": 0.002600077279752705,
      "loss": 1.7254,
      "step": 1380
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.31401142477989197,
      "learning_rate": 0.002597179289026275,
      "loss": 1.669,
      "step": 1390
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.33966898918151855,
      "learning_rate": 0.0025942812982998457,
      "loss": 1.7943,
      "step": 1400
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.2728850841522217,
      "learning_rate": 0.0025913833075734156,
      "loss": 1.8371,
      "step": 1410
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.28309166431427,
      "learning_rate": 0.0025884853168469863,
      "loss": 1.5729,
      "step": 1420
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.14292491972446442,
      "learning_rate": 0.0025855873261205566,
      "loss": 1.6942,
      "step": 1430
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.27935585379600525,
      "learning_rate": 0.0025826893353941265,
      "loss": 1.6682,
      "step": 1440
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.14284934103488922,
      "learning_rate": 0.0025797913446676973,
      "loss": 1.6487,
      "step": 1450
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.1870577484369278,
      "learning_rate": 0.002576893353941267,
      "loss": 1.7227,
      "step": 1460
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.3525446057319641,
      "learning_rate": 0.002573995363214838,
      "loss": 1.7795,
      "step": 1470
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.17609505355358124,
      "learning_rate": 0.0025710973724884082,
      "loss": 1.7904,
      "step": 1480
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.2241760641336441,
      "learning_rate": 0.0025681993817619785,
      "loss": 1.5795,
      "step": 1490
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.2663746774196625,
      "learning_rate": 0.002565301391035549,
      "loss": 1.8814,
      "step": 1500
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.21549668908119202,
      "learning_rate": 0.002562403400309119,
      "loss": 1.6762,
      "step": 1510
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.12485943734645844,
      "learning_rate": 0.0025595054095826895,
      "loss": 1.5788,
      "step": 1520
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.2568577527999878,
      "learning_rate": 0.0025566074188562594,
      "loss": 1.61,
      "step": 1530
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.20741796493530273,
      "learning_rate": 0.00255370942812983,
      "loss": 1.6772,
      "step": 1540
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.27522602677345276,
      "learning_rate": 0.0025508114374034004,
      "loss": 1.6997,
      "step": 1550
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.3206414580345154,
      "learning_rate": 0.0025479134466769708,
      "loss": 1.6468,
      "step": 1560
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.42145419120788574,
      "learning_rate": 0.002545015455950541,
      "loss": 1.4983,
      "step": 1570
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.1473386138677597,
      "learning_rate": 0.0025421174652241114,
      "loss": 1.651,
      "step": 1580
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.20300354063510895,
      "learning_rate": 0.0025392194744976817,
      "loss": 1.6898,
      "step": 1590
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.31612393260002136,
      "learning_rate": 0.002536321483771252,
      "loss": 1.7802,
      "step": 1600
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.21786555647850037,
      "learning_rate": 0.0025334234930448223,
      "loss": 1.8129,
      "step": 1610
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.3086621165275574,
      "learning_rate": 0.0025305255023183927,
      "loss": 1.718,
      "step": 1620
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.5280370712280273,
      "learning_rate": 0.002527627511591963,
      "loss": 1.8033,
      "step": 1630
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.18864260613918304,
      "learning_rate": 0.0025247295208655333,
      "loss": 1.7792,
      "step": 1640
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.134832963347435,
      "learning_rate": 0.0025218315301391036,
      "loss": 1.6668,
      "step": 1650
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.26113882660865784,
      "learning_rate": 0.002518933539412674,
      "loss": 1.7087,
      "step": 1660
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.26879310607910156,
      "learning_rate": 0.0025160355486862442,
      "loss": 1.6259,
      "step": 1670
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.2292596697807312,
      "learning_rate": 0.0025131375579598146,
      "loss": 1.703,
      "step": 1680
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.4080394506454468,
      "learning_rate": 0.002510239567233385,
      "loss": 1.557,
      "step": 1690
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.44835537672042847,
      "learning_rate": 0.002507341576506955,
      "loss": 1.7523,
      "step": 1700
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.16027626395225525,
      "learning_rate": 0.0025044435857805255,
      "loss": 1.6801,
      "step": 1710
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.38599473237991333,
      "learning_rate": 0.002501545595054096,
      "loss": 1.6712,
      "step": 1720
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.39108020067214966,
      "learning_rate": 0.002498647604327666,
      "loss": 1.7313,
      "step": 1730
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.3044987618923187,
      "learning_rate": 0.0024957496136012365,
      "loss": 1.8914,
      "step": 1740
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.32991594076156616,
      "learning_rate": 0.0024928516228748068,
      "loss": 1.6874,
      "step": 1750
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.277590274810791,
      "learning_rate": 0.0024899536321483775,
      "loss": 1.7314,
      "step": 1760
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.27364689111709595,
      "learning_rate": 0.0024870556414219474,
      "loss": 1.6894,
      "step": 1770
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.23698624968528748,
      "learning_rate": 0.0024841576506955177,
      "loss": 1.7818,
      "step": 1780
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.40450650453567505,
      "learning_rate": 0.002481259659969088,
      "loss": 1.8133,
      "step": 1790
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.22919267416000366,
      "learning_rate": 0.0024783616692426584,
      "loss": 1.681,
      "step": 1800
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.18261481821537018,
      "learning_rate": 0.0024754636785162287,
      "loss": 1.708,
      "step": 1810
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.3132842183113098,
      "learning_rate": 0.002472565687789799,
      "loss": 1.5901,
      "step": 1820
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.32656753063201904,
      "learning_rate": 0.0024696676970633697,
      "loss": 1.7246,
      "step": 1830
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.1863078474998474,
      "learning_rate": 0.0024667697063369396,
      "loss": 1.7807,
      "step": 1840
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.13914313912391663,
      "learning_rate": 0.00246387171561051,
      "loss": 1.6655,
      "step": 1850
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.13727647066116333,
      "learning_rate": 0.0024609737248840803,
      "loss": 1.6626,
      "step": 1860
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.4151228964328766,
      "learning_rate": 0.0024580757341576506,
      "loss": 1.736,
      "step": 1870
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.4870697259902954,
      "learning_rate": 0.0024551777434312213,
      "loss": 1.6108,
      "step": 1880
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.3282456398010254,
      "learning_rate": 0.002452279752704791,
      "loss": 1.6971,
      "step": 1890
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.19117259979248047,
      "learning_rate": 0.002449381761978362,
      "loss": 1.7479,
      "step": 1900
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.13856084644794464,
      "learning_rate": 0.002446483771251932,
      "loss": 1.6676,
      "step": 1910
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.2359079271554947,
      "learning_rate": 0.0024435857805255026,
      "loss": 1.6013,
      "step": 1920
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.3734150230884552,
      "learning_rate": 0.0024406877897990725,
      "loss": 1.6467,
      "step": 1930
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.15876667201519012,
      "learning_rate": 0.002437789799072643,
      "loss": 1.6952,
      "step": 1940
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.18256047368049622,
      "learning_rate": 0.0024348918083462135,
      "loss": 1.6658,
      "step": 1950
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.18281085789203644,
      "learning_rate": 0.0024319938176197834,
      "loss": 1.6885,
      "step": 1960
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.19141313433647156,
      "learning_rate": 0.002429095826893354,
      "loss": 1.6306,
      "step": 1970
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.24538835883140564,
      "learning_rate": 0.002426197836166924,
      "loss": 1.7779,
      "step": 1980
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.3200352191925049,
      "learning_rate": 0.002423299845440495,
      "loss": 1.7311,
      "step": 1990
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.2785976231098175,
      "learning_rate": 0.002420401854714065,
      "loss": 1.6333,
      "step": 2000
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.1943909078836441,
      "learning_rate": 0.0024175038639876354,
      "loss": 1.6093,
      "step": 2010
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.2574034035205841,
      "learning_rate": 0.0024146058732612058,
      "loss": 1.8008,
      "step": 2020
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.3381669223308563,
      "learning_rate": 0.0024117078825347756,
      "loss": 1.7412,
      "step": 2030
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.6214108467102051,
      "learning_rate": 0.0024088098918083464,
      "loss": 1.7161,
      "step": 2040
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.2550256848335266,
      "learning_rate": 0.0024059119010819163,
      "loss": 1.6507,
      "step": 2050
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.2835811972618103,
      "learning_rate": 0.002403013910355487,
      "loss": 1.6245,
      "step": 2060
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.2195117324590683,
      "learning_rate": 0.0024001159196290573,
      "loss": 1.7768,
      "step": 2070
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.28187331557273865,
      "learning_rate": 0.0023972179289026277,
      "loss": 1.7692,
      "step": 2080
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.18294771015644073,
      "learning_rate": 0.002394319938176198,
      "loss": 1.8866,
      "step": 2090
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.15383094549179077,
      "learning_rate": 0.0023914219474497683,
      "loss": 1.5405,
      "step": 2100
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.2570042312145233,
      "learning_rate": 0.0023885239567233386,
      "loss": 1.5743,
      "step": 2110
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.36335113644599915,
      "learning_rate": 0.002385625965996909,
      "loss": 1.7301,
      "step": 2120
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.23044471442699432,
      "learning_rate": 0.0023827279752704792,
      "loss": 1.6031,
      "step": 2130
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.25676608085632324,
      "learning_rate": 0.0023798299845440496,
      "loss": 1.6749,
      "step": 2140
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.3109188675880432,
      "learning_rate": 0.00237693199381762,
      "loss": 1.6883,
      "step": 2150
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.20523950457572937,
      "learning_rate": 0.00237403400309119,
      "loss": 1.6556,
      "step": 2160
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.24774055182933807,
      "learning_rate": 0.0023711360123647605,
      "loss": 1.7151,
      "step": 2170
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.2590191960334778,
      "learning_rate": 0.002368238021638331,
      "loss": 1.8209,
      "step": 2180
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.22251515090465546,
      "learning_rate": 0.002365340030911901,
      "loss": 1.59,
      "step": 2190
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.15933777391910553,
      "learning_rate": 0.0023624420401854715,
      "loss": 1.581,
      "step": 2200
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.18932509422302246,
      "learning_rate": 0.0023595440494590418,
      "loss": 1.7503,
      "step": 2210
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.2452549785375595,
      "learning_rate": 0.002356646058732612,
      "loss": 1.6488,
      "step": 2220
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.30703800916671753,
      "learning_rate": 0.0023537480680061824,
      "loss": 1.6998,
      "step": 2230
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.219762921333313,
      "learning_rate": 0.0023508500772797527,
      "loss": 1.7476,
      "step": 2240
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.21492186188697815,
      "learning_rate": 0.002347952086553323,
      "loss": 1.6958,
      "step": 2250
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.3184072971343994,
      "learning_rate": 0.0023450540958268934,
      "loss": 1.7942,
      "step": 2260
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.2210620939731598,
      "learning_rate": 0.0023421561051004637,
      "loss": 1.5695,
      "step": 2270
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.17990334331989288,
      "learning_rate": 0.002339258114374034,
      "loss": 1.5422,
      "step": 2280
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.30467867851257324,
      "learning_rate": 0.0023363601236476043,
      "loss": 1.6541,
      "step": 2290
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.26530763506889343,
      "learning_rate": 0.0023334621329211746,
      "loss": 1.737,
      "step": 2300
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.30822739005088806,
      "learning_rate": 0.002330564142194745,
      "loss": 1.7402,
      "step": 2310
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.265848845243454,
      "learning_rate": 0.0023276661514683153,
      "loss": 1.6195,
      "step": 2320
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.4811626076698303,
      "learning_rate": 0.002324768160741886,
      "loss": 1.7393,
      "step": 2330
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.2383442223072052,
      "learning_rate": 0.002321870170015456,
      "loss": 1.5941,
      "step": 2340
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.556446373462677,
      "learning_rate": 0.0023189721792890266,
      "loss": 1.6212,
      "step": 2350
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.15951934456825256,
      "learning_rate": 0.0023160741885625965,
      "loss": 1.5984,
      "step": 2360
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.30612713098526,
      "learning_rate": 0.002313176197836167,
      "loss": 1.6621,
      "step": 2370
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.29649367928504944,
      "learning_rate": 0.002310278207109737,
      "loss": 1.8373,
      "step": 2380
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.28999853134155273,
      "learning_rate": 0.0023073802163833075,
      "loss": 1.6391,
      "step": 2390
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.28858473896980286,
      "learning_rate": 0.0023044822256568782,
      "loss": 1.7788,
      "step": 2400
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.18884333968162537,
      "learning_rate": 0.002301584234930448,
      "loss": 1.7432,
      "step": 2410
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.20600265264511108,
      "learning_rate": 0.002298686244204019,
      "loss": 1.7134,
      "step": 2420
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.1992437243461609,
      "learning_rate": 0.0022957882534775887,
      "loss": 1.6677,
      "step": 2430
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.34283745288848877,
      "learning_rate": 0.002292890262751159,
      "loss": 1.7957,
      "step": 2440
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.31241679191589355,
      "learning_rate": 0.00228999227202473,
      "loss": 1.8691,
      "step": 2450
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.21188056468963623,
      "learning_rate": 0.0022870942812982997,
      "loss": 1.6242,
      "step": 2460
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.24645769596099854,
      "learning_rate": 0.0022841962905718704,
      "loss": 1.6253,
      "step": 2470
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.29004818201065063,
      "learning_rate": 0.0022812982998454403,
      "loss": 1.6822,
      "step": 2480
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.21691292524337769,
      "learning_rate": 0.002278400309119011,
      "loss": 1.6227,
      "step": 2490
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.21976451575756073,
      "learning_rate": 0.002275502318392581,
      "loss": 1.6878,
      "step": 2500
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.3849089443683624,
      "learning_rate": 0.0022726043276661517,
      "loss": 1.5985,
      "step": 2510
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.26093176007270813,
      "learning_rate": 0.002269706336939722,
      "loss": 1.7058,
      "step": 2520
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.18229542672634125,
      "learning_rate": 0.002266808346213292,
      "loss": 1.7103,
      "step": 2530
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.19332900643348694,
      "learning_rate": 0.0022639103554868627,
      "loss": 1.7577,
      "step": 2540
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.14542540907859802,
      "learning_rate": 0.0022610123647604325,
      "loss": 1.7108,
      "step": 2550
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.33067354559898376,
      "learning_rate": 0.0022581143740340033,
      "loss": 1.6105,
      "step": 2560
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.29112040996551514,
      "learning_rate": 0.0022552163833075736,
      "loss": 1.704,
      "step": 2570
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.19613204896450043,
      "learning_rate": 0.002252318392581144,
      "loss": 1.6789,
      "step": 2580
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.7281059622764587,
      "learning_rate": 0.0022494204018547142,
      "loss": 1.6724,
      "step": 2590
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.2035171538591385,
      "learning_rate": 0.0022465224111282846,
      "loss": 1.5933,
      "step": 2600
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.19748008251190186,
      "learning_rate": 0.002243624420401855,
      "loss": 1.6951,
      "step": 2610
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.18422627449035645,
      "learning_rate": 0.0022407264296754248,
      "loss": 1.69,
      "step": 2620
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.2535536587238312,
      "learning_rate": 0.0022378284389489955,
      "loss": 1.5568,
      "step": 2630
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.29016774892807007,
      "learning_rate": 0.002234930448222566,
      "loss": 1.5731,
      "step": 2640
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.1496671885251999,
      "learning_rate": 0.002232032457496136,
      "loss": 1.5917,
      "step": 2650
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.26247119903564453,
      "learning_rate": 0.0022291344667697065,
      "loss": 1.6159,
      "step": 2660
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.25557592511177063,
      "learning_rate": 0.0022262364760432768,
      "loss": 1.6342,
      "step": 2670
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.19172556698322296,
      "learning_rate": 0.002223338485316847,
      "loss": 1.7256,
      "step": 2680
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.27465277910232544,
      "learning_rate": 0.0022204404945904174,
      "loss": 1.66,
      "step": 2690
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.24417874217033386,
      "learning_rate": 0.0022175425038639877,
      "loss": 1.7016,
      "step": 2700
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.15401574969291687,
      "learning_rate": 0.002214644513137558,
      "loss": 1.809,
      "step": 2710
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.33166787028312683,
      "learning_rate": 0.0022117465224111284,
      "loss": 1.6816,
      "step": 2720
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.2503820061683655,
      "learning_rate": 0.0022088485316846987,
      "loss": 1.6001,
      "step": 2730
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.24871417880058289,
      "learning_rate": 0.002205950540958269,
      "loss": 1.5959,
      "step": 2740
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.21457867324352264,
      "learning_rate": 0.0022030525502318393,
      "loss": 1.6992,
      "step": 2750
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.3250669836997986,
      "learning_rate": 0.0022001545595054096,
      "loss": 1.7005,
      "step": 2760
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.17970550060272217,
      "learning_rate": 0.00219725656877898,
      "loss": 1.7136,
      "step": 2770
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.2673308253288269,
      "learning_rate": 0.0021943585780525503,
      "loss": 1.536,
      "step": 2780
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.43040141463279724,
      "learning_rate": 0.0021914605873261206,
      "loss": 1.7088,
      "step": 2790
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.1152317002415657,
      "learning_rate": 0.002188562596599691,
      "loss": 1.6583,
      "step": 2800
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.17377479374408722,
      "learning_rate": 0.002185664605873261,
      "loss": 1.669,
      "step": 2810
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.2320980727672577,
      "learning_rate": 0.0021827666151468315,
      "loss": 1.7083,
      "step": 2820
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.20371702313423157,
      "learning_rate": 0.002179868624420402,
      "loss": 1.636,
      "step": 2830
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.20856687426567078,
      "learning_rate": 0.002176970633693972,
      "loss": 1.6861,
      "step": 2840
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.47369471192359924,
      "learning_rate": 0.002174072642967543,
      "loss": 1.7038,
      "step": 2850
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.32992902398109436,
      "learning_rate": 0.002171174652241113,
      "loss": 1.6726,
      "step": 2860
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.18743500113487244,
      "learning_rate": 0.002168276661514683,
      "loss": 1.6597,
      "step": 2870
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.2849077582359314,
      "learning_rate": 0.0021653786707882534,
      "loss": 1.6497,
      "step": 2880
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.5466490387916565,
      "learning_rate": 0.0021624806800618237,
      "loss": 1.6834,
      "step": 2890
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.2406885325908661,
      "learning_rate": 0.002159582689335394,
      "loss": 1.6556,
      "step": 2900
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.13525733351707458,
      "learning_rate": 0.0021566846986089644,
      "loss": 1.7035,
      "step": 2910
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.22263368964195251,
      "learning_rate": 0.002153786707882535,
      "loss": 1.8403,
      "step": 2920
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.2447434812784195,
      "learning_rate": 0.002150888717156105,
      "loss": 1.7676,
      "step": 2930
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.24046429991722107,
      "learning_rate": 0.0021479907264296758,
      "loss": 1.6559,
      "step": 2940
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.22574180364608765,
      "learning_rate": 0.0021450927357032456,
      "loss": 1.5864,
      "step": 2950
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.1920219361782074,
      "learning_rate": 0.002142194744976816,
      "loss": 1.7918,
      "step": 2960
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.27831652760505676,
      "learning_rate": 0.0021392967542503867,
      "loss": 1.7349,
      "step": 2970
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.18308010697364807,
      "learning_rate": 0.0021363987635239566,
      "loss": 1.6449,
      "step": 2980
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.31729334592819214,
      "learning_rate": 0.0021335007727975273,
      "loss": 1.7145,
      "step": 2990
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.2050798088312149,
      "learning_rate": 0.0021306027820710972,
      "loss": 1.721,
      "step": 3000
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.2651529908180237,
      "learning_rate": 0.002127704791344668,
      "loss": 1.6575,
      "step": 3010
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.3661412298679352,
      "learning_rate": 0.002124806800618238,
      "loss": 1.7662,
      "step": 3020
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.2153230458498001,
      "learning_rate": 0.002121908809891808,
      "loss": 1.6641,
      "step": 3030
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.20978787541389465,
      "learning_rate": 0.002119010819165379,
      "loss": 1.6783,
      "step": 3040
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.2791019082069397,
      "learning_rate": 0.002116112828438949,
      "loss": 1.854,
      "step": 3050
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.255357950925827,
      "learning_rate": 0.0021132148377125196,
      "loss": 1.6253,
      "step": 3060
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.19657474756240845,
      "learning_rate": 0.0021103168469860894,
      "loss": 1.6991,
      "step": 3070
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.1855277121067047,
      "learning_rate": 0.00210741885625966,
      "loss": 1.5756,
      "step": 3080
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.5349098443984985,
      "learning_rate": 0.0021045208655332305,
      "loss": 1.7018,
      "step": 3090
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.3730393648147583,
      "learning_rate": 0.002101622874806801,
      "loss": 1.6477,
      "step": 3100
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.32079869508743286,
      "learning_rate": 0.002098724884080371,
      "loss": 1.6503,
      "step": 3110
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.1760714203119278,
      "learning_rate": 0.002095826893353941,
      "loss": 1.6643,
      "step": 3120
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.21495357155799866,
      "learning_rate": 0.0020929289026275118,
      "loss": 1.8339,
      "step": 3130
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.2829347550868988,
      "learning_rate": 0.0020900309119010817,
      "loss": 1.6451,
      "step": 3140
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.2359134703874588,
      "learning_rate": 0.0020871329211746524,
      "loss": 1.6642,
      "step": 3150
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.35208404064178467,
      "learning_rate": 0.0020842349304482227,
      "loss": 1.6205,
      "step": 3160
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.2238345444202423,
      "learning_rate": 0.002081336939721793,
      "loss": 1.6704,
      "step": 3170
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.38963544368743896,
      "learning_rate": 0.0020784389489953634,
      "loss": 1.6604,
      "step": 3180
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.16691699624061584,
      "learning_rate": 0.0020755409582689337,
      "loss": 1.7828,
      "step": 3190
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.22718475759029388,
      "learning_rate": 0.002072642967542504,
      "loss": 1.7065,
      "step": 3200
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.4821971356868744,
      "learning_rate": 0.0020697449768160743,
      "loss": 1.7187,
      "step": 3210
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.31168514490127563,
      "learning_rate": 0.0020668469860896446,
      "loss": 1.6646,
      "step": 3220
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.2872549593448639,
      "learning_rate": 0.002063948995363215,
      "loss": 1.869,
      "step": 3230
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.2406676709651947,
      "learning_rate": 0.0020610510046367853,
      "loss": 1.6189,
      "step": 3240
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.1955055594444275,
      "learning_rate": 0.0020581530139103556,
      "loss": 1.6556,
      "step": 3250
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.29304367303848267,
      "learning_rate": 0.002055255023183926,
      "loss": 1.5795,
      "step": 3260
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.21634501218795776,
      "learning_rate": 0.002052357032457496,
      "loss": 1.7934,
      "step": 3270
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.26917606592178345,
      "learning_rate": 0.0020494590417310665,
      "loss": 1.627,
      "step": 3280
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.22040162980556488,
      "learning_rate": 0.002046561051004637,
      "loss": 1.7082,
      "step": 3290
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.2649768888950348,
      "learning_rate": 0.002043663060278207,
      "loss": 1.8668,
      "step": 3300
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.3572404086589813,
      "learning_rate": 0.0020407650695517775,
      "loss": 1.6693,
      "step": 3310
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.2545929551124573,
      "learning_rate": 0.002037867078825348,
      "loss": 1.5051,
      "step": 3320
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.6119149923324585,
      "learning_rate": 0.002034969088098918,
      "loss": 1.6702,
      "step": 3330
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.1939893662929535,
      "learning_rate": 0.0020320710973724884,
      "loss": 1.7117,
      "step": 3340
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.24802114069461823,
      "learning_rate": 0.0020291731066460587,
      "loss": 1.6529,
      "step": 3350
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.24869076907634735,
      "learning_rate": 0.002026275115919629,
      "loss": 1.6348,
      "step": 3360
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.34022510051727295,
      "learning_rate": 0.0020233771251931994,
      "loss": 1.6087,
      "step": 3370
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.30113616585731506,
      "learning_rate": 0.0020204791344667697,
      "loss": 1.891,
      "step": 3380
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.3152991235256195,
      "learning_rate": 0.00201758114374034,
      "loss": 1.6376,
      "step": 3390
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.15229804813861847,
      "learning_rate": 0.0020146831530139103,
      "loss": 1.8066,
      "step": 3400
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.28357577323913574,
      "learning_rate": 0.0020117851622874806,
      "loss": 1.6787,
      "step": 3410
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.32419124245643616,
      "learning_rate": 0.002008887171561051,
      "loss": 1.719,
      "step": 3420
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.30393311381340027,
      "learning_rate": 0.0020059891808346213,
      "loss": 1.6148,
      "step": 3430
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.27487486600875854,
      "learning_rate": 0.002003091190108192,
      "loss": 1.7282,
      "step": 3440
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.18642164766788483,
      "learning_rate": 0.002000193199381762,
      "loss": 1.6612,
      "step": 3450
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.1858203262090683,
      "learning_rate": 0.0019972952086553322,
      "loss": 1.5679,
      "step": 3460
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.1744309365749359,
      "learning_rate": 0.0019943972179289025,
      "loss": 1.5431,
      "step": 3470
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.23882469534873962,
      "learning_rate": 0.001991499227202473,
      "loss": 1.751,
      "step": 3480
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.2286844253540039,
      "learning_rate": 0.0019886012364760436,
      "loss": 1.6416,
      "step": 3490
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.27884408831596375,
      "learning_rate": 0.0019857032457496135,
      "loss": 1.6926,
      "step": 3500
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.24410957098007202,
      "learning_rate": 0.0019828052550231842,
      "loss": 1.7418,
      "step": 3510
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.28100481629371643,
      "learning_rate": 0.001979907264296754,
      "loss": 1.6332,
      "step": 3520
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.29929471015930176,
      "learning_rate": 0.0019770092735703244,
      "loss": 1.6136,
      "step": 3530
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.2542932331562042,
      "learning_rate": 0.0019741112828438948,
      "loss": 1.7202,
      "step": 3540
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.1551925539970398,
      "learning_rate": 0.001971213292117465,
      "loss": 1.6544,
      "step": 3550
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.8006027340888977,
      "learning_rate": 0.001968315301391036,
      "loss": 1.9108,
      "step": 3560
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.2677581012248993,
      "learning_rate": 0.0019654173106646057,
      "loss": 1.5323,
      "step": 3570
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.24799612164497375,
      "learning_rate": 0.0019625193199381765,
      "loss": 1.8521,
      "step": 3580
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.15705516934394836,
      "learning_rate": 0.0019596213292117463,
      "loss": 1.5858,
      "step": 3590
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.24026255309581757,
      "learning_rate": 0.001956723338485317,
      "loss": 1.7008,
      "step": 3600
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.22312571108341217,
      "learning_rate": 0.0019538253477588874,
      "loss": 1.6882,
      "step": 3610
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.1733396202325821,
      "learning_rate": 0.0019509273570324573,
      "loss": 1.7612,
      "step": 3620
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.23449201881885529,
      "learning_rate": 0.0019480293663060278,
      "loss": 1.7431,
      "step": 3630
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.3364650011062622,
      "learning_rate": 0.0019451313755795981,
      "loss": 1.6528,
      "step": 3640
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.2742888033390045,
      "learning_rate": 0.0019422333848531687,
      "loss": 1.6337,
      "step": 3650
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.4541037082672119,
      "learning_rate": 0.0019393353941267388,
      "loss": 1.5696,
      "step": 3660
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.35670292377471924,
      "learning_rate": 0.0019364374034003093,
      "loss": 1.8717,
      "step": 3670
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.20941799879074097,
      "learning_rate": 0.0019335394126738794,
      "loss": 1.5606,
      "step": 3680
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.3306344449520111,
      "learning_rate": 0.00193064142194745,
      "loss": 1.6782,
      "step": 3690
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.33458569645881653,
      "learning_rate": 0.00192774343122102,
      "loss": 1.6732,
      "step": 3700
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.210732102394104,
      "learning_rate": 0.0019248454404945904,
      "loss": 1.8055,
      "step": 3710
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.2485242486000061,
      "learning_rate": 0.0019219474497681609,
      "loss": 1.6845,
      "step": 3720
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.9439730048179626,
      "learning_rate": 0.001919049459041731,
      "loss": 1.6469,
      "step": 3730
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.2825937271118164,
      "learning_rate": 0.0019161514683153015,
      "loss": 1.7691,
      "step": 3740
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.18216125667095184,
      "learning_rate": 0.0019132534775888716,
      "loss": 1.6599,
      "step": 3750
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.24957339465618134,
      "learning_rate": 0.0019103554868624422,
      "loss": 1.7063,
      "step": 3760
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.3010319173336029,
      "learning_rate": 0.0019074574961360125,
      "loss": 1.6124,
      "step": 3770
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.21442823112010956,
      "learning_rate": 0.0019045595054095828,
      "loss": 1.9048,
      "step": 3780
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.21331152319908142,
      "learning_rate": 0.001901661514683153,
      "loss": 1.6782,
      "step": 3790
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.25773143768310547,
      "learning_rate": 0.0018987635239567232,
      "loss": 1.6277,
      "step": 3800
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.18961122632026672,
      "learning_rate": 0.0018958655332302937,
      "loss": 1.6039,
      "step": 3810
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.20593583583831787,
      "learning_rate": 0.0018929675425038638,
      "loss": 1.6577,
      "step": 3820
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.2750979959964752,
      "learning_rate": 0.0018900695517774344,
      "loss": 1.7533,
      "step": 3830
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.1681860387325287,
      "learning_rate": 0.0018871715610510047,
      "loss": 1.7725,
      "step": 3840
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.3055609464645386,
      "learning_rate": 0.0018842735703245752,
      "loss": 1.7679,
      "step": 3850
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.5881098508834839,
      "learning_rate": 0.0018813755795981453,
      "loss": 1.6423,
      "step": 3860
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.25464776158332825,
      "learning_rate": 0.0018784775888717154,
      "loss": 1.6483,
      "step": 3870
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.4215536415576935,
      "learning_rate": 0.001875579598145286,
      "loss": 1.6639,
      "step": 3880
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.2589356601238251,
      "learning_rate": 0.0018726816074188563,
      "loss": 1.7403,
      "step": 3890
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.18377676606178284,
      "learning_rate": 0.0018697836166924266,
      "loss": 1.6696,
      "step": 3900
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.1942884624004364,
      "learning_rate": 0.001866885625965997,
      "loss": 1.5345,
      "step": 3910
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.3188963532447815,
      "learning_rate": 0.0018639876352395674,
      "loss": 1.6313,
      "step": 3920
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.15980727970600128,
      "learning_rate": 0.0018610896445131375,
      "loss": 1.6257,
      "step": 3930
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.2776991128921509,
      "learning_rate": 0.001858191653786708,
      "loss": 1.6369,
      "step": 3940
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.24526973068714142,
      "learning_rate": 0.0018552936630602782,
      "loss": 1.6858,
      "step": 3950
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.17285053431987762,
      "learning_rate": 0.0018523956723338485,
      "loss": 1.6234,
      "step": 3960
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.2158391922712326,
      "learning_rate": 0.001849497681607419,
      "loss": 1.5533,
      "step": 3970
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.3329746723175049,
      "learning_rate": 0.0018465996908809891,
      "loss": 1.5306,
      "step": 3980
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.19671528041362762,
      "learning_rate": 0.0018437017001545597,
      "loss": 1.4882,
      "step": 3990
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.1866525560617447,
      "learning_rate": 0.0018408037094281298,
      "loss": 1.5984,
      "step": 4000
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.23658527433872223,
      "learning_rate": 0.0018379057187017003,
      "loss": 1.6097,
      "step": 4010
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.16310375928878784,
      "learning_rate": 0.0018350077279752704,
      "loss": 1.7003,
      "step": 4020
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.3179527819156647,
      "learning_rate": 0.001832109737248841,
      "loss": 1.6318,
      "step": 4030
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.12550359964370728,
      "learning_rate": 0.0018292117465224112,
      "loss": 1.6028,
      "step": 4040
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.21847103536128998,
      "learning_rate": 0.0018263137557959813,
      "loss": 1.5731,
      "step": 4050
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.25917211174964905,
      "learning_rate": 0.0018234157650695519,
      "loss": 1.6472,
      "step": 4060
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.3290086090564728,
      "learning_rate": 0.001820517774343122,
      "loss": 1.6201,
      "step": 4070
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.28363126516342163,
      "learning_rate": 0.0018176197836166925,
      "loss": 1.5605,
      "step": 4080
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.20582671463489532,
      "learning_rate": 0.0018147217928902628,
      "loss": 1.6407,
      "step": 4090
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.22935080528259277,
      "learning_rate": 0.0018118238021638334,
      "loss": 1.6247,
      "step": 4100
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.21818891167640686,
      "learning_rate": 0.0018089258114374035,
      "loss": 1.6526,
      "step": 4110
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.16079813241958618,
      "learning_rate": 0.0018060278207109736,
      "loss": 1.6467,
      "step": 4120
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.3226242661476135,
      "learning_rate": 0.001803129829984544,
      "loss": 1.5833,
      "step": 4130
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.2189464569091797,
      "learning_rate": 0.0018002318392581142,
      "loss": 1.4564,
      "step": 4140
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.22659556567668915,
      "learning_rate": 0.0017973338485316847,
      "loss": 1.6667,
      "step": 4150
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.22244387865066528,
      "learning_rate": 0.001794435857805255,
      "loss": 1.6979,
      "step": 4160
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.3146996796131134,
      "learning_rate": 0.0017915378670788256,
      "loss": 1.8323,
      "step": 4170
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.15111055970191956,
      "learning_rate": 0.0017886398763523957,
      "loss": 1.8148,
      "step": 4180
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.1625906080007553,
      "learning_rate": 0.0017857418856259662,
      "loss": 1.6182,
      "step": 4190
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.21928393840789795,
      "learning_rate": 0.0017828438948995363,
      "loss": 1.6033,
      "step": 4200
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.24729911983013153,
      "learning_rate": 0.0017799459041731066,
      "loss": 1.7774,
      "step": 4210
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.45570874214172363,
      "learning_rate": 0.001777047913446677,
      "loss": 1.6429,
      "step": 4220
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.4667363464832306,
      "learning_rate": 0.0017741499227202473,
      "loss": 1.7009,
      "step": 4230
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.41820093989372253,
      "learning_rate": 0.0017712519319938178,
      "loss": 1.7218,
      "step": 4240
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.2293250411748886,
      "learning_rate": 0.0017683539412673879,
      "loss": 1.7258,
      "step": 4250
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.29344111680984497,
      "learning_rate": 0.0017654559505409584,
      "loss": 1.7419,
      "step": 4260
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.301442950963974,
      "learning_rate": 0.0017625579598145285,
      "loss": 1.6821,
      "step": 4270
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.5751182436943054,
      "learning_rate": 0.001759659969088099,
      "loss": 1.628,
      "step": 4280
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.15260837972164154,
      "learning_rate": 0.0017567619783616694,
      "loss": 1.7271,
      "step": 4290
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.2715487778186798,
      "learning_rate": 0.0017538639876352395,
      "loss": 1.6968,
      "step": 4300
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.5261834859848022,
      "learning_rate": 0.00175096599690881,
      "loss": 1.5621,
      "step": 4310
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.2956831157207489,
      "learning_rate": 0.00174806800618238,
      "loss": 1.7452,
      "step": 4320
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.2567105293273926,
      "learning_rate": 0.0017451700154559506,
      "loss": 1.5295,
      "step": 4330
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.48275208473205566,
      "learning_rate": 0.0017422720247295207,
      "loss": 1.7429,
      "step": 4340
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.24650922417640686,
      "learning_rate": 0.0017393740340030913,
      "loss": 1.5648,
      "step": 4350
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.26427361369132996,
      "learning_rate": 0.0017364760432766616,
      "loss": 1.7829,
      "step": 4360
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.33767396211624146,
      "learning_rate": 0.0017335780525502317,
      "loss": 1.5545,
      "step": 4370
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.2918432652950287,
      "learning_rate": 0.0017306800618238022,
      "loss": 1.6281,
      "step": 4380
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.2976169288158417,
      "learning_rate": 0.0017277820710973723,
      "loss": 1.5579,
      "step": 4390
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.24709568917751312,
      "learning_rate": 0.0017248840803709429,
      "loss": 1.537,
      "step": 4400
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.29128849506378174,
      "learning_rate": 0.0017219860896445132,
      "loss": 1.6476,
      "step": 4410
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.5086513161659241,
      "learning_rate": 0.0017190880989180837,
      "loss": 1.5886,
      "step": 4420
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.20599235594272614,
      "learning_rate": 0.0017161901081916538,
      "loss": 1.7159,
      "step": 4430
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.21256199479103088,
      "learning_rate": 0.0017132921174652243,
      "loss": 1.6739,
      "step": 4440
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.26494815945625305,
      "learning_rate": 0.0017103941267387944,
      "loss": 1.5793,
      "step": 4450
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.2678658068180084,
      "learning_rate": 0.0017074961360123645,
      "loss": 1.656,
      "step": 4460
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.262018084526062,
      "learning_rate": 0.001704598145285935,
      "loss": 1.6328,
      "step": 4470
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.3976218104362488,
      "learning_rate": 0.0017017001545595054,
      "loss": 1.6708,
      "step": 4480
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.2458704262971878,
      "learning_rate": 0.001698802163833076,
      "loss": 1.6615,
      "step": 4490
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.25954943895339966,
      "learning_rate": 0.001695904173106646,
      "loss": 1.6219,
      "step": 4500
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.32631492614746094,
      "learning_rate": 0.0016930061823802166,
      "loss": 1.7085,
      "step": 4510
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.1628546416759491,
      "learning_rate": 0.0016901081916537867,
      "loss": 1.796,
      "step": 4520
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.3929605185985565,
      "learning_rate": 0.0016872102009273572,
      "loss": 1.6506,
      "step": 4530
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.35737600922584534,
      "learning_rate": 0.0016843122102009275,
      "loss": 1.6313,
      "step": 4540
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.8338249325752258,
      "learning_rate": 0.0016814142194744976,
      "loss": 1.804,
      "step": 4550
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.40402793884277344,
      "learning_rate": 0.0016785162287480681,
      "loss": 1.674,
      "step": 4560
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.3139020502567291,
      "learning_rate": 0.0016756182380216382,
      "loss": 1.7206,
      "step": 4570
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.2648477852344513,
      "learning_rate": 0.0016727202472952088,
      "loss": 1.6505,
      "step": 4580
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.2643379271030426,
      "learning_rate": 0.0016698222565687789,
      "loss": 1.5615,
      "step": 4590
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.23554760217666626,
      "learning_rate": 0.0016669242658423494,
      "loss": 1.6354,
      "step": 4600
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.17253847420215607,
      "learning_rate": 0.0016640262751159197,
      "loss": 1.6314,
      "step": 4610
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.3335855007171631,
      "learning_rate": 0.0016611282843894903,
      "loss": 1.6544,
      "step": 4620
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.1924777328968048,
      "learning_rate": 0.0016582302936630604,
      "loss": 1.6953,
      "step": 4630
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.2606380879878998,
      "learning_rate": 0.0016553323029366305,
      "loss": 1.5399,
      "step": 4640
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.29351043701171875,
      "learning_rate": 0.001652434312210201,
      "loss": 1.7043,
      "step": 4650
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.25137490034103394,
      "learning_rate": 0.0016495363214837713,
      "loss": 1.5314,
      "step": 4660
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.1862151175737381,
      "learning_rate": 0.0016466383307573416,
      "loss": 1.6042,
      "step": 4670
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.2205187976360321,
      "learning_rate": 0.001643740340030912,
      "loss": 1.7376,
      "step": 4680
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.31166884303092957,
      "learning_rate": 0.0016408423493044825,
      "loss": 1.6259,
      "step": 4690
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.19434751570224762,
      "learning_rate": 0.0016379443585780526,
      "loss": 1.6361,
      "step": 4700
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.20114386081695557,
      "learning_rate": 0.0016350463678516227,
      "loss": 1.7628,
      "step": 4710
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.18321985006332397,
      "learning_rate": 0.0016321483771251932,
      "loss": 1.7309,
      "step": 4720
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.22743909060955048,
      "learning_rate": 0.0016292503863987635,
      "loss": 1.6757,
      "step": 4730
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.21527895331382751,
      "learning_rate": 0.001626352395672334,
      "loss": 1.8069,
      "step": 4740
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.19745109975337982,
      "learning_rate": 0.0016234544049459042,
      "loss": 1.625,
      "step": 4750
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.15384191274642944,
      "learning_rate": 0.0016205564142194747,
      "loss": 1.6246,
      "step": 4760
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.1681683212518692,
      "learning_rate": 0.0016176584234930448,
      "loss": 1.5446,
      "step": 4770
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.15725083649158478,
      "learning_rate": 0.0016147604327666153,
      "loss": 1.5518,
      "step": 4780
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.37381672859191895,
      "learning_rate": 0.0016118624420401854,
      "loss": 1.673,
      "step": 4790
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.3019605875015259,
      "learning_rate": 0.0016089644513137557,
      "loss": 1.7179,
      "step": 4800
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.1972401887178421,
      "learning_rate": 0.0016060664605873263,
      "loss": 1.5606,
      "step": 4810
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.28197506070137024,
      "learning_rate": 0.0016031684698608964,
      "loss": 1.6787,
      "step": 4820
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.16762912273406982,
      "learning_rate": 0.001600270479134467,
      "loss": 1.6541,
      "step": 4830
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.35203903913497925,
      "learning_rate": 0.001597372488408037,
      "loss": 1.5984,
      "step": 4840
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.275341272354126,
      "learning_rate": 0.0015944744976816075,
      "loss": 1.5544,
      "step": 4850
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.1504753977060318,
      "learning_rate": 0.0015915765069551779,
      "loss": 1.6148,
      "step": 4860
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.2329685240983963,
      "learning_rate": 0.0015886785162287482,
      "loss": 1.516,
      "step": 4870
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.4113372266292572,
      "learning_rate": 0.0015857805255023185,
      "loss": 1.6089,
      "step": 4880
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.14296357333660126,
      "learning_rate": 0.0015828825347758886,
      "loss": 1.6754,
      "step": 4890
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.23401488363742828,
      "learning_rate": 0.0015799845440494591,
      "loss": 1.7614,
      "step": 4900
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.31203383207321167,
      "learning_rate": 0.0015770865533230292,
      "loss": 1.63,
      "step": 4910
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.3308892548084259,
      "learning_rate": 0.0015741885625965998,
      "loss": 1.5866,
      "step": 4920
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.3842521905899048,
      "learning_rate": 0.00157129057187017,
      "loss": 1.6398,
      "step": 4930
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.22781836986541748,
      "learning_rate": 0.0015683925811437406,
      "loss": 1.6803,
      "step": 4940
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.2685161232948303,
      "learning_rate": 0.0015654945904173107,
      "loss": 1.6282,
      "step": 4950
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.23098860681056976,
      "learning_rate": 0.0015625965996908808,
      "loss": 1.5327,
      "step": 4960
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.20419460535049438,
      "learning_rate": 0.0015596986089644513,
      "loss": 1.8265,
      "step": 4970
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.16696825623512268,
      "learning_rate": 0.0015568006182380217,
      "loss": 1.5053,
      "step": 4980
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.2503381371498108,
      "learning_rate": 0.001553902627511592,
      "loss": 1.7064,
      "step": 4990
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.2891048491001129,
      "learning_rate": 0.0015510046367851623,
      "loss": 1.6716,
      "step": 5000
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.3331651985645294,
      "learning_rate": 0.0015481066460587328,
      "loss": 1.5207,
      "step": 5010
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.2254379391670227,
      "learning_rate": 0.001545208655332303,
      "loss": 1.5078,
      "step": 5020
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.16178511083126068,
      "learning_rate": 0.0015423106646058735,
      "loss": 1.6722,
      "step": 5030
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.35671985149383545,
      "learning_rate": 0.0015394126738794436,
      "loss": 1.695,
      "step": 5040
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.2772288918495178,
      "learning_rate": 0.0015365146831530139,
      "loss": 1.5921,
      "step": 5050
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.39767855405807495,
      "learning_rate": 0.0015336166924265844,
      "loss": 1.6645,
      "step": 5060
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.29869920015335083,
      "learning_rate": 0.0015307187017001545,
      "loss": 1.5835,
      "step": 5070
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.14445099234580994,
      "learning_rate": 0.001527820710973725,
      "loss": 1.6067,
      "step": 5080
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.2179243266582489,
      "learning_rate": 0.0015249227202472951,
      "loss": 1.6664,
      "step": 5090
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.24262209236621857,
      "learning_rate": 0.0015220247295208657,
      "loss": 1.5486,
      "step": 5100
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.20524096488952637,
      "learning_rate": 0.0015191267387944358,
      "loss": 1.5766,
      "step": 5110
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.23879407346248627,
      "learning_rate": 0.0015162287480680063,
      "loss": 1.6454,
      "step": 5120
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.31623128056526184,
      "learning_rate": 0.0015133307573415766,
      "loss": 1.7081,
      "step": 5130
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.17740149796009064,
      "learning_rate": 0.0015104327666151467,
      "loss": 1.6442,
      "step": 5140
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.2653811275959015,
      "learning_rate": 0.0015075347758887173,
      "loss": 1.8059,
      "step": 5150
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.28346607089042664,
      "learning_rate": 0.0015046367851622874,
      "loss": 1.8314,
      "step": 5160
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.31336328387260437,
      "learning_rate": 0.0015017387944358579,
      "loss": 1.6133,
      "step": 5170
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.739008903503418,
      "eval_runtime": 4657.6005,
      "eval_samples_per_second": 2.223,
      "eval_steps_per_second": 0.278,
      "step": 5176
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.16710929572582245,
      "learning_rate": 0.0014988408037094282,
      "loss": 1.6081,
      "step": 5180
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.19382880628108978,
      "learning_rate": 0.0014959428129829985,
      "loss": 1.6068,
      "step": 5190
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.29760852456092834,
      "learning_rate": 0.0014930448222565688,
      "loss": 1.5815,
      "step": 5200
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.20761215686798096,
      "learning_rate": 0.0014901468315301392,
      "loss": 1.4786,
      "step": 5210
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.15274754166603088,
      "learning_rate": 0.0014872488408037095,
      "loss": 1.5261,
      "step": 5220
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.2121644765138626,
      "learning_rate": 0.0014843508500772798,
      "loss": 1.4543,
      "step": 5230
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.3609199821949005,
      "learning_rate": 0.00148145285935085,
      "loss": 1.5001,
      "step": 5240
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.18782547116279602,
      "learning_rate": 0.0014785548686244204,
      "loss": 1.5227,
      "step": 5250
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.1338544487953186,
      "learning_rate": 0.0014756568778979907,
      "loss": 1.4951,
      "step": 5260
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.2007717341184616,
      "learning_rate": 0.001472758887171561,
      "loss": 1.6361,
      "step": 5270
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.1877211332321167,
      "learning_rate": 0.0014698608964451314,
      "loss": 1.5879,
      "step": 5280
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.17796489596366882,
      "learning_rate": 0.0014669629057187017,
      "loss": 1.4523,
      "step": 5290
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.26400187611579895,
      "learning_rate": 0.001464064914992272,
      "loss": 1.5098,
      "step": 5300
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.16644218564033508,
      "learning_rate": 0.0014611669242658423,
      "loss": 1.5484,
      "step": 5310
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.15952126681804657,
      "learning_rate": 0.0014582689335394129,
      "loss": 1.3817,
      "step": 5320
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.1805364489555359,
      "learning_rate": 0.0014553709428129832,
      "loss": 1.4302,
      "step": 5330
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.24431324005126953,
      "learning_rate": 0.0014524729520865533,
      "loss": 1.4942,
      "step": 5340
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.2456492930650711,
      "learning_rate": 0.0014495749613601236,
      "loss": 1.5521,
      "step": 5350
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.2440434992313385,
      "learning_rate": 0.001446676970633694,
      "loss": 1.4654,
      "step": 5360
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.2304479479789734,
      "learning_rate": 0.0014437789799072642,
      "loss": 1.5084,
      "step": 5370
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.21535338461399078,
      "learning_rate": 0.0014408809891808348,
      "loss": 1.4294,
      "step": 5380
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.2053097039461136,
      "learning_rate": 0.001437982998454405,
      "loss": 1.7315,
      "step": 5390
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.33702558279037476,
      "learning_rate": 0.0014350850077279754,
      "loss": 1.5552,
      "step": 5400
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.3174591362476349,
      "learning_rate": 0.0014321870170015457,
      "loss": 1.4963,
      "step": 5410
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.22980347275733948,
      "learning_rate": 0.0014292890262751158,
      "loss": 1.4284,
      "step": 5420
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.17783387005329132,
      "learning_rate": 0.0014263910355486861,
      "loss": 1.4517,
      "step": 5430
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.13442076742649078,
      "learning_rate": 0.0014234930448222567,
      "loss": 1.5303,
      "step": 5440
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.30477315187454224,
      "learning_rate": 0.001420595054095827,
      "loss": 1.4588,
      "step": 5450
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.1442832499742508,
      "learning_rate": 0.0014176970633693973,
      "loss": 1.4113,
      "step": 5460
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.3460938036441803,
      "learning_rate": 0.0014147990726429676,
      "loss": 1.5397,
      "step": 5470
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.2989896535873413,
      "learning_rate": 0.001411901081916538,
      "loss": 1.4624,
      "step": 5480
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.15988576412200928,
      "learning_rate": 0.0014090030911901082,
      "loss": 1.493,
      "step": 5490
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.26652318239212036,
      "learning_rate": 0.0014061051004636786,
      "loss": 1.4304,
      "step": 5500
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.46619468927383423,
      "learning_rate": 0.0014032071097372489,
      "loss": 1.5073,
      "step": 5510
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.22218400239944458,
      "learning_rate": 0.0014003091190108192,
      "loss": 1.5418,
      "step": 5520
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.2100156992673874,
      "learning_rate": 0.0013974111282843895,
      "loss": 1.6792,
      "step": 5530
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.2667834758758545,
      "learning_rate": 0.0013945131375579598,
      "loss": 1.563,
      "step": 5540
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.20854319632053375,
      "learning_rate": 0.0013916151468315301,
      "loss": 1.5547,
      "step": 5550
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.39202818274497986,
      "learning_rate": 0.0013887171561051005,
      "loss": 1.5298,
      "step": 5560
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.23251427710056305,
      "learning_rate": 0.0013858191653786708,
      "loss": 1.4638,
      "step": 5570
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.4166489839553833,
      "learning_rate": 0.0013829211746522413,
      "loss": 1.4957,
      "step": 5580
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.2429089993238449,
      "learning_rate": 0.0013800231839258114,
      "loss": 1.4667,
      "step": 5590
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.22367393970489502,
      "learning_rate": 0.0013771251931993817,
      "loss": 1.5987,
      "step": 5600
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.23172134160995483,
      "learning_rate": 0.001374227202472952,
      "loss": 1.4711,
      "step": 5610
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.20583054423332214,
      "learning_rate": 0.0013713292117465224,
      "loss": 1.6299,
      "step": 5620
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.18835203349590302,
      "learning_rate": 0.0013684312210200927,
      "loss": 1.565,
      "step": 5630
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.24356912076473236,
      "learning_rate": 0.0013655332302936632,
      "loss": 1.4638,
      "step": 5640
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.31159672141075134,
      "learning_rate": 0.0013626352395672335,
      "loss": 1.5611,
      "step": 5650
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.254083514213562,
      "learning_rate": 0.0013597372488408038,
      "loss": 1.5318,
      "step": 5660
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.2533279061317444,
      "learning_rate": 0.0013568392581143742,
      "loss": 1.5005,
      "step": 5670
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.17526109516620636,
      "learning_rate": 0.0013539412673879443,
      "loss": 1.4024,
      "step": 5680
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.7599300742149353,
      "learning_rate": 0.0013510432766615146,
      "loss": 1.5092,
      "step": 5690
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.25070852041244507,
      "learning_rate": 0.001348145285935085,
      "loss": 1.5276,
      "step": 5700
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.2801817059516907,
      "learning_rate": 0.0013452472952086554,
      "loss": 1.4482,
      "step": 5710
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.24934156239032745,
      "learning_rate": 0.0013423493044822257,
      "loss": 1.4743,
      "step": 5720
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.23014721274375916,
      "learning_rate": 0.001339451313755796,
      "loss": 1.581,
      "step": 5730
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.30860432982444763,
      "learning_rate": 0.0013365533230293664,
      "loss": 1.5579,
      "step": 5740
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.28144317865371704,
      "learning_rate": 0.0013336553323029367,
      "loss": 1.4483,
      "step": 5750
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.18005575239658356,
      "learning_rate": 0.001330757341576507,
      "loss": 1.4966,
      "step": 5760
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.6038385629653931,
      "learning_rate": 0.0013278593508500773,
      "loss": 1.5341,
      "step": 5770
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.27775514125823975,
      "learning_rate": 0.0013249613601236476,
      "loss": 1.4783,
      "step": 5780
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.1752295047044754,
      "learning_rate": 0.001322063369397218,
      "loss": 1.447,
      "step": 5790
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.1713954657316208,
      "learning_rate": 0.0013191653786707883,
      "loss": 1.4018,
      "step": 5800
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.3492087721824646,
      "learning_rate": 0.0013162673879443586,
      "loss": 1.5437,
      "step": 5810
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.15660221874713898,
      "learning_rate": 0.001313369397217929,
      "loss": 1.5724,
      "step": 5820
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.15124501287937164,
      "learning_rate": 0.0013104714064914992,
      "loss": 1.5128,
      "step": 5830
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.21566689014434814,
      "learning_rate": 0.0013075734157650695,
      "loss": 1.5818,
      "step": 5840
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.22319850325584412,
      "learning_rate": 0.0013046754250386398,
      "loss": 1.5056,
      "step": 5850
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.2003381848335266,
      "learning_rate": 0.0013017774343122102,
      "loss": 1.5147,
      "step": 5860
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.302062451839447,
      "learning_rate": 0.0012988794435857805,
      "loss": 1.5016,
      "step": 5870
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.27068156003952026,
      "learning_rate": 0.0012959814528593508,
      "loss": 1.5284,
      "step": 5880
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.12969611585140228,
      "learning_rate": 0.0012930834621329211,
      "loss": 1.5476,
      "step": 5890
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.20879997313022614,
      "learning_rate": 0.0012901854714064916,
      "loss": 1.5249,
      "step": 5900
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.3139457404613495,
      "learning_rate": 0.001287287480680062,
      "loss": 1.4782,
      "step": 5910
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.2999454140663147,
      "learning_rate": 0.0012843894899536323,
      "loss": 1.444,
      "step": 5920
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.22804823517799377,
      "learning_rate": 0.0012814914992272024,
      "loss": 1.5593,
      "step": 5930
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.22633662819862366,
      "learning_rate": 0.0012785935085007727,
      "loss": 1.5474,
      "step": 5940
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.2392260581254959,
      "learning_rate": 0.001275695517774343,
      "loss": 1.5693,
      "step": 5950
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.20673607289791107,
      "learning_rate": 0.0012727975270479135,
      "loss": 1.2936,
      "step": 5960
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.3017018437385559,
      "learning_rate": 0.0012698995363214839,
      "loss": 1.5544,
      "step": 5970
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.18666917085647583,
      "learning_rate": 0.0012670015455950542,
      "loss": 1.4888,
      "step": 5980
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.28382182121276855,
      "learning_rate": 0.0012641035548686245,
      "loss": 1.6087,
      "step": 5990
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.25148794054985046,
      "learning_rate": 0.0012612055641421948,
      "loss": 1.4782,
      "step": 6000
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.238485187292099,
      "learning_rate": 0.001258307573415765,
      "loss": 1.5305,
      "step": 6010
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.1882355660200119,
      "learning_rate": 0.0012554095826893354,
      "loss": 1.5124,
      "step": 6020
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.2216668576002121,
      "learning_rate": 0.0012525115919629058,
      "loss": 1.5046,
      "step": 6030
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.3236182630062103,
      "learning_rate": 0.001249613601236476,
      "loss": 1.4324,
      "step": 6040
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.2609699070453644,
      "learning_rate": 0.0012467156105100464,
      "loss": 1.5139,
      "step": 6050
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.1420997977256775,
      "learning_rate": 0.0012438176197836167,
      "loss": 1.4582,
      "step": 6060
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.22682996094226837,
      "learning_rate": 0.001240919629057187,
      "loss": 1.5221,
      "step": 6070
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.25586390495300293,
      "learning_rate": 0.0012380216383307573,
      "loss": 1.5203,
      "step": 6080
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.25280189514160156,
      "learning_rate": 0.0012351236476043277,
      "loss": 1.5053,
      "step": 6090
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.18089967966079712,
      "learning_rate": 0.001232225656877898,
      "loss": 1.6538,
      "step": 6100
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.4716000258922577,
      "learning_rate": 0.0012293276661514683,
      "loss": 1.526,
      "step": 6110
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.26538458466529846,
      "learning_rate": 0.0012264296754250386,
      "loss": 1.5416,
      "step": 6120
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.1950240135192871,
      "learning_rate": 0.001223531684698609,
      "loss": 1.643,
      "step": 6130
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.30742159485816956,
      "learning_rate": 0.0012206336939721792,
      "loss": 1.4611,
      "step": 6140
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.22798441350460052,
      "learning_rate": 0.0012177357032457496,
      "loss": 1.4819,
      "step": 6150
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.38748055696487427,
      "learning_rate": 0.00121483771251932,
      "loss": 1.5978,
      "step": 6160
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.22894495725631714,
      "learning_rate": 0.0012119397217928904,
      "loss": 1.5923,
      "step": 6170
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.31796225905418396,
      "learning_rate": 0.0012090417310664605,
      "loss": 1.497,
      "step": 6180
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.14697036147117615,
      "learning_rate": 0.0012061437403400308,
      "loss": 1.4391,
      "step": 6190
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.3961874544620514,
      "learning_rate": 0.0012032457496136011,
      "loss": 1.5383,
      "step": 6200
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.27188655734062195,
      "learning_rate": 0.0012003477588871715,
      "loss": 1.5738,
      "step": 6210
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.23887722194194794,
      "learning_rate": 0.001197449768160742,
      "loss": 1.4998,
      "step": 6220
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.25036555528640747,
      "learning_rate": 0.0011945517774343123,
      "loss": 1.5049,
      "step": 6230
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.3606553375720978,
      "learning_rate": 0.0011916537867078826,
      "loss": 1.5167,
      "step": 6240
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.30491429567337036,
      "learning_rate": 0.001188755795981453,
      "loss": 1.6212,
      "step": 6250
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.3411105275154114,
      "learning_rate": 0.001185857805255023,
      "loss": 1.5013,
      "step": 6260
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.2899100184440613,
      "learning_rate": 0.0011829598145285934,
      "loss": 1.4049,
      "step": 6270
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.18087999522686005,
      "learning_rate": 0.001180061823802164,
      "loss": 1.4781,
      "step": 6280
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.3491039276123047,
      "learning_rate": 0.0011771638330757342,
      "loss": 1.5192,
      "step": 6290
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.1824312061071396,
      "learning_rate": 0.0011742658423493045,
      "loss": 1.5215,
      "step": 6300
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.3330855667591095,
      "learning_rate": 0.0011713678516228748,
      "loss": 1.5411,
      "step": 6310
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.30516645312309265,
      "learning_rate": 0.0011684698608964452,
      "loss": 1.4361,
      "step": 6320
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.27348122000694275,
      "learning_rate": 0.0011655718701700155,
      "loss": 1.4817,
      "step": 6330
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.30738988518714905,
      "learning_rate": 0.0011626738794435858,
      "loss": 1.5401,
      "step": 6340
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.2534078359603882,
      "learning_rate": 0.0011597758887171561,
      "loss": 1.5985,
      "step": 6350
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.4233766198158264,
      "learning_rate": 0.0011568778979907264,
      "loss": 1.4889,
      "step": 6360
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.3425494134426117,
      "learning_rate": 0.0011539799072642967,
      "loss": 1.496,
      "step": 6370
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.20382839441299438,
      "learning_rate": 0.001151081916537867,
      "loss": 1.4976,
      "step": 6380
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.1730310022830963,
      "learning_rate": 0.0011481839258114374,
      "loss": 1.596,
      "step": 6390
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.3755786418914795,
      "learning_rate": 0.0011452859350850077,
      "loss": 1.6086,
      "step": 6400
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.3297109305858612,
      "learning_rate": 0.0011423879443585782,
      "loss": 1.4383,
      "step": 6410
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.19196857511997223,
      "learning_rate": 0.0011394899536321485,
      "loss": 1.3992,
      "step": 6420
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.297500878572464,
      "learning_rate": 0.0011365919629057186,
      "loss": 1.5041,
      "step": 6430
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.5096255540847778,
      "learning_rate": 0.001133693972179289,
      "loss": 1.5199,
      "step": 6440
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.29056188464164734,
      "learning_rate": 0.0011307959814528593,
      "loss": 1.6649,
      "step": 6450
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.29933348298072815,
      "learning_rate": 0.0011278979907264296,
      "loss": 1.3654,
      "step": 6460
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.3114548623561859,
      "learning_rate": 0.0011250000000000001,
      "loss": 1.5531,
      "step": 6470
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.41482383012771606,
      "learning_rate": 0.0011221020092735704,
      "loss": 1.6132,
      "step": 6480
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.36030930280685425,
      "learning_rate": 0.0011192040185471408,
      "loss": 1.5919,
      "step": 6490
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.2457202672958374,
      "learning_rate": 0.001116306027820711,
      "loss": 1.632,
      "step": 6500
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.44531285762786865,
      "learning_rate": 0.0011134080370942814,
      "loss": 1.6491,
      "step": 6510
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.3705672323703766,
      "learning_rate": 0.0011105100463678515,
      "loss": 1.4886,
      "step": 6520
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.22037231922149658,
      "learning_rate": 0.0011076120556414218,
      "loss": 1.4429,
      "step": 6530
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.19123533368110657,
      "learning_rate": 0.0011047140649149923,
      "loss": 1.414,
      "step": 6540
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.34242936968803406,
      "learning_rate": 0.0011018160741885627,
      "loss": 1.6326,
      "step": 6550
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.2724582850933075,
      "learning_rate": 0.001098918083462133,
      "loss": 1.5408,
      "step": 6560
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.16174696385860443,
      "learning_rate": 0.0010960200927357033,
      "loss": 1.5251,
      "step": 6570
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.29060718417167664,
      "learning_rate": 0.0010931221020092736,
      "loss": 1.546,
      "step": 6580
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.25001904368400574,
      "learning_rate": 0.001090224111282844,
      "loss": 1.4759,
      "step": 6590
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.2027885764837265,
      "learning_rate": 0.0010873261205564142,
      "loss": 1.4818,
      "step": 6600
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.24821993708610535,
      "learning_rate": 0.0010844281298299846,
      "loss": 1.4593,
      "step": 6610
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.3393831253051758,
      "learning_rate": 0.0010815301391035549,
      "loss": 1.5362,
      "step": 6620
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.18794019520282745,
      "learning_rate": 0.0010786321483771252,
      "loss": 1.3891,
      "step": 6630
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.30010414123535156,
      "learning_rate": 0.0010757341576506955,
      "loss": 1.3952,
      "step": 6640
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.3883132338523865,
      "learning_rate": 0.0010728361669242658,
      "loss": 1.4964,
      "step": 6650
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.24824832379817963,
      "learning_rate": 0.0010699381761978361,
      "loss": 1.5565,
      "step": 6660
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.3117397129535675,
      "learning_rate": 0.0010670401854714067,
      "loss": 1.6247,
      "step": 6670
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.2384738177061081,
      "learning_rate": 0.001064142194744977,
      "loss": 1.4761,
      "step": 6680
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.23398642241954803,
      "learning_rate": 0.001061244204018547,
      "loss": 1.4743,
      "step": 6690
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.2806922495365143,
      "learning_rate": 0.0010583462132921174,
      "loss": 1.4318,
      "step": 6700
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.3229005038738251,
      "learning_rate": 0.0010554482225656877,
      "loss": 1.3872,
      "step": 6710
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.24848224222660065,
      "learning_rate": 0.001052550231839258,
      "loss": 1.5274,
      "step": 6720
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.3537907302379608,
      "learning_rate": 0.0010496522411128286,
      "loss": 1.5178,
      "step": 6730
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.2420070618391037,
      "learning_rate": 0.001046754250386399,
      "loss": 1.5117,
      "step": 6740
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.3589670956134796,
      "learning_rate": 0.0010438562596599692,
      "loss": 1.5867,
      "step": 6750
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.18885865807533264,
      "learning_rate": 0.0010409582689335395,
      "loss": 1.5236,
      "step": 6760
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.21674685180187225,
      "learning_rate": 0.0010380602782071096,
      "loss": 1.5232,
      "step": 6770
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.21558097004890442,
      "learning_rate": 0.00103516228748068,
      "loss": 1.5333,
      "step": 6780
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.27582547068595886,
      "learning_rate": 0.0010322642967542505,
      "loss": 1.486,
      "step": 6790
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.23409448564052582,
      "learning_rate": 0.0010293663060278208,
      "loss": 1.5648,
      "step": 6800
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.36100319027900696,
      "learning_rate": 0.0010264683153013911,
      "loss": 1.6032,
      "step": 6810
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.18147939443588257,
      "learning_rate": 0.0010235703245749614,
      "loss": 1.5419,
      "step": 6820
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.24108701944351196,
      "learning_rate": 0.0010206723338485317,
      "loss": 1.5163,
      "step": 6830
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.38885098695755005,
      "learning_rate": 0.001017774343122102,
      "loss": 1.4266,
      "step": 6840
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.29709988832473755,
      "learning_rate": 0.0010148763523956724,
      "loss": 1.5213,
      "step": 6850
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.37791216373443604,
      "learning_rate": 0.0010119783616692427,
      "loss": 1.4613,
      "step": 6860
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.37668678164482117,
      "learning_rate": 0.001009080370942813,
      "loss": 1.5359,
      "step": 6870
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.41951891779899597,
      "learning_rate": 0.0010061823802163833,
      "loss": 1.6349,
      "step": 6880
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.23353733122348785,
      "learning_rate": 0.0010032843894899536,
      "loss": 1.5079,
      "step": 6890
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.2533807158470154,
      "learning_rate": 0.001000386398763524,
      "loss": 1.5661,
      "step": 6900
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.21367383003234863,
      "learning_rate": 0.0009974884080370943,
      "loss": 1.5797,
      "step": 6910
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.3766079545021057,
      "learning_rate": 0.0009945904173106646,
      "loss": 1.5062,
      "step": 6920
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.21948757767677307,
      "learning_rate": 0.0009916924265842351,
      "loss": 1.4109,
      "step": 6930
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.36391720175743103,
      "learning_rate": 0.0009887944358578052,
      "loss": 1.4416,
      "step": 6940
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.33574721217155457,
      "learning_rate": 0.0009858964451313755,
      "loss": 1.6417,
      "step": 6950
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.3997417092323303,
      "learning_rate": 0.0009829984544049459,
      "loss": 1.429,
      "step": 6960
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.41321131587028503,
      "learning_rate": 0.0009801004636785162,
      "loss": 1.5629,
      "step": 6970
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.30602744221687317,
      "learning_rate": 0.0009772024729520865,
      "loss": 1.4197,
      "step": 6980
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.45502352714538574,
      "learning_rate": 0.0009743044822256569,
      "loss": 1.6169,
      "step": 6990
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.2745433449745178,
      "learning_rate": 0.0009714064914992272,
      "loss": 1.5054,
      "step": 7000
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.20370426774024963,
      "learning_rate": 0.0009685085007727977,
      "loss": 1.5022,
      "step": 7010
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.2962646782398224,
      "learning_rate": 0.0009656105100463678,
      "loss": 1.6402,
      "step": 7020
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.30127570033073425,
      "learning_rate": 0.0009627125193199382,
      "loss": 1.5253,
      "step": 7030
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.18422962725162506,
      "learning_rate": 0.0009598145285935085,
      "loss": 1.4524,
      "step": 7040
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.18437010049819946,
      "learning_rate": 0.0009569165378670788,
      "loss": 1.5891,
      "step": 7050
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.27727070450782776,
      "learning_rate": 0.0009540185471406491,
      "loss": 1.5037,
      "step": 7060
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.31304511427879333,
      "learning_rate": 0.0009511205564142196,
      "loss": 1.4056,
      "step": 7070
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.1505921185016632,
      "learning_rate": 0.0009482225656877899,
      "loss": 1.6692,
      "step": 7080
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.2753550112247467,
      "learning_rate": 0.0009453245749613602,
      "loss": 1.4567,
      "step": 7090
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.6310771107673645,
      "learning_rate": 0.0009424265842349305,
      "loss": 1.6317,
      "step": 7100
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.3368096351623535,
      "learning_rate": 0.0009395285935085007,
      "loss": 1.7131,
      "step": 7110
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.20194968581199646,
      "learning_rate": 0.000936630602782071,
      "loss": 1.4194,
      "step": 7120
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.21834607422351837,
      "learning_rate": 0.0009337326120556415,
      "loss": 1.4944,
      "step": 7130
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.27226731181144714,
      "learning_rate": 0.0009308346213292118,
      "loss": 1.4926,
      "step": 7140
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.19652681052684784,
      "learning_rate": 0.0009279366306027821,
      "loss": 1.5623,
      "step": 7150
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.3078966438770294,
      "learning_rate": 0.0009250386398763524,
      "loss": 1.5899,
      "step": 7160
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.22916853427886963,
      "learning_rate": 0.0009221406491499228,
      "loss": 1.4596,
      "step": 7170
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.25801312923431396,
      "learning_rate": 0.0009192426584234932,
      "loss": 1.5423,
      "step": 7180
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.42124733328819275,
      "learning_rate": 0.0009163446676970634,
      "loss": 1.5426,
      "step": 7190
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.27820342779159546,
      "learning_rate": 0.0009134466769706337,
      "loss": 1.6252,
      "step": 7200
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.24212686717510223,
      "learning_rate": 0.000910548686244204,
      "loss": 1.4717,
      "step": 7210
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.20463746786117554,
      "learning_rate": 0.0009076506955177743,
      "loss": 1.5865,
      "step": 7220
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.16391298174858093,
      "learning_rate": 0.0009047527047913447,
      "loss": 1.4525,
      "step": 7230
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.7561429142951965,
      "learning_rate": 0.0009018547140649151,
      "loss": 1.4803,
      "step": 7240
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.28943267464637756,
      "learning_rate": 0.0008989567233384854,
      "loss": 1.5324,
      "step": 7250
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.3579610288143158,
      "learning_rate": 0.0008960587326120557,
      "loss": 1.4834,
      "step": 7260
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.33495742082595825,
      "learning_rate": 0.0008931607418856259,
      "loss": 1.623,
      "step": 7270
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.2203809767961502,
      "learning_rate": 0.0008902627511591962,
      "loss": 1.6391,
      "step": 7280
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.25561684370040894,
      "learning_rate": 0.0008873647604327666,
      "loss": 1.5317,
      "step": 7290
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.14403100311756134,
      "learning_rate": 0.000884466769706337,
      "loss": 1.4434,
      "step": 7300
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.19278770685195923,
      "learning_rate": 0.0008815687789799073,
      "loss": 1.5056,
      "step": 7310
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.257517009973526,
      "learning_rate": 0.0008786707882534776,
      "loss": 1.5686,
      "step": 7320
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.2433503419160843,
      "learning_rate": 0.000875772797527048,
      "loss": 1.4529,
      "step": 7330
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.22845275700092316,
      "learning_rate": 0.0008728748068006183,
      "loss": 1.543,
      "step": 7340
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.4935228228569031,
      "learning_rate": 0.0008699768160741886,
      "loss": 1.6228,
      "step": 7350
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.15435920655727386,
      "learning_rate": 0.0008670788253477589,
      "loss": 1.7284,
      "step": 7360
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.30511319637298584,
      "learning_rate": 0.0008641808346213292,
      "loss": 1.482,
      "step": 7370
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.3272065222263336,
      "learning_rate": 0.0008612828438948995,
      "loss": 1.5175,
      "step": 7380
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.2450864464044571,
      "learning_rate": 0.0008583848531684699,
      "loss": 1.5046,
      "step": 7390
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.2536563575267792,
      "learning_rate": 0.0008554868624420402,
      "loss": 1.4854,
      "step": 7400
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.3235589563846588,
      "learning_rate": 0.0008525888717156105,
      "loss": 1.5259,
      "step": 7410
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.2613394260406494,
      "learning_rate": 0.0008496908809891809,
      "loss": 1.4795,
      "step": 7420
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.21076041460037231,
      "learning_rate": 0.0008467928902627513,
      "loss": 1.5634,
      "step": 7430
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.27715471386909485,
      "learning_rate": 0.0008438948995363214,
      "loss": 1.408,
      "step": 7440
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.1632375568151474,
      "learning_rate": 0.0008409969088098918,
      "loss": 1.4547,
      "step": 7450
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.429990291595459,
      "learning_rate": 0.0008380989180834621,
      "loss": 1.5707,
      "step": 7460
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.14352822303771973,
      "learning_rate": 0.0008352009273570324,
      "loss": 1.4812,
      "step": 7470
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.21575841307640076,
      "learning_rate": 0.0008323029366306028,
      "loss": 1.4841,
      "step": 7480
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.20057982206344604,
      "learning_rate": 0.0008294049459041732,
      "loss": 1.3956,
      "step": 7490
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.46943673491477966,
      "learning_rate": 0.0008265069551777435,
      "loss": 1.4514,
      "step": 7500
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.22915776073932648,
      "learning_rate": 0.0008236089644513138,
      "loss": 1.4157,
      "step": 7510
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.18804827332496643,
      "learning_rate": 0.0008207109737248841,
      "loss": 1.6362,
      "step": 7520
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.2874651253223419,
      "learning_rate": 0.0008178129829984543,
      "loss": 1.4217,
      "step": 7530
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.2649081349372864,
      "learning_rate": 0.0008149149922720247,
      "loss": 1.5545,
      "step": 7540
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.2907043695449829,
      "learning_rate": 0.0008120170015455951,
      "loss": 1.4193,
      "step": 7550
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.1442076712846756,
      "learning_rate": 0.0008091190108191654,
      "loss": 1.4525,
      "step": 7560
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.20988769829273224,
      "learning_rate": 0.0008062210200927357,
      "loss": 1.4587,
      "step": 7570
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.4053424000740051,
      "learning_rate": 0.000803323029366306,
      "loss": 1.4687,
      "step": 7580
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.3951297402381897,
      "learning_rate": 0.0008004250386398765,
      "loss": 1.7051,
      "step": 7590
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.22599150240421295,
      "learning_rate": 0.0007975270479134468,
      "loss": 1.566,
      "step": 7600
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.3238944411277771,
      "learning_rate": 0.000794629057187017,
      "loss": 1.5275,
      "step": 7610
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.3100896179676056,
      "learning_rate": 0.0007917310664605873,
      "loss": 1.3901,
      "step": 7620
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.5409083366394043,
      "learning_rate": 0.0007888330757341576,
      "loss": 1.5296,
      "step": 7630
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.5234825611114502,
      "learning_rate": 0.0007859350850077279,
      "loss": 1.5334,
      "step": 7640
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.3833597004413605,
      "learning_rate": 0.0007830370942812984,
      "loss": 1.4463,
      "step": 7650
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.15597569942474365,
      "learning_rate": 0.0007801391035548687,
      "loss": 1.4331,
      "step": 7660
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.1827719509601593,
      "learning_rate": 0.000777241112828439,
      "loss": 1.6273,
      "step": 7670
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.26286399364471436,
      "learning_rate": 0.0007743431221020093,
      "loss": 1.609,
      "step": 7680
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.2136349081993103,
      "learning_rate": 0.0007714451313755795,
      "loss": 1.4249,
      "step": 7690
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.2938110828399658,
      "learning_rate": 0.0007685471406491498,
      "loss": 1.5968,
      "step": 7700
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.2073156237602234,
      "learning_rate": 0.0007656491499227203,
      "loss": 1.5534,
      "step": 7710
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.25523415207862854,
      "learning_rate": 0.0007627511591962906,
      "loss": 1.3724,
      "step": 7720
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.21562480926513672,
      "learning_rate": 0.0007598531684698609,
      "loss": 1.5425,
      "step": 7730
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.2968670129776001,
      "learning_rate": 0.0007569551777434312,
      "loss": 1.4006,
      "step": 7740
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.2189972698688507,
      "learning_rate": 0.0007540571870170016,
      "loss": 1.4724,
      "step": 7750
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.3593066930770874,
      "learning_rate": 0.000751159196290572,
      "loss": 1.6108,
      "step": 7760
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.312730997800827,
      "learning_rate": 0.0007482612055641422,
      "loss": 1.47,
      "step": 7770
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.30787894129753113,
      "learning_rate": 0.0007453632148377126,
      "loss": 1.5532,
      "step": 7780
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.36684226989746094,
      "learning_rate": 0.0007424652241112829,
      "loss": 1.4431,
      "step": 7790
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.2968132793903351,
      "learning_rate": 0.0007395672333848531,
      "loss": 1.3746,
      "step": 7800
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.27947840094566345,
      "learning_rate": 0.0007366692426584235,
      "loss": 1.5316,
      "step": 7810
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.13807784020900726,
      "learning_rate": 0.0007337712519319939,
      "loss": 1.5066,
      "step": 7820
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.27820640802383423,
      "learning_rate": 0.0007308732612055642,
      "loss": 1.5954,
      "step": 7830
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.19721783697605133,
      "learning_rate": 0.0007279752704791345,
      "loss": 1.4261,
      "step": 7840
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.3159390389919281,
      "learning_rate": 0.0007250772797527048,
      "loss": 1.5293,
      "step": 7850
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.23947152495384216,
      "learning_rate": 0.0007221792890262751,
      "loss": 1.5481,
      "step": 7860
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.1669035106897354,
      "learning_rate": 0.0007192812982998454,
      "loss": 1.5122,
      "step": 7870
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.2853796184062958,
      "learning_rate": 0.0007163833075734158,
      "loss": 1.5352,
      "step": 7880
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.32532545924186707,
      "learning_rate": 0.0007134853168469861,
      "loss": 1.461,
      "step": 7890
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.2750403583049774,
      "learning_rate": 0.0007105873261205564,
      "loss": 1.5752,
      "step": 7900
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.8626047372817993,
      "learning_rate": 0.0007076893353941268,
      "loss": 1.6729,
      "step": 7910
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.24258120357990265,
      "learning_rate": 0.000704791344667697,
      "loss": 1.4807,
      "step": 7920
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.2601841688156128,
      "learning_rate": 0.0007018933539412673,
      "loss": 1.4924,
      "step": 7930
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.17130526900291443,
      "learning_rate": 0.0006989953632148378,
      "loss": 1.6332,
      "step": 7940
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.18726152181625366,
      "learning_rate": 0.0006960973724884081,
      "loss": 1.4868,
      "step": 7950
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.2954862415790558,
      "learning_rate": 0.0006931993817619784,
      "loss": 1.5433,
      "step": 7960
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.2536356449127197,
      "learning_rate": 0.0006903013910355487,
      "loss": 1.3804,
      "step": 7970
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.26273268461227417,
      "learning_rate": 0.000687403400309119,
      "loss": 1.5489,
      "step": 7980
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.20456385612487793,
      "learning_rate": 0.0006845054095826893,
      "loss": 1.5545,
      "step": 7990
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.24833063781261444,
      "learning_rate": 0.0006816074188562597,
      "loss": 1.4457,
      "step": 8000
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.4832055866718292,
      "learning_rate": 0.00067870942812983,
      "loss": 1.3904,
      "step": 8010
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.19052201509475708,
      "learning_rate": 0.0006758114374034003,
      "loss": 1.4894,
      "step": 8020
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.22473202645778656,
      "learning_rate": 0.0006729134466769706,
      "loss": 1.6015,
      "step": 8030
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.46195679903030396,
      "learning_rate": 0.000670015455950541,
      "loss": 1.6367,
      "step": 8040
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.2890683114528656,
      "learning_rate": 0.0006671174652241112,
      "loss": 1.625,
      "step": 8050
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.29708677530288696,
      "learning_rate": 0.0006642194744976816,
      "loss": 1.5125,
      "step": 8060
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.33059054613113403,
      "learning_rate": 0.000661321483771252,
      "loss": 1.4896,
      "step": 8070
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.3305750787258148,
      "learning_rate": 0.0006584234930448223,
      "loss": 1.407,
      "step": 8080
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.4403376281261444,
      "learning_rate": 0.0006555255023183925,
      "loss": 1.4646,
      "step": 8090
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.4211503565311432,
      "learning_rate": 0.0006526275115919629,
      "loss": 1.5969,
      "step": 8100
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.24159260094165802,
      "learning_rate": 0.0006497295208655333,
      "loss": 1.5827,
      "step": 8110
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.2535944879055023,
      "learning_rate": 0.0006468315301391036,
      "loss": 1.4286,
      "step": 8120
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.23672032356262207,
      "learning_rate": 0.0006439335394126739,
      "loss": 1.4897,
      "step": 8130
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.22775787115097046,
      "learning_rate": 0.0006410355486862442,
      "loss": 1.6617,
      "step": 8140
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.140718013048172,
      "learning_rate": 0.0006381375579598145,
      "loss": 1.4772,
      "step": 8150
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.276784211397171,
      "learning_rate": 0.0006352395672333848,
      "loss": 1.5696,
      "step": 8160
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.2129618376493454,
      "learning_rate": 0.0006323415765069553,
      "loss": 1.6416,
      "step": 8170
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.32354506850242615,
      "learning_rate": 0.0006294435857805255,
      "loss": 1.53,
      "step": 8180
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.4030933678150177,
      "learning_rate": 0.0006265455950540958,
      "loss": 1.5847,
      "step": 8190
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.34787648916244507,
      "learning_rate": 0.0006236476043276662,
      "loss": 1.6033,
      "step": 8200
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.33844617009162903,
      "learning_rate": 0.0006207496136012365,
      "loss": 1.56,
      "step": 8210
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.28325390815734863,
      "learning_rate": 0.0006178516228748067,
      "loss": 1.4049,
      "step": 8220
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.2055843025445938,
      "learning_rate": 0.0006149536321483772,
      "loss": 1.5338,
      "step": 8230
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.39746731519699097,
      "learning_rate": 0.0006120556414219475,
      "loss": 1.7359,
      "step": 8240
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.34257856011390686,
      "learning_rate": 0.0006091576506955178,
      "loss": 1.6583,
      "step": 8250
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.20192848145961761,
      "learning_rate": 0.0006062596599690881,
      "loss": 1.5159,
      "step": 8260
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.2677879333496094,
      "learning_rate": 0.0006033616692426584,
      "loss": 1.5652,
      "step": 8270
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.26040294766426086,
      "learning_rate": 0.0006004636785162287,
      "loss": 1.4061,
      "step": 8280
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.21377326548099518,
      "learning_rate": 0.0005975656877897991,
      "loss": 1.6272,
      "step": 8290
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.2665982246398926,
      "learning_rate": 0.0005946676970633694,
      "loss": 1.4478,
      "step": 8300
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.2512306869029999,
      "learning_rate": 0.0005917697063369397,
      "loss": 1.5087,
      "step": 8310
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.2501053810119629,
      "learning_rate": 0.00058887171561051,
      "loss": 1.6497,
      "step": 8320
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.26292040944099426,
      "learning_rate": 0.0005859737248840804,
      "loss": 1.4067,
      "step": 8330
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.252582311630249,
      "learning_rate": 0.0005830757341576506,
      "loss": 1.5216,
      "step": 8340
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.23893630504608154,
      "learning_rate": 0.000580177743431221,
      "loss": 1.5864,
      "step": 8350
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.6269095540046692,
      "learning_rate": 0.0005772797527047914,
      "loss": 1.537,
      "step": 8360
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.31512904167175293,
      "learning_rate": 0.0005743817619783617,
      "loss": 1.4917,
      "step": 8370
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.20488901436328888,
      "learning_rate": 0.000571483771251932,
      "loss": 1.6083,
      "step": 8380
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.36964792013168335,
      "learning_rate": 0.0005685857805255023,
      "loss": 1.5319,
      "step": 8390
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.27751532196998596,
      "learning_rate": 0.0005656877897990727,
      "loss": 1.3915,
      "step": 8400
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.4597133994102478,
      "learning_rate": 0.000562789799072643,
      "loss": 1.6292,
      "step": 8410
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.23265062272548676,
      "learning_rate": 0.0005598918083462133,
      "loss": 1.4819,
      "step": 8420
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.1539796143770218,
      "learning_rate": 0.0005569938176197836,
      "loss": 1.5416,
      "step": 8430
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.2553362548351288,
      "learning_rate": 0.0005540958268933539,
      "loss": 1.54,
      "step": 8440
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.29805347323417664,
      "learning_rate": 0.0005511978361669242,
      "loss": 1.6126,
      "step": 8450
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.23361822962760925,
      "learning_rate": 0.0005482998454404947,
      "loss": 1.4349,
      "step": 8460
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.32253098487854004,
      "learning_rate": 0.0005454018547140649,
      "loss": 1.6183,
      "step": 8470
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.1802223026752472,
      "learning_rate": 0.0005425038639876352,
      "loss": 1.495,
      "step": 8480
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.38347774744033813,
      "learning_rate": 0.0005396058732612056,
      "loss": 1.4408,
      "step": 8490
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.3296341300010681,
      "learning_rate": 0.0005367078825347759,
      "loss": 1.506,
      "step": 8500
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.2451552450656891,
      "learning_rate": 0.0005338098918083461,
      "loss": 1.5691,
      "step": 8510
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.3004821538925171,
      "learning_rate": 0.0005309119010819166,
      "loss": 1.6221,
      "step": 8520
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.3482872545719147,
      "learning_rate": 0.0005280139103554869,
      "loss": 1.5555,
      "step": 8530
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.34776318073272705,
      "learning_rate": 0.0005251159196290572,
      "loss": 1.5116,
      "step": 8540
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.2954006791114807,
      "learning_rate": 0.0005222179289026275,
      "loss": 1.4534,
      "step": 8550
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.6800978779792786,
      "learning_rate": 0.0005193199381761978,
      "loss": 1.5577,
      "step": 8560
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.5292263031005859,
      "learning_rate": 0.0005164219474497681,
      "loss": 1.574,
      "step": 8570
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.290941447019577,
      "learning_rate": 0.0005135239567233385,
      "loss": 1.5611,
      "step": 8580
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.20774146914482117,
      "learning_rate": 0.0005106259659969089,
      "loss": 1.6368,
      "step": 8590
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.20316874980926514,
      "learning_rate": 0.0005077279752704791,
      "loss": 1.4053,
      "step": 8600
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.27140823006629944,
      "learning_rate": 0.0005048299845440494,
      "loss": 1.4893,
      "step": 8610
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.27284830808639526,
      "learning_rate": 0.0005019319938176198,
      "loss": 1.543,
      "step": 8620
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.2862854301929474,
      "learning_rate": 0.0004990340030911902,
      "loss": 1.5822,
      "step": 8630
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.3617786765098572,
      "learning_rate": 0.0004961360123647604,
      "loss": 1.51,
      "step": 8640
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.2620408833026886,
      "learning_rate": 0.0004932380216383308,
      "loss": 1.4997,
      "step": 8650
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.30046430230140686,
      "learning_rate": 0.0004903400309119011,
      "loss": 1.6164,
      "step": 8660
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.34609949588775635,
      "learning_rate": 0.00048744204018547147,
      "loss": 1.5853,
      "step": 8670
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.28702789545059204,
      "learning_rate": 0.0004845440494590417,
      "loss": 1.4932,
      "step": 8680
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.23796255886554718,
      "learning_rate": 0.00048164605873261205,
      "loss": 1.5359,
      "step": 8690
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.23746536672115326,
      "learning_rate": 0.0004787480680061824,
      "loss": 1.5247,
      "step": 8700
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.2819928526878357,
      "learning_rate": 0.00047585007727975274,
      "loss": 1.578,
      "step": 8710
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.24760080873966217,
      "learning_rate": 0.000472952086553323,
      "loss": 1.5209,
      "step": 8720
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.29462677240371704,
      "learning_rate": 0.00047005409582689337,
      "loss": 1.6142,
      "step": 8730
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.33302250504493713,
      "learning_rate": 0.0004671561051004637,
      "loss": 1.4477,
      "step": 8740
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.4156724512577057,
      "learning_rate": 0.00046425811437403406,
      "loss": 1.5609,
      "step": 8750
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.35067903995513916,
      "learning_rate": 0.0004613601236476043,
      "loss": 1.46,
      "step": 8760
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.26389414072036743,
      "learning_rate": 0.00045846213292117464,
      "loss": 1.454,
      "step": 8770
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.2385970503091812,
      "learning_rate": 0.000455564142194745,
      "loss": 1.5171,
      "step": 8780
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.19866667687892914,
      "learning_rate": 0.0004526661514683153,
      "loss": 1.6974,
      "step": 8790
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.3140312135219574,
      "learning_rate": 0.0004497681607418857,
      "loss": 1.5264,
      "step": 8800
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.1944950968027115,
      "learning_rate": 0.00044687017001545596,
      "loss": 1.4601,
      "step": 8810
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.6051931381225586,
      "learning_rate": 0.0004439721792890263,
      "loss": 1.5012,
      "step": 8820
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.40593641996383667,
      "learning_rate": 0.00044107418856259665,
      "loss": 1.4127,
      "step": 8830
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.6295647025108337,
      "learning_rate": 0.00043817619783616696,
      "loss": 1.5981,
      "step": 8840
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.24309806525707245,
      "learning_rate": 0.0004352782071097372,
      "loss": 1.5095,
      "step": 8850
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.2324894219636917,
      "learning_rate": 0.0004323802163833076,
      "loss": 1.5319,
      "step": 8860
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.27118948101997375,
      "learning_rate": 0.0004294822256568779,
      "loss": 1.5792,
      "step": 8870
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.3378048539161682,
      "learning_rate": 0.0004265842349304483,
      "loss": 1.5513,
      "step": 8880
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.4281601309776306,
      "learning_rate": 0.00042368624420401855,
      "loss": 1.5805,
      "step": 8890
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.6147106289863586,
      "learning_rate": 0.00042078825347758886,
      "loss": 1.4438,
      "step": 8900
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.2861926853656769,
      "learning_rate": 0.00041789026275115923,
      "loss": 1.5235,
      "step": 8910
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.2109145075082779,
      "learning_rate": 0.00041499227202472955,
      "loss": 1.4868,
      "step": 8920
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.25562480092048645,
      "learning_rate": 0.0004120942812982998,
      "loss": 1.4644,
      "step": 8930
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.2869751453399658,
      "learning_rate": 0.0004091962905718702,
      "loss": 1.4147,
      "step": 8940
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.2870786190032959,
      "learning_rate": 0.0004062982998454405,
      "loss": 1.4673,
      "step": 8950
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.22262711822986603,
      "learning_rate": 0.00040340030911901087,
      "loss": 1.3783,
      "step": 8960
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.22974833846092224,
      "learning_rate": 0.00040050231839258113,
      "loss": 1.3257,
      "step": 8970
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.21338006854057312,
      "learning_rate": 0.00039760432766615145,
      "loss": 1.4518,
      "step": 8980
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.4128793478012085,
      "learning_rate": 0.0003947063369397218,
      "loss": 1.3624,
      "step": 8990
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.2669835090637207,
      "learning_rate": 0.00039180834621329214,
      "loss": 1.4224,
      "step": 9000
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.318866491317749,
      "learning_rate": 0.0003889103554868625,
      "loss": 1.5325,
      "step": 9010
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.35372552275657654,
      "learning_rate": 0.00038601236476043277,
      "loss": 1.5085,
      "step": 9020
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.1827588528394699,
      "learning_rate": 0.0003831143740340031,
      "loss": 1.5475,
      "step": 9030
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.3277885317802429,
      "learning_rate": 0.00038021638330757346,
      "loss": 1.5918,
      "step": 9040
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.2718327045440674,
      "learning_rate": 0.0003773183925811438,
      "loss": 1.5154,
      "step": 9050
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.43376970291137695,
      "learning_rate": 0.0003744204018547141,
      "loss": 1.3318,
      "step": 9060
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.5452284812927246,
      "learning_rate": 0.0003715224111282844,
      "loss": 1.4708,
      "step": 9070
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.301024854183197,
      "learning_rate": 0.0003686244204018547,
      "loss": 1.5506,
      "step": 9080
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.33526018261909485,
      "learning_rate": 0.00036572642967542504,
      "loss": 1.4599,
      "step": 9090
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.34690967202186584,
      "learning_rate": 0.00036282843894899536,
      "loss": 1.4435,
      "step": 9100
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.20088347792625427,
      "learning_rate": 0.0003599304482225657,
      "loss": 1.4466,
      "step": 9110
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.20508232712745667,
      "learning_rate": 0.00035703245749613605,
      "loss": 1.445,
      "step": 9120
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.21697542071342468,
      "learning_rate": 0.0003541344667697063,
      "loss": 1.4908,
      "step": 9130
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.1843608319759369,
      "learning_rate": 0.0003512364760432767,
      "loss": 1.4553,
      "step": 9140
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.18010316789150238,
      "learning_rate": 0.000348338485316847,
      "loss": 1.5195,
      "step": 9150
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.2984198033809662,
      "learning_rate": 0.0003454404945904173,
      "loss": 1.4024,
      "step": 9160
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.2558923065662384,
      "learning_rate": 0.00034254250386398763,
      "loss": 1.5733,
      "step": 9170
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.18320909142494202,
      "learning_rate": 0.00033964451313755795,
      "loss": 1.6829,
      "step": 9180
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.25830569863319397,
      "learning_rate": 0.0003367465224111283,
      "loss": 1.3811,
      "step": 9190
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.2858661413192749,
      "learning_rate": 0.00033384853168469863,
      "loss": 1.6061,
      "step": 9200
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.3093240261077881,
      "learning_rate": 0.00033095054095826895,
      "loss": 1.5508,
      "step": 9210
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.28859394788742065,
      "learning_rate": 0.00032805255023183927,
      "loss": 1.5359,
      "step": 9220
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.399770587682724,
      "learning_rate": 0.0003251545595054096,
      "loss": 1.4234,
      "step": 9230
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.2706194818019867,
      "learning_rate": 0.0003222565687789799,
      "loss": 1.5304,
      "step": 9240
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.2757903039455414,
      "learning_rate": 0.00031935857805255027,
      "loss": 1.5933,
      "step": 9250
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.37227195501327515,
      "learning_rate": 0.00031646058732612053,
      "loss": 1.4474,
      "step": 9260
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.2115892767906189,
      "learning_rate": 0.0003135625965996909,
      "loss": 1.4885,
      "step": 9270
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.47332632541656494,
      "learning_rate": 0.0003106646058732612,
      "loss": 1.6254,
      "step": 9280
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.257834792137146,
      "learning_rate": 0.00030776661514683154,
      "loss": 1.4438,
      "step": 9290
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.3721349239349365,
      "learning_rate": 0.00030486862442040185,
      "loss": 1.5629,
      "step": 9300
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.1912510097026825,
      "learning_rate": 0.00030197063369397217,
      "loss": 1.3809,
      "step": 9310
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.28376638889312744,
      "learning_rate": 0.0002990726429675425,
      "loss": 1.654,
      "step": 9320
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.28749608993530273,
      "learning_rate": 0.00029617465224111286,
      "loss": 1.5012,
      "step": 9330
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.32395172119140625,
      "learning_rate": 0.0002932766615146831,
      "loss": 1.3942,
      "step": 9340
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.325797975063324,
      "learning_rate": 0.0002903786707882535,
      "loss": 1.5451,
      "step": 9350
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.41286182403564453,
      "learning_rate": 0.0002874806800618238,
      "loss": 1.5925,
      "step": 9360
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.3303880989551544,
      "learning_rate": 0.0002845826893353941,
      "loss": 1.4635,
      "step": 9370
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.23750847578048706,
      "learning_rate": 0.00028168469860896444,
      "loss": 1.4674,
      "step": 9380
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.15739117562770844,
      "learning_rate": 0.00027878670788253476,
      "loss": 1.5831,
      "step": 9390
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.21856094896793365,
      "learning_rate": 0.00027588871715610513,
      "loss": 1.56,
      "step": 9400
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.17843270301818848,
      "learning_rate": 0.00027299072642967545,
      "loss": 1.6159,
      "step": 9410
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.2714165449142456,
      "learning_rate": 0.00027009273570324576,
      "loss": 1.5408,
      "step": 9420
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.2091442346572876,
      "learning_rate": 0.0002671947449768161,
      "loss": 1.4362,
      "step": 9430
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.2528344988822937,
      "learning_rate": 0.0002642967542503864,
      "loss": 1.4652,
      "step": 9440
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.20882803201675415,
      "learning_rate": 0.0002613987635239567,
      "loss": 1.479,
      "step": 9450
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.30727580189704895,
      "learning_rate": 0.0002585007727975271,
      "loss": 1.408,
      "step": 9460
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.3316057026386261,
      "learning_rate": 0.00025560278207109735,
      "loss": 1.4864,
      "step": 9470
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.2906207740306854,
      "learning_rate": 0.0002527047913446677,
      "loss": 1.5058,
      "step": 9480
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.2556336224079132,
      "learning_rate": 0.00024980680061823803,
      "loss": 1.4995,
      "step": 9490
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.22260114550590515,
      "learning_rate": 0.00024690880989180835,
      "loss": 1.4926,
      "step": 9500
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.27679339051246643,
      "learning_rate": 0.00024401081916537867,
      "loss": 1.7725,
      "step": 9510
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.3094691336154938,
      "learning_rate": 0.000241112828438949,
      "loss": 1.3899,
      "step": 9520
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.26047155261039734,
      "learning_rate": 0.0002382148377125193,
      "loss": 1.4463,
      "step": 9530
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.1332738995552063,
      "learning_rate": 0.00023531684698608964,
      "loss": 1.4612,
      "step": 9540
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.23816853761672974,
      "learning_rate": 0.00023241885625965996,
      "loss": 1.604,
      "step": 9550
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.23877552151679993,
      "learning_rate": 0.0002295208655332303,
      "loss": 1.4669,
      "step": 9560
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.6597626805305481,
      "learning_rate": 0.0002266228748068006,
      "loss": 1.4588,
      "step": 9570
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.3195168972015381,
      "learning_rate": 0.00022372488408037094,
      "loss": 1.3841,
      "step": 9580
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.29977917671203613,
      "learning_rate": 0.00022082689335394125,
      "loss": 1.3916,
      "step": 9590
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.15731126070022583,
      "learning_rate": 0.0002179289026275116,
      "loss": 1.4545,
      "step": 9600
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.24896955490112305,
      "learning_rate": 0.00021503091190108194,
      "loss": 1.5217,
      "step": 9610
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.4648255705833435,
      "learning_rate": 0.00021213292117465223,
      "loss": 1.5113,
      "step": 9620
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.4647230803966522,
      "learning_rate": 0.00020923493044822257,
      "loss": 1.5268,
      "step": 9630
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.26574912667274475,
      "learning_rate": 0.0002063369397217929,
      "loss": 1.4845,
      "step": 9640
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.1677768975496292,
      "learning_rate": 0.00020343894899536324,
      "loss": 1.3931,
      "step": 9650
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.6385800242424011,
      "learning_rate": 0.00020054095826893352,
      "loss": 1.4479,
      "step": 9660
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.29266002774238586,
      "learning_rate": 0.00019764296754250387,
      "loss": 1.5238,
      "step": 9670
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.26471683382987976,
      "learning_rate": 0.00019474497681607419,
      "loss": 1.3548,
      "step": 9680
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.1729702353477478,
      "learning_rate": 0.00019184698608964453,
      "loss": 1.5831,
      "step": 9690
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.24938993155956268,
      "learning_rate": 0.00018894899536321482,
      "loss": 1.4703,
      "step": 9700
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.24173025786876678,
      "learning_rate": 0.0001860510046367852,
      "loss": 1.5443,
      "step": 9710
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.22310292720794678,
      "learning_rate": 0.0001831530139103555,
      "loss": 1.5214,
      "step": 9720
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.2641667425632477,
      "learning_rate": 0.00018025502318392582,
      "loss": 1.49,
      "step": 9730
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.33326515555381775,
      "learning_rate": 0.00017735703245749614,
      "loss": 1.3563,
      "step": 9740
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.32115447521209717,
      "learning_rate": 0.00017445904173106648,
      "loss": 1.6208,
      "step": 9750
    },
    {
      "epoch": 1.89,
      "grad_norm": 0.45416077971458435,
      "learning_rate": 0.0001715610510046368,
      "loss": 1.6035,
      "step": 9760
    },
    {
      "epoch": 1.89,
      "grad_norm": 0.286516010761261,
      "learning_rate": 0.00016866306027820712,
      "loss": 1.6019,
      "step": 9770
    },
    {
      "epoch": 1.89,
      "grad_norm": 0.2988051474094391,
      "learning_rate": 0.00016576506955177743,
      "loss": 1.4533,
      "step": 9780
    },
    {
      "epoch": 1.89,
      "grad_norm": 0.22135822474956512,
      "learning_rate": 0.00016286707882534778,
      "loss": 1.4526,
      "step": 9790
    },
    {
      "epoch": 1.89,
      "grad_norm": 0.17810016870498657,
      "learning_rate": 0.0001599690880989181,
      "loss": 1.3553,
      "step": 9800
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.2604820132255554,
      "learning_rate": 0.0001570710973724884,
      "loss": 1.5508,
      "step": 9810
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.20647032558918,
      "learning_rate": 0.00015417310664605873,
      "loss": 1.4701,
      "step": 9820
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.14645516872406006,
      "learning_rate": 0.00015127511591962907,
      "loss": 1.5268,
      "step": 9830
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.23276568949222565,
      "learning_rate": 0.0001483771251931994,
      "loss": 1.3566,
      "step": 9840
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.27737101912498474,
      "learning_rate": 0.0001454791344667697,
      "loss": 1.4961,
      "step": 9850
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.230781689286232,
      "learning_rate": 0.00014258114374034002,
      "loss": 1.6226,
      "step": 9860
    },
    {
      "epoch": 1.91,
      "grad_norm": 0.2939196527004242,
      "learning_rate": 0.00013968315301391036,
      "loss": 1.5778,
      "step": 9870
    },
    {
      "epoch": 1.91,
      "grad_norm": 0.3009508550167084,
      "learning_rate": 0.00013678516228748068,
      "loss": 1.4728,
      "step": 9880
    },
    {
      "epoch": 1.91,
      "grad_norm": 0.30798307061195374,
      "learning_rate": 0.000133887171561051,
      "loss": 1.5014,
      "step": 9890
    },
    {
      "epoch": 1.91,
      "grad_norm": 0.2770700752735138,
      "learning_rate": 0.00013098918083462131,
      "loss": 1.3764,
      "step": 9900
    },
    {
      "epoch": 1.91,
      "grad_norm": 0.2542928457260132,
      "learning_rate": 0.00012809119010819166,
      "loss": 1.4589,
      "step": 9910
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.3343620300292969,
      "learning_rate": 0.000125193199381762,
      "loss": 1.5417,
      "step": 9920
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.20016755163669586,
      "learning_rate": 0.00012229520865533232,
      "loss": 1.6747,
      "step": 9930
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.3815073072910309,
      "learning_rate": 0.00011939721792890263,
      "loss": 1.6058,
      "step": 9940
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.3225597143173218,
      "learning_rate": 0.00011649922720247296,
      "loss": 1.5009,
      "step": 9950
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.4474504590034485,
      "learning_rate": 0.00011360123647604328,
      "loss": 1.4763,
      "step": 9960
    },
    {
      "epoch": 1.93,
      "grad_norm": 0.3179337680339813,
      "learning_rate": 0.00011070324574961361,
      "loss": 1.6136,
      "step": 9970
    },
    {
      "epoch": 1.93,
      "grad_norm": 0.29828009009361267,
      "learning_rate": 0.00010780525502318393,
      "loss": 1.6232,
      "step": 9980
    },
    {
      "epoch": 1.93,
      "grad_norm": 0.1975731998682022,
      "learning_rate": 0.00010490726429675426,
      "loss": 1.5727,
      "step": 9990
    },
    {
      "epoch": 1.93,
      "grad_norm": 0.41645339131355286,
      "learning_rate": 0.00010200927357032458,
      "loss": 1.4098,
      "step": 10000
    }
  ],
  "logging_steps": 10,
  "max_steps": 10352,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "total_flos": 4.660335543048192e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
