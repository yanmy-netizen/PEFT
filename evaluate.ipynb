{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = load_dataset(\"tatsu-lab/alpaca_eval\") this line does not work\n",
    "#eval_set = load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval\")[\"eval\"]\n",
    "#eval_set.save_to_disk(\"./AlpacaEval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./AlpacaEval\"\n",
    "eval_set = load_from_disk(data_path)\n",
    "eval_set = eval_set.select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': ['What are the names of some famous actors that started their careers on Broadway?',\n",
       "  'How did US states get their names?',\n",
       "  \"Hi, my sister and her girlfriends want me to play kickball with them. Can you explain how the game is played, so they don't take advantage of me?\"],\n",
       " 'output': ['Some famous actors that started their careers on Broadway include: \\n1. Hugh Jackman \\n2. Meryl Streep \\n3. Denzel Washington \\n4. Julia Roberts \\n5. Christopher Walken \\n6. Anthony Rapp \\n7. Audra McDonald \\n8. Nathan Lane \\n9. Sarah Jessica Parker \\n10. Lin-Manuel Miranda',\n",
       "  'US states get their names from a variety of sources, including Native American tribes, Spanish explorers, British colonists, and even presidents. For example, the state of Alabama was named after the Native American tribe that lived in the area, while the state of Florida gets its name from the Spanish explorer, Ponce de Leon, who explored the area in the 1500s. Other states are named after English kings (like Virginia, named after England\\'s \"Virgin Queen,\" Queen Elizabeth I) or presidents (like Washington, named after George Washington).',\n",
       "  'Kickball is a game similar to baseball, but with a large rubber ball instead of a bat and a ball. The game is usually played with two teams of six players each. Each team has three bases and a home plate. The players on the kicking team line up at home plate and take turns kicking the ball. The object of the game is to score runs by running around all three bases and back to home plate without being tagged out by the defense. The team with the most runs at the end of the game is the winner.'],\n",
       " 'generator': ['text_davinci_003', 'text_davinci_003', 'text_davinci_003'],\n",
       " 'dataset': ['helpful_base', 'helpful_base', 'helpful_base']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomTokenizerFast(name_or_path='./Bloom1b1', vocab_size=250680, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-1b1\")\n",
    "bloom_path = \"./Bloom1b1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bloom_path)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b1\", low_cpu_mem_usage=True)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(bloom_path, low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"p_tuning\"\n",
    "model_path = f\".\\{method}\\chatbot\\checkpoint-10000\"\n",
    "ft_model = PeftModel.from_pretrained(model=base_model, model_id=model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_bleu = evaluate.load(\"bleu\")\n",
    "eval_rouge = evaluate.load(\"rouge\")\n",
    "eval_bertscore = evaluate.load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_output(model, instruction):\n",
    "    with torch.no_grad(): \n",
    "        input_string = \"Human: {}\\n{}\".format(instruction, \"\").strip() + \"\\n\\nAssistant: \"        \n",
    "        ipt = tokenizer(input_string, return_tensors=\"pt\").to(model.device)\n",
    "        otpt = model.generate(**ipt, max_new_tokens=256, do_sample=True)[0]       \n",
    "        result = tokenizer.decode(otpt, skip_special_tokens=True).replace(input_string, \"\")        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.cuda()\n",
    "base_model.eval() \n",
    "\n",
    "pred_base = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in eval_set:        \n",
    "        output = generate_model_output(model=base_model,instruction=data['instruction'])\n",
    "        pred_base.append(output)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.cuda()\n",
    "ft_model.eval()\n",
    "\n",
    "pred_ft = []\n",
    "with torch.no_grad():\n",
    "    for data in eval_set:\n",
    "        output = generate_model_output(model=ft_model,instruction=data['instruction'])\n",
    "        pred_ft.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = []\n",
    "for data in eval_set:\n",
    "    ref.append(data['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.010969173044895468, 'precisions': [0.1449696279422931, 0.019511729925615107, 0.003967037178995784, 0.0012901983439245139], 'brevity_penalty': 1.0, 'length_ratio': 1.9976110573915022, 'translation_length': 105360, 'reference_length': 52743}\n",
      "{'rouge1': 0.16237897969571732, 'rouge2': 0.02475328223523595, 'rougeL': 0.09984475869642855, 'rougeLsum': 0.13522026779994906}\n",
      "Average bertscore precision: \n",
      "0.7960452679395675\n"
     ]
    }
   ],
   "source": [
    "base_bleu = eval_bleu.compute(predictions = pred_base, references= ref)\n",
    "base_rouge = eval_rouge.compute(predictions = pred_base, references= ref)\n",
    "\n",
    "base_bert = eval_bertscore.compute(predictions=pred_base, references=ref, lang=\"en\")\n",
    "base_bert = base_bert['precision']\n",
    "\n",
    "print(base_bleu)\n",
    "print(base_rouge)\n",
    "print(\"Average bertscore precision: \")\n",
    "print(sum(base_bert)/len(base_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.031113092632829267, 'precisions': [0.2223935550870126, 0.04792565338746511, 0.01432225063938619, 0.006138626623272001], 'brevity_penalty': 1.0, 'length_ratio': 1.2049750677815065, 'translation_length': 63554, 'reference_length': 52743}\n",
      "{'rouge1': 0.2229655337233108, 'rouge2': 0.051207244107131546, 'rougeL': 0.14802214894464744, 'rougeLsum': 0.17962834147424978}\n",
      "Average bertscore precision: \n",
      "0.8341480239629745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "ft_bleu = eval_bleu.compute(predictions = pred_ft, references= ref)\n",
    "ft_rouge = eval_rouge.compute(predictions = pred_ft, references= ref)\n",
    "\n",
    "ft_bert = eval_bertscore.compute(predictions=pred_ft, references=ref, lang=\"en\")\n",
    "ft_bert = ft_bert['precision']\n",
    "\n",
    "print(ft_bleu)\n",
    "print(ft_rouge)\n",
    "print(\"Average bertscore precision: \")\n",
    "print(sum(ft_bert)/len(ft_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "import json\n",
    "import os\n",
    "\n",
    "Eval_result = {\n",
    "    \"bleu\": ft_bleu,\n",
    "    \"rouge\": ft_rouge,\n",
    "    \"bertscore\": sum(ft_bert)/len(ft_bert),\n",
    "}\n",
    "\n",
    "directory = f\"./{method}/\"\n",
    "json_filename = \"eval_result.json\"\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "json_file_path = os.path.join(directory, json_filename)\n",
    "\n",
    "with open(json_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(Eval_result, f, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
