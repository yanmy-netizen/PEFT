{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01,
      "grad_norm": 0.9479593634605408,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 2.3789,
      "step": 10
    },
    {
      "epoch": 0.02,
      "grad_norm": 1.5464972257614136,
      "learning_rate": 4.9e-05,
      "loss": 2.3549,
      "step": 20
    },
    {
      "epoch": 0.03,
      "grad_norm": 2.3337299823760986,
      "learning_rate": 4.85e-05,
      "loss": 2.1671,
      "step": 30
    },
    {
      "epoch": 0.04,
      "grad_norm": 2.6896445751190186,
      "learning_rate": 4.8e-05,
      "loss": 2.1884,
      "step": 40
    },
    {
      "epoch": 0.05,
      "grad_norm": 2.3402836322784424,
      "learning_rate": 4.75e-05,
      "loss": 2.2034,
      "step": 50
    },
    {
      "epoch": 0.06,
      "grad_norm": 2.5278422832489014,
      "learning_rate": 4.7e-05,
      "loss": 1.9327,
      "step": 60
    },
    {
      "epoch": 0.07,
      "grad_norm": 1.3716206550598145,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 1.9221,
      "step": 70
    },
    {
      "epoch": 0.08,
      "grad_norm": 7.409725666046143,
      "learning_rate": 4.600000000000001e-05,
      "loss": 2.0053,
      "step": 80
    },
    {
      "epoch": 0.09,
      "grad_norm": 1.6158283948898315,
      "learning_rate": 4.55e-05,
      "loss": 2.0212,
      "step": 90
    },
    {
      "epoch": 0.1,
      "grad_norm": 2.48403000831604,
      "learning_rate": 4.5e-05,
      "loss": 1.8959,
      "step": 100
    },
    {
      "epoch": 0.11,
      "grad_norm": 1.4590351581573486,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 1.7994,
      "step": 110
    },
    {
      "epoch": 0.12,
      "grad_norm": 7.807621002197266,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.9688,
      "step": 120
    },
    {
      "epoch": 0.13,
      "grad_norm": 1.782036304473877,
      "learning_rate": 4.35e-05,
      "loss": 1.8454,
      "step": 130
    },
    {
      "epoch": 0.14,
      "grad_norm": 1.6472448110580444,
      "learning_rate": 4.3e-05,
      "loss": 1.8773,
      "step": 140
    },
    {
      "epoch": 0.15,
      "grad_norm": 13.436528205871582,
      "learning_rate": 4.25e-05,
      "loss": 1.8684,
      "step": 150
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.7069646120071411,
      "learning_rate": 4.2e-05,
      "loss": 1.9199,
      "step": 160
    },
    {
      "epoch": 0.17,
      "grad_norm": 2.1392927169799805,
      "learning_rate": 4.15e-05,
      "loss": 1.872,
      "step": 170
    },
    {
      "epoch": 0.18,
      "grad_norm": 2.2833409309387207,
      "learning_rate": 4.1e-05,
      "loss": 1.8646,
      "step": 180
    },
    {
      "epoch": 0.19,
      "grad_norm": 2.951127529144287,
      "learning_rate": 4.05e-05,
      "loss": 1.9761,
      "step": 190
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.927355170249939,
      "learning_rate": 4e-05,
      "loss": 1.9869,
      "step": 200
    },
    {
      "epoch": 0.21,
      "grad_norm": 2.5045218467712402,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 1.8841,
      "step": 210
    },
    {
      "epoch": 0.22,
      "grad_norm": 1.4008122682571411,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.8116,
      "step": 220
    },
    {
      "epoch": 0.23,
      "grad_norm": 1.7159337997436523,
      "learning_rate": 3.85e-05,
      "loss": 1.8972,
      "step": 230
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.0310620069503784,
      "learning_rate": 3.8e-05,
      "loss": 1.8893,
      "step": 240
    },
    {
      "epoch": 0.25,
      "grad_norm": 3.8333520889282227,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 1.8964,
      "step": 250
    },
    {
      "epoch": 0.26,
      "grad_norm": 1.6094211339950562,
      "learning_rate": 3.7e-05,
      "loss": 1.8713,
      "step": 260
    },
    {
      "epoch": 0.27,
      "grad_norm": 1.3568891286849976,
      "learning_rate": 3.65e-05,
      "loss": 1.8017,
      "step": 270
    },
    {
      "epoch": 0.28,
      "grad_norm": 2.475947856903076,
      "learning_rate": 3.6e-05,
      "loss": 1.9438,
      "step": 280
    },
    {
      "epoch": 0.29,
      "grad_norm": 1.6581863164901733,
      "learning_rate": 3.55e-05,
      "loss": 1.8186,
      "step": 290
    },
    {
      "epoch": 0.3,
      "grad_norm": 3.2852535247802734,
      "learning_rate": 3.5e-05,
      "loss": 1.8365,
      "step": 300
    },
    {
      "epoch": 0.31,
      "grad_norm": 1.715097427368164,
      "learning_rate": 3.45e-05,
      "loss": 1.8292,
      "step": 310
    },
    {
      "epoch": 0.32,
      "grad_norm": 4.043025016784668,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.8128,
      "step": 320
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.5570554733276367,
      "learning_rate": 3.35e-05,
      "loss": 1.9504,
      "step": 330
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.7877521514892578,
      "learning_rate": 3.3e-05,
      "loss": 1.9937,
      "step": 340
    },
    {
      "epoch": 0.35,
      "grad_norm": 1.7406948804855347,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 1.9222,
      "step": 350
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.781418800354004,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.7718,
      "step": 360
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.9669230580329895,
      "learning_rate": 3.15e-05,
      "loss": 1.8339,
      "step": 370
    },
    {
      "epoch": 0.38,
      "grad_norm": 3.663727283477783,
      "learning_rate": 3.1e-05,
      "loss": 1.9475,
      "step": 380
    },
    {
      "epoch": 0.39,
      "grad_norm": 2.3957645893096924,
      "learning_rate": 3.05e-05,
      "loss": 1.8141,
      "step": 390
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.1903808116912842,
      "learning_rate": 3e-05,
      "loss": 2.1153,
      "step": 400
    },
    {
      "epoch": 0.41,
      "grad_norm": 1.5157729387283325,
      "learning_rate": 2.95e-05,
      "loss": 1.9589,
      "step": 410
    },
    {
      "epoch": 0.42,
      "grad_norm": 1.487905740737915,
      "learning_rate": 2.9e-05,
      "loss": 1.7591,
      "step": 420
    },
    {
      "epoch": 0.43,
      "grad_norm": 2.1528892517089844,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 1.8682,
      "step": 430
    },
    {
      "epoch": 0.44,
      "grad_norm": 2.253916025161743,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.7862,
      "step": 440
    },
    {
      "epoch": 0.45,
      "grad_norm": 9.462148666381836,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 1.8934,
      "step": 450
    },
    {
      "epoch": 0.46,
      "grad_norm": 1.682309865951538,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.9483,
      "step": 460
    },
    {
      "epoch": 0.47,
      "grad_norm": 1.697963833808899,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 1.8275,
      "step": 470
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.877429187297821,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.6758,
      "step": 480
    },
    {
      "epoch": 0.49,
      "grad_norm": 3.508242607116699,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 1.8869,
      "step": 490
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.8541114330291748,
      "learning_rate": 2.5e-05,
      "loss": 1.8732,
      "step": 500
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.9758174419403076,
      "learning_rate": 2.45e-05,
      "loss": 1.7755,
      "step": 510
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.4049344062805176,
      "learning_rate": 2.4e-05,
      "loss": 1.8496,
      "step": 520
    },
    {
      "epoch": 0.53,
      "grad_norm": 1.116855263710022,
      "learning_rate": 2.35e-05,
      "loss": 1.8553,
      "step": 530
    },
    {
      "epoch": 0.54,
      "grad_norm": 1.999426245689392,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 1.9339,
      "step": 540
    },
    {
      "epoch": 0.55,
      "grad_norm": 2.188495635986328,
      "learning_rate": 2.25e-05,
      "loss": 1.8373,
      "step": 550
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.9436901807785034,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.9196,
      "step": 560
    },
    {
      "epoch": 0.57,
      "grad_norm": 1.60861074924469,
      "learning_rate": 2.15e-05,
      "loss": 1.8257,
      "step": 570
    },
    {
      "epoch": 0.58,
      "grad_norm": 1.1723037958145142,
      "learning_rate": 2.1e-05,
      "loss": 1.7706,
      "step": 580
    },
    {
      "epoch": 0.59,
      "grad_norm": 1.581987738609314,
      "learning_rate": 2.05e-05,
      "loss": 1.8801,
      "step": 590
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.631639838218689,
      "learning_rate": 2e-05,
      "loss": 1.9671,
      "step": 600
    },
    {
      "epoch": 0.61,
      "grad_norm": 1.4412548542022705,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 1.7254,
      "step": 610
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.9883862733840942,
      "learning_rate": 1.9e-05,
      "loss": 1.77,
      "step": 620
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.907845675945282,
      "learning_rate": 1.85e-05,
      "loss": 1.8052,
      "step": 630
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.6443005204200745,
      "learning_rate": 1.8e-05,
      "loss": 2.0686,
      "step": 640
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.7813799977302551,
      "learning_rate": 1.75e-05,
      "loss": 1.7472,
      "step": 650
    },
    {
      "epoch": 0.66,
      "grad_norm": 1.5682690143585205,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 2.0775,
      "step": 660
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.7604361772537231,
      "learning_rate": 1.65e-05,
      "loss": 1.7664,
      "step": 670
    },
    {
      "epoch": 0.68,
      "grad_norm": 1.9593510627746582,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.7214,
      "step": 680
    },
    {
      "epoch": 0.69,
      "grad_norm": 1.993166446685791,
      "learning_rate": 1.55e-05,
      "loss": 1.8969,
      "step": 690
    },
    {
      "epoch": 0.7,
      "grad_norm": 1.4610729217529297,
      "learning_rate": 1.5e-05,
      "loss": 1.7273,
      "step": 700
    },
    {
      "epoch": 0.71,
      "grad_norm": 1.5250203609466553,
      "learning_rate": 1.45e-05,
      "loss": 1.8476,
      "step": 710
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.2756234407424927,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.7775,
      "step": 720
    },
    {
      "epoch": 0.73,
      "grad_norm": 1.28706955909729,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 1.812,
      "step": 730
    },
    {
      "epoch": 0.74,
      "grad_norm": 1.0518466234207153,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.8186,
      "step": 740
    },
    {
      "epoch": 0.75,
      "grad_norm": 2.9019205570220947,
      "learning_rate": 1.25e-05,
      "loss": 2.1327,
      "step": 750
    },
    {
      "epoch": 0.76,
      "grad_norm": 1.6924575567245483,
      "learning_rate": 1.2e-05,
      "loss": 1.9148,
      "step": 760
    },
    {
      "epoch": 0.77,
      "grad_norm": 1.4269981384277344,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 1.8199,
      "step": 770
    },
    {
      "epoch": 0.78,
      "grad_norm": 7.1663713455200195,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.6893,
      "step": 780
    },
    {
      "epoch": 0.79,
      "grad_norm": 2.3364105224609375,
      "learning_rate": 1.05e-05,
      "loss": 1.8137,
      "step": 790
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.147952675819397,
      "learning_rate": 1e-05,
      "loss": 1.6936,
      "step": 800
    },
    {
      "epoch": 0.81,
      "grad_norm": 1.8092856407165527,
      "learning_rate": 9.5e-06,
      "loss": 1.7527,
      "step": 810
    },
    {
      "epoch": 0.82,
      "grad_norm": 1.0527667999267578,
      "learning_rate": 9e-06,
      "loss": 1.8537,
      "step": 820
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.9952320456504822,
      "learning_rate": 8.500000000000002e-06,
      "loss": 1.9552,
      "step": 830
    },
    {
      "epoch": 0.84,
      "grad_norm": 6.583702564239502,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.9164,
      "step": 840
    },
    {
      "epoch": 0.85,
      "grad_norm": 1.392901062965393,
      "learning_rate": 7.5e-06,
      "loss": 1.7548,
      "step": 850
    },
    {
      "epoch": 0.86,
      "grad_norm": 3.3367786407470703,
      "learning_rate": 7.000000000000001e-06,
      "loss": 1.7856,
      "step": 860
    },
    {
      "epoch": 0.87,
      "grad_norm": 1.021989345550537,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 1.8187,
      "step": 870
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.9172995090484619,
      "learning_rate": 6e-06,
      "loss": 1.8444,
      "step": 880
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.7507585287094116,
      "learning_rate": 5.500000000000001e-06,
      "loss": 1.729,
      "step": 890
    },
    {
      "epoch": 0.9,
      "grad_norm": 7.665128707885742,
      "learning_rate": 5e-06,
      "loss": 1.9745,
      "step": 900
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.7538059949874878,
      "learning_rate": 4.5e-06,
      "loss": 1.8489,
      "step": 910
    },
    {
      "epoch": 0.92,
      "grad_norm": 1.2099156379699707,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.8645,
      "step": 920
    },
    {
      "epoch": 0.93,
      "grad_norm": 1.1706262826919556,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 1.7608,
      "step": 930
    },
    {
      "epoch": 0.94,
      "grad_norm": 1.8447185754776,
      "learning_rate": 3e-06,
      "loss": 1.85,
      "step": 940
    },
    {
      "epoch": 0.95,
      "grad_norm": 1.4056522846221924,
      "learning_rate": 2.5e-06,
      "loss": 1.9175,
      "step": 950
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.32125985622406,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.7755,
      "step": 960
    },
    {
      "epoch": 0.97,
      "grad_norm": 2.765195846557617,
      "learning_rate": 1.5e-06,
      "loss": 1.7547,
      "step": 970
    },
    {
      "epoch": 0.98,
      "grad_norm": 2.036929130554199,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 1.8618,
      "step": 980
    },
    {
      "epoch": 0.99,
      "grad_norm": 1.2011584043502808,
      "learning_rate": 5.000000000000001e-07,
      "loss": 1.8031,
      "step": 990
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.2497855424880981,
      "learning_rate": 0.0,
      "loss": 1.7156,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 20,
  "total_flos": 4491070176079872.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
