{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloom 1b1 finetuning with PEFT p_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, TrainerCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset, dataset used is alpaca cleaned version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'instruction', 'input'],\n",
       "    num_rows: 8000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:8000]\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': ['1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.',\n",
       "  'The three primary colors are red, blue, and yellow. These colors are called primary because they cannot be created by mixing other colors and all other colors can be made by combining them in various proportions. In the additive color system, used for light, the primary colors are red, green, and blue (RGB).',\n",
       "  \"An atom is the basic building block of all matter and is made up of three types of particles: protons, neutrons, and electrons. The structure of an atom can be described as a nucleus at the center surrounded by a cloud of electrons.\\n\\nThe nucleus of an atom is made up of protons and neutrons. Protons are positively charged particles and neutrons are neutral particles with no charge. Both of these particles are located in the nucleus of the atom, which is at the center of the atom and contains most of the atom's mass.\\n\\nSurrounding the nucleus of the atom is a cloud of electrons. Electrons are negatively charged particles that are in constant motion around the nucleus. The electron cloud is divided into shells or orbitals, and each shell can hold a certain number of electrons. The number of electrons in the outermost shell, called the valence shell, determines the chemical properties of the atom. \\n\\nIn a neutral atom, the number of protons in the nucleus is equal to the number of electrons in the electron cloud, so the positive and negative charges balance out and the atom has no overall charge. The number of protons, also called the atomic number, determines what element the atom is.\"],\n",
       " 'instruction': ['Give three tips for staying healthy.',\n",
       "  'What are the three primary colors?',\n",
       "  'Describe the structure of an atom.'],\n",
       " 'input': ['', '', '']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process dataset using the tokenizer from the pretrained model  \n",
    "Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomTokenizerFast(name_or_path='bigscience/bloom-1b1', vocab_size=250680, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-1b1\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for data processing and map the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 8000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 256\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(\"\\n\".join([\"Human: \" + example[\"instruction\"], example[\"input\"]]).strip() + \"\\n\\nAssistant: \")\n",
    "    response = tokenizer(example[\"output\"] + tokenizer.eos_token)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "tokenized_ds = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What are the three primary colors?\\n\\nAssistant: The three primary colors are red, blue, and yellow. These colors are called primary because they cannot be created by mixing other colors and all other colors can be made by combining them in various proportions. In the additive color system, used for light, the primary colors are red, green, and blue (RGB).</s>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_ds[1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The three primary colors are red, blue, and yellow. These colors are called primary because they cannot be created by mixing other colors and all other colors can be made by combining them in various proportions. In the additive color system, used for light, the primary colors are red, green, and blue (RGB).</s>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_ds[1][\"labels\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-trianed model Bloom-1b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomForCausalLM(\n",
       "  (transformer): BloomModel(\n",
       "    (word_embeddings): Embedding(250880, 1536)\n",
       "    (word_embeddings_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "          (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=250880, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b1\", low_cpu_mem_usage=True)\n",
    "model # inspect model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set configuration for p_tuning using PEFT library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "from peft import PromptEncoderConfig, TaskType, get_peft_model, PromptEncoderReparameterizationType\n",
    "\n",
    "config = PromptEncoderConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=10,\n",
    "                             encoder_reparameterization_type=PromptEncoderReparameterizationType.MLP,\n",
    "                             encoder_dropout=0.1, encoder_num_layers=5, encoder_hidden_size=1024)\n",
    "#config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model for finetuining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23686\\AppData\\Roaming\\Python\\Python310\\site-packages\\peft\\tuners\\p_tuning\\model.py:105: UserWarning: for MLP, the argument `encoder_num_layers` is ignored. Exactly 2 MLP layers are used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): BloomForCausalLM(\n",
       "    (transformer): BloomModel(\n",
       "      (word_embeddings): Embedding(250880, 1536)\n",
       "      (word_embeddings_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      (h): ModuleList(\n",
       "        (0-23): 24 x BloomBlock(\n",
       "          (input_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attention): BloomAttention(\n",
       "            (query_key_value): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "            (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BloomMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "            (gelu_impl): BloomGelu()\n",
       "            (dense_4h_to_h): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1536, out_features=250880, bias=False)\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PromptEncoder(\n",
       "      (embedding): Embedding(10, 1536)\n",
       "      (mlp_head): Sequential(\n",
       "        (0): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=1024, out_features=1536, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(250880, 1536)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_peft_model(model, config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,213,248 || all params: 1,069,527,552 || trainable%: 0.39393543365211037\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set trainer configuration arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./chatbot\", # Save checkpoints to a folder\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=20,    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23686\\AppData\\Roaming\\Python\\Python310\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_ds,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")\n",
    "\n",
    "# define a callback function for logging the losses to a text file\n",
    "class LossLoggingCallback(TrainerCallback):\n",
    "    def __init__(self, output_dir):\n",
    "        self.output_dir = output_dir\n",
    "        self.losses = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if 'loss' in logs:\n",
    "            self.losses.append(logs['loss'])\n",
    "            with open(f\"{self.output_dir}/losses.txt\", \"a\") as f:\n",
    "                f.write(f\"{state.global_step}: {logs['loss']}\\n\")\n",
    "\n",
    "trainer.add_callback(LossLoggingCallback(output_dir=\"./\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e22ad684d2496596022362a7a08469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3789, 'grad_norm': 0.9479593634605408, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 2.3549, 'grad_norm': 1.5464972257614136, 'learning_rate': 4.9e-05, 'epoch': 0.02}\n",
      "{'loss': 2.1671, 'grad_norm': 2.3337299823760986, 'learning_rate': 4.85e-05, 'epoch': 0.03}\n",
      "{'loss': 2.1884, 'grad_norm': 2.6896445751190186, 'learning_rate': 4.8e-05, 'epoch': 0.04}\n",
      "{'loss': 2.2034, 'grad_norm': 2.3402836322784424, 'learning_rate': 4.75e-05, 'epoch': 0.05}\n",
      "{'loss': 1.9327, 'grad_norm': 2.5278422832489014, 'learning_rate': 4.7e-05, 'epoch': 0.06}\n",
      "{'loss': 1.9221, 'grad_norm': 1.3716206550598145, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.07}\n",
      "{'loss': 2.0053, 'grad_norm': 7.409725666046143, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 2.0212, 'grad_norm': 1.6158283948898315, 'learning_rate': 4.55e-05, 'epoch': 0.09}\n",
      "{'loss': 1.8959, 'grad_norm': 2.48403000831604, 'learning_rate': 4.5e-05, 'epoch': 0.1}\n",
      "{'loss': 1.7994, 'grad_norm': 1.4590351581573486, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.11}\n",
      "{'loss': 1.9688, 'grad_norm': 7.807621002197266, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.12}\n",
      "{'loss': 1.8454, 'grad_norm': 1.782036304473877, 'learning_rate': 4.35e-05, 'epoch': 0.13}\n",
      "{'loss': 1.8773, 'grad_norm': 1.6472448110580444, 'learning_rate': 4.3e-05, 'epoch': 0.14}\n",
      "{'loss': 1.8684, 'grad_norm': 13.436528205871582, 'learning_rate': 4.25e-05, 'epoch': 0.15}\n",
      "{'loss': 1.9199, 'grad_norm': 1.7069646120071411, 'learning_rate': 4.2e-05, 'epoch': 0.16}\n",
      "{'loss': 1.872, 'grad_norm': 2.1392927169799805, 'learning_rate': 4.15e-05, 'epoch': 0.17}\n",
      "{'loss': 1.8646, 'grad_norm': 2.2833409309387207, 'learning_rate': 4.1e-05, 'epoch': 0.18}\n",
      "{'loss': 1.9761, 'grad_norm': 2.951127529144287, 'learning_rate': 4.05e-05, 'epoch': 0.19}\n",
      "{'loss': 1.9869, 'grad_norm': 1.927355170249939, 'learning_rate': 4e-05, 'epoch': 0.2}\n",
      "{'loss': 1.8841, 'grad_norm': 2.5045218467712402, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.21}\n",
      "{'loss': 1.8116, 'grad_norm': 1.4008122682571411, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.22}\n",
      "{'loss': 1.8972, 'grad_norm': 1.7159337997436523, 'learning_rate': 3.85e-05, 'epoch': 0.23}\n",
      "{'loss': 1.8893, 'grad_norm': 1.0310620069503784, 'learning_rate': 3.8e-05, 'epoch': 0.24}\n",
      "{'loss': 1.8964, 'grad_norm': 3.8333520889282227, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.25}\n",
      "{'loss': 1.8713, 'grad_norm': 1.6094211339950562, 'learning_rate': 3.7e-05, 'epoch': 0.26}\n",
      "{'loss': 1.8017, 'grad_norm': 1.3568891286849976, 'learning_rate': 3.65e-05, 'epoch': 0.27}\n",
      "{'loss': 1.9438, 'grad_norm': 2.475947856903076, 'learning_rate': 3.6e-05, 'epoch': 0.28}\n",
      "{'loss': 1.8186, 'grad_norm': 1.6581863164901733, 'learning_rate': 3.55e-05, 'epoch': 0.29}\n",
      "{'loss': 1.8365, 'grad_norm': 3.2852535247802734, 'learning_rate': 3.5e-05, 'epoch': 0.3}\n",
      "{'loss': 1.8292, 'grad_norm': 1.715097427368164, 'learning_rate': 3.45e-05, 'epoch': 0.31}\n",
      "{'loss': 1.8128, 'grad_norm': 4.043025016784668, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.32}\n",
      "{'loss': 1.9504, 'grad_norm': 0.5570554733276367, 'learning_rate': 3.35e-05, 'epoch': 0.33}\n",
      "{'loss': 1.9937, 'grad_norm': 0.7877521514892578, 'learning_rate': 3.3e-05, 'epoch': 0.34}\n",
      "{'loss': 1.9222, 'grad_norm': 1.7406948804855347, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.35}\n",
      "{'loss': 1.7718, 'grad_norm': 1.781418800354004, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.36}\n",
      "{'loss': 1.8339, 'grad_norm': 0.9669230580329895, 'learning_rate': 3.15e-05, 'epoch': 0.37}\n",
      "{'loss': 1.9475, 'grad_norm': 3.663727283477783, 'learning_rate': 3.1e-05, 'epoch': 0.38}\n",
      "{'loss': 1.8141, 'grad_norm': 2.3957645893096924, 'learning_rate': 3.05e-05, 'epoch': 0.39}\n",
      "{'loss': 2.1153, 'grad_norm': 1.1903808116912842, 'learning_rate': 3e-05, 'epoch': 0.4}\n",
      "{'loss': 1.9589, 'grad_norm': 1.5157729387283325, 'learning_rate': 2.95e-05, 'epoch': 0.41}\n",
      "{'loss': 1.7591, 'grad_norm': 1.487905740737915, 'learning_rate': 2.9e-05, 'epoch': 0.42}\n",
      "{'loss': 1.8682, 'grad_norm': 2.1528892517089844, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.43}\n",
      "{'loss': 1.7862, 'grad_norm': 2.253916025161743, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.44}\n",
      "{'loss': 1.8934, 'grad_norm': 9.462148666381836, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.45}\n",
      "{'loss': 1.9483, 'grad_norm': 1.682309865951538, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.46}\n",
      "{'loss': 1.8275, 'grad_norm': 1.697963833808899, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.47}\n",
      "{'loss': 1.6758, 'grad_norm': 0.877429187297821, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.48}\n",
      "{'loss': 1.8869, 'grad_norm': 3.508242607116699, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.49}\n",
      "{'loss': 1.8732, 'grad_norm': 0.8541114330291748, 'learning_rate': 2.5e-05, 'epoch': 0.5}\n",
      "{'loss': 1.7755, 'grad_norm': 0.9758174419403076, 'learning_rate': 2.45e-05, 'epoch': 0.51}\n",
      "{'loss': 1.8496, 'grad_norm': 1.4049344062805176, 'learning_rate': 2.4e-05, 'epoch': 0.52}\n",
      "{'loss': 1.8553, 'grad_norm': 1.116855263710022, 'learning_rate': 2.35e-05, 'epoch': 0.53}\n",
      "{'loss': 1.9339, 'grad_norm': 1.999426245689392, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.54}\n",
      "{'loss': 1.8373, 'grad_norm': 2.188495635986328, 'learning_rate': 2.25e-05, 'epoch': 0.55}\n",
      "{'loss': 1.9196, 'grad_norm': 1.9436901807785034, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.56}\n",
      "{'loss': 1.8257, 'grad_norm': 1.60861074924469, 'learning_rate': 2.15e-05, 'epoch': 0.57}\n",
      "{'loss': 1.7706, 'grad_norm': 1.1723037958145142, 'learning_rate': 2.1e-05, 'epoch': 0.58}\n",
      "{'loss': 1.8801, 'grad_norm': 1.581987738609314, 'learning_rate': 2.05e-05, 'epoch': 0.59}\n",
      "{'loss': 1.9671, 'grad_norm': 1.631639838218689, 'learning_rate': 2e-05, 'epoch': 0.6}\n",
      "{'loss': 1.7254, 'grad_norm': 1.4412548542022705, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.61}\n",
      "{'loss': 1.77, 'grad_norm': 0.9883862733840942, 'learning_rate': 1.9e-05, 'epoch': 0.62}\n",
      "{'loss': 1.8052, 'grad_norm': 0.907845675945282, 'learning_rate': 1.85e-05, 'epoch': 0.63}\n",
      "{'loss': 2.0686, 'grad_norm': 0.6443005204200745, 'learning_rate': 1.8e-05, 'epoch': 0.64}\n",
      "{'loss': 1.7472, 'grad_norm': 0.7813799977302551, 'learning_rate': 1.75e-05, 'epoch': 0.65}\n",
      "{'loss': 2.0775, 'grad_norm': 1.5682690143585205, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.66}\n",
      "{'loss': 1.7664, 'grad_norm': 0.7604361772537231, 'learning_rate': 1.65e-05, 'epoch': 0.67}\n",
      "{'loss': 1.7214, 'grad_norm': 1.9593510627746582, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.68}\n",
      "{'loss': 1.8969, 'grad_norm': 1.993166446685791, 'learning_rate': 1.55e-05, 'epoch': 0.69}\n",
      "{'loss': 1.7273, 'grad_norm': 1.4610729217529297, 'learning_rate': 1.5e-05, 'epoch': 0.7}\n",
      "{'loss': 1.8476, 'grad_norm': 1.5250203609466553, 'learning_rate': 1.45e-05, 'epoch': 0.71}\n",
      "{'loss': 1.7775, 'grad_norm': 1.2756234407424927, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.72}\n",
      "{'loss': 1.812, 'grad_norm': 1.28706955909729, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.73}\n",
      "{'loss': 1.8186, 'grad_norm': 1.0518466234207153, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.74}\n",
      "{'loss': 2.1327, 'grad_norm': 2.9019205570220947, 'learning_rate': 1.25e-05, 'epoch': 0.75}\n",
      "{'loss': 1.9148, 'grad_norm': 1.6924575567245483, 'learning_rate': 1.2e-05, 'epoch': 0.76}\n",
      "{'loss': 1.8199, 'grad_norm': 1.4269981384277344, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.77}\n",
      "{'loss': 1.6893, 'grad_norm': 7.1663713455200195, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.78}\n",
      "{'loss': 1.8137, 'grad_norm': 2.3364105224609375, 'learning_rate': 1.05e-05, 'epoch': 0.79}\n",
      "{'loss': 1.6936, 'grad_norm': 1.147952675819397, 'learning_rate': 1e-05, 'epoch': 0.8}\n",
      "{'loss': 1.7527, 'grad_norm': 1.8092856407165527, 'learning_rate': 9.5e-06, 'epoch': 0.81}\n",
      "{'loss': 1.8537, 'grad_norm': 1.0527667999267578, 'learning_rate': 9e-06, 'epoch': 0.82}\n",
      "{'loss': 1.9552, 'grad_norm': 0.9952320456504822, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.83}\n",
      "{'loss': 1.9164, 'grad_norm': 6.583702564239502, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.84}\n",
      "{'loss': 1.7548, 'grad_norm': 1.392901062965393, 'learning_rate': 7.5e-06, 'epoch': 0.85}\n",
      "{'loss': 1.7856, 'grad_norm': 3.3367786407470703, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.86}\n",
      "{'loss': 1.8187, 'grad_norm': 1.021989345550537, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.87}\n",
      "{'loss': 1.8444, 'grad_norm': 0.9172995090484619, 'learning_rate': 6e-06, 'epoch': 0.88}\n",
      "{'loss': 1.729, 'grad_norm': 0.7507585287094116, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.89}\n",
      "{'loss': 1.9745, 'grad_norm': 7.665128707885742, 'learning_rate': 5e-06, 'epoch': 0.9}\n",
      "{'loss': 1.8489, 'grad_norm': 0.7538059949874878, 'learning_rate': 4.5e-06, 'epoch': 0.91}\n",
      "{'loss': 1.8645, 'grad_norm': 1.2099156379699707, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.92}\n",
      "{'loss': 1.7608, 'grad_norm': 1.1706262826919556, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.93}\n",
      "{'loss': 1.85, 'grad_norm': 1.8447185754776, 'learning_rate': 3e-06, 'epoch': 0.94}\n",
      "{'loss': 1.9175, 'grad_norm': 1.4056522846221924, 'learning_rate': 2.5e-06, 'epoch': 0.95}\n",
      "{'loss': 1.7755, 'grad_norm': 1.32125985622406, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.96}\n",
      "{'loss': 1.7547, 'grad_norm': 2.765195846557617, 'learning_rate': 1.5e-06, 'epoch': 0.97}\n",
      "{'loss': 1.8618, 'grad_norm': 2.036929130554199, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.98}\n",
      "{'loss': 1.8031, 'grad_norm': 1.2011584043502808, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.99}\n",
      "{'loss': 1.7156, 'grad_norm': 1.2497855424880981, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 1007.4501, 'train_samples_per_second': 7.941, 'train_steps_per_second': 0.993, 'train_loss': 1.8794574642181396, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=1.8794574642181396, metrics={'train_runtime': 1007.4501, 'train_samples_per_second': 7.941, 'train_steps_per_second': 0.993, 'train_loss': 1.8794574642181396, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[114330,     29,   7535,    427,  52615,    660,  19728,   2498,    603,\n",
      "           9096,  61339,     29,    210]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "Human: How to prepare an exam？\n",
      "\n",
      "Assistant: Prepare for an exam using the following exam preparations. Take all the practice subjects you need so that you can practice it consistently. Consider the exam series you took; study topics in accordance with its subjects; and prepare for the exam in the most effective way possible.\n",
      "\n",
      "There are many exam preparation resources for the preparation of any exam. For exam preparation, I suggest you access the following resources:\n"
     ]
    }
   ],
   "source": [
    "#model = model.cuda()\n",
    "ipt = tokenizer(\"Human: {}\\n{}\".format(\"How to prepare an exam？\", \"\").strip() + \"\\n\\nAssistant: \", return_tensors=\"pt\").to(model.device)\n",
    "print( ipt)\n",
    "print(tokenizer.decode(model.generate(**ipt, max_length=256, do_sample=True)[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
